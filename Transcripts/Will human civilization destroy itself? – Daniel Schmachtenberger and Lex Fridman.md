---
tags: transcript
aliases:
youtube_id: 01rVKtbqtnA
---

<div class="yt-container"><iframe src="https://www.youtube.com/embed/01rVKtbqtnA"></iframe></div>

what flavors of [[catastrophic risk]] are we talking about here what's your favorite flavor in terms of ice cream so mine is coconut nobody seems to like coconut ice cream so ice cream aside what's uh what are you most worried about if there's a [[catastrophic risk]] that will help us kind of um make concrete the the the discussion we're having about how to fix this whole [26](https://www.youtube.com/watch?v=01rVKtbqtnA&t=26.8s)

thing yeah i think it's worth taking a historical perspective briefly to just kind of orient everyone to it we don't have to um go all the way back to the aliens who've seen all of civilization but to just recognize that for all of human history as far as we're aware there were [[existential risk|existential risks]] to civilizations and they happened right like there were civilizations that were killed in war um that tribes that were killed in tribal warfare or whatever so uh people faced [[existential risk]] to the group that they identified with it's just those were local phenomena right there it wasn't a fully global [64](https://www.youtube.com/watch?v=01rVKtbqtnA&t=64.4s)

phenomena so an empire could fall and surrounding empires didn't fall maybe they came in and filled the space the first time that we were able to think about [[catastrophic risk]] not from like a solar flare or something that we couldn't control but from something that humans would actually create at a global level was world war ii in the bomb because it was the first [85](https://www.youtube.com/watch?v=01rVKtbqtnA&t=85.92s)

time that we had tech big enough that could actually mess up everything at a global level it could mess up habitability we just weren't powerful enough to do that before it's not that we didn't behave in ways that would have done it we just only behaved in those ways at the scale we could affect and [102](https://www.youtube.com/watch?v=01rVKtbqtnA&t=102.079s)

so it's important to get that there's the entire world before world war ii where we don't have the ability to make a non-habitable biosphere non-inhabitable for us and then there's world war ii and the beginning of a completely new phase where global human induced [[catastrophic risk]] is now a real thing [121](https://www.youtube.com/watch?v=01rVKtbqtnA&t=121.36s)

and that was such a big deal that it changed the entire world in a really fundamental way which is you know when you study history it's amazing how big a percentage of history is studying war right in the history of wars you said european history whatever it's generals and wars and empire expansions and and so the the major empires near each [140](https://www.youtube.com/watch?v=01rVKtbqtnA&t=140.16s)

other never had really long periods of time where they weren't engaged in war or preparation for or something like that that was humans don't have a good precedent in the post tribal phase the civilization phase of being able to solve conflicts without war for very long world war ii was the first time [158](https://www.youtube.com/watch?v=01rVKtbqtnA&t=158.959s)

where we could have a war that no one could win and so the superpowers couldn't fight again they couldn't do a real kinetic war they could do diplomatic wars and cold war type stuff and they could fight proxy wars through other countries that didn't have the big weapons and so [[mutually assured destruction]] and like coming out of world war ii we actually realized that [[nation state|nation states]] couldn't prevent world war and so we needed a new type of supervening government in addition to [[nation state|nation states]] which was the whole [[bretton woods]] world the united nations the world bank the imf the globalization trade type agreements [193](https://www.youtube.com/watch?v=01rVKtbqtnA&t=193.76s)

[[mutually assured destruction]] that was how do we have some coordination beyond just [[nation state|nation states]] between them since we have to stop war between at least the superpowers and it was pretty successful given that we've had like 75 years of no superpower on superpower war um we've had lots of proxy wars during that [215](https://www.youtube.com/watch?v=01rVKtbqtnA&t=215.2s)

time we've had you know cold war and i would say we're in a new phase now where the [[bretton woods]] solution is basically over almost over can you describe the breadth of solution yeah so the [[bretton woods]] the series of agreements for how uh the nations would be able to engage with each other in a solution [239](https://www.youtube.com/watch?v=01rVKtbqtnA&t=239.519s)

other than war um was these igos these intergovernmental organizations and was the idea of globalization since we could have global effects we needed to be able to think about things globally where we had trade relationships with each other where it would not be profitable to war with each other to be more [256](https://www.youtube.com/watch?v=01rVKtbqtnA&t=256.88s)

profitable to actually be able to trade with each other so our own self-interest was you know going to drive our non-more interest and so this started to look like and obviously this this couldn't have happened that much earlier either because industrialization hadn't gotten far [272](https://www.youtube.com/watch?v=01rVKtbqtnA&t=272.32s)

enough to be able to do massive global industrial [[supply chain|supply chains]] and ships stuff around you know quickly but like we were mentioning earlier almost all the electronics that we use today just basic cheap stuff for us is made on six continents made in many countries there's no single country in the world that could actually [288](https://www.youtube.com/watch?v=01rVKtbqtnA&t=288.08s)

make many of the things that we have and from the raw material extraction to the plastics and polymers and the you know et cetera and so the i the idea that we made a world that could do that kind of trade and create massive gdp growth we could all work together to be able to mine natural resources and grow stuff [305](https://www.youtube.com/watch?v=01rVKtbqtnA&t=305.28s)

with the rapid gdp growth there was the idea that everybody could keep having more without having to take each other's stuff and so that that was part of kind of the the [[bretton woods]] post world war ii model the other was that we would be so economically interdependent that blowing each other up would never make [322](https://www.youtube.com/watch?v=01rVKtbqtnA&t=322.96s)

sense that worked for a while now it also brought us up into [[planetary boundary|planetary boundaries]] faster the unrenewable use of resource and turning those resources into pollution on the other side of the [[supply chain]] so obviously that faster gdp growth meant uh the overfishing of the oceans and the cutting down of the trees and the [347](https://www.youtube.com/watch?v=01rVKtbqtnA&t=347.12s)

[[climate change]] and the mining toxic mining tailings going into the water and the mountaintop removal mining and all those types of things that's the over consumption side of the of the risk that we're talking about and so the answer of let's do positive gdp is the answer rapidly and exponentially obviously [365](https://www.youtube.com/watch?v=01rVKtbqtnA&t=365.44s)

accelerated the [[planetary boundary]] side and that started to be that that was thought about for a long time but it started to be modeled with the club of rome and limits of growth um and it but it's just very obvious to say if you have a linear [[materials economy]] where you take stuff [385](https://www.youtube.com/watch?v=01rVKtbqtnA&t=385.36s)

out of the earth faster whether it's fish or trees or or ore you take or oil you take it out of the earth faster than it can replenish itself and you turn it into trash after using it for a short period of time you put the trash in the environment faster than it can process itself and there's toxicity [400](https://www.youtube.com/watch?v=01rVKtbqtnA&t=400.319s)

associated with both sides of this you can't run an exponentially growing linear [[materials economy]] on a finite planet forever that's not a hard thing to figure out and it has to be exponential if there's an exponentiation in the monetary supply because of interest and then fractional reserve banking and to then be able to keep up with the growing monetary supply [420](https://www.youtube.com/watch?v=01rVKtbqtnA&t=420.16s)

you have to have growth of goods and services and so that's that kind of thing that has happened um but you also see that when you get these [[supply chain|supply chains]] that are so interconnected across the world you get increased fragility because a collapse or a problem in one area then affects the whole world in a much bigger [437](https://www.youtube.com/watch?v=01rVKtbqtnA&t=437.44s)

area as opposed to the issues being local right so we got to see with covid and an issue that started in one part of china affecting the whole world so much more rapidly than would have happened before [[bretton woods]] right before international travel [[supply chain|supply chains]] you know that whole kind of thing and [455](https://www.youtube.com/watch?v=01rVKtbqtnA&t=455.44s)

with a bunch of second and third order effects that people wouldn't have predicted okay we have to stop certain kinds of travel because of viral contaminants but the countries doing agriculture depend upon fertilizer they don't produce that is shipped into them and depend upon pesticides they don't produce so we got both crop failures and crops being eaten by locusts in scale in northern africa and iran and things like that because they couldn't get the supplies of stuff in so then you get massive starvation or future kind of hunger issues because of [[supply chain]] shutdowns so you get this increased fragility and cascade dynamics where a small problem [488](https://www.youtube.com/watch?v=01rVKtbqtnA&t=488.479s)

can end up leading to cascade effects and also we went from two superpowers with one [[catastrophe weapon]] to now that same [[catastrophe weapon]] is there's more countries that have it eight or nine countries that have it and there's a lot more types of [[catastrophe weapon|catastrophe weapons]] we now have [[catastrophe weapon|catastrophe weapons]] with weaponized drones that can hit infrastructure targets with bio with in fact every new type of tech has created an [[arms race]] so we have not with the un or the other kind of intergovernmental organizations we haven't been able to really do nuclear deproliferation we've actually had more countries get nukes and keep getting faster nukes the [533](https://www.youtube.com/watch?v=01rVKtbqtnA&t=533.76s)

race to hypersonics and things like that um and every new type of technology that has emerged has created an [[arms race]] and so you can't do [[mutually assured destruction]] with multiple agents the way you can with two agents two agents it's a much easier to create a stable nash equilibrium that's forced [554](https://www.youtube.com/watch?v=01rVKtbqtnA&t=554.48s)

but the ability to monitor and say if these guys shoot who do i shoot do i shoot them do i shoot everybody do i and so you get a three-body problem you get a very complex type of thing when you have multiple agents and multiple different types of [[catastrophe weapon|catastrophe weapons]] including ones that can be much more easily produced than nukes are really hard to produce there's [570](https://www.youtube.com/watch?v=01rVKtbqtnA&t=570.24s)

only uranium in a few areas uranium enrichment is hard icbms are hard but weaponized drones hitting smart targets is not so hard there's a lot of other things where basically the scale at being able to manufacture them is going way way down to where even non-state actors can have them and so when we talk about [[exponential tech]] and the decentralization of [[exponential tech]] what that means is decentralized [[catastrophe weapon]] capacity and especially in a world of increasing numbers of people feeling disenfranchised frantic whatever for different reasons so uh i would say where the [[bretton woods]] world doesn't prepare us to be able to deal with lots of different agents having lots of different types of [[catastrophe weapon|catastrophe weapons]] you can't put mutually sure destruction on where you can't keep doing growth of [[materials economy]] in the same way because of hitting [[planetary boundary|planetary boundaries]] and where the fragility dynamics are [625](https://www.youtube.com/watch?v=01rVKtbqtnA&t=625.36s)

actually now their own source of [[catastrophic risk]] so now we're so like there was all the world until world war ii and world war ii is just from uh from a civilization time scale point of view was just a second ago it seems like a long time but it is really not we get a short period of relative peace at the level of [642](https://www.youtube.com/watch?v=01rVKtbqtnA&t=642.24s)

superpowers while building up the military capacity for much much much worse war the entire time and then now we're at this new phase where the things that allowed us to make it through the nuclear power are not the same systems that will let us make it through the next stage so what is this next post [[bretton woods]] how how do we become safe vessels safe stewards of many different types of exponential technology is a key question when we're thinking about x-risk okay so and i i'd like to try to answer the how if you thought a few ways but first on the mutually sure destruction [682](https://www.youtube.com/watch?v=01rVKtbqtnA&t=682.8s)

do you give credit to the idea of two superpowers not blowing each other up with [[nuclear weapon|nuclear weapons]] to the simple game theoretic model of mutually shared destruction or something you've said previously this idea of inverse correlation which i tend to believe between now if you were talking about tech but i think it's maybe broadly true the inverse correlation between competence and propensity for destruction so the better the the the bigger your weapons not because you're afraid of uh mutually assured self-destruction but because we're human beings and there's a deep moral fortitude there that's somehow [731](https://www.youtube.com/watch?v=01rVKtbqtnA&t=731.04s)

aligned with competence and being good at your job that like it's very hard to be a psychopath and be good at killing at scale do you share any of that intuition kind of i think most people would say that alexander the great and genghis khan and napoleon were effective [757](https://www.youtube.com/watch?v=01rVKtbqtnA&t=757.12s)

people that were good at their job uh that were actually maybe asymmetrically good at being able to organize people and do certain kinds of things that were pretty oriented towards certain types of destruction um or pretty willing to maybe they would say they were oriented towards empire expansion but pretty willing to [779](https://www.youtube.com/watch?v=01rVKtbqtnA&t=779.279s)

commit certain acts of destruction in the name of it what are you worried about the genghis khan or you could argue he's not a psychopath uh that are you worried about genghis khan are you worried about hitler or are you worried about a terrorist who is has a very different ethic which is not [801](https://www.youtube.com/watch?v=01rVKtbqtnA&t=801.04s)

even for uh for it's not trying to preserve and build and expand my community it's more about just the destruction in itself is the goal i think the thing that you're looking at that i do agree with is that there's a psychological disposition towards construction and a psychological [822](https://www.youtube.com/watch?v=01rVKtbqtnA&t=822.72s)

disposition more towards destruction obviously everybody has both and can toggle between both and oftentimes one is willing to destroy certain things we have this idea of creative destruction right willing to destroy certain things to create other things and utilitarianism and trolley problems are all about exploring that [840](https://www.youtube.com/watch?v=01rVKtbqtnA&t=840.88s)

space and the idea of war is all about that i am trying to create something for our people and it requires destroying some other people sociopathy is a funny topic because it's possible to have very high fealty to your in-group and work on perfecting the methods of torture to the out group [859](https://www.youtube.com/watch?v=01rVKtbqtnA&t=859.6s)

at the same time because you can dehumanize and then remove empathy and i would also say that there are types so the reason the thing that gives hope about the orientation towards construction and destruction being a little different in psychologies is what it takes to build really catastrophic tech [884](https://www.youtube.com/watch?v=01rVKtbqtnA&t=884.079s)

even today where it doesn't take what it took to make a new a small group people could do it takes still some real technical knowledge that required having studied for a while and some in building capacity and there's a question of is that psychologically inversely correlated with the desire to [901](https://www.youtube.com/watch?v=01rVKtbqtnA&t=901.519s)

damage civilization meaningfully uh a little bit a little bit i think um i think a lot i think it's actually i mean this is the conversation i had like with i think offline with dan carlin which is like it's pretty easy to come up with ways that any competent like i can come up with a lot of ways to hurt a lot of [926](https://www.youtube.com/watch?v=01rVKtbqtnA&t=926.48s)

people and it's it's pretty easy like i alone can do it and like there's a lot of people as smart or smarter than me at least in the creation of explosives why are we not seeing more insane mass murder i i think there's something fascinating [947](https://www.youtube.com/watch?v=01rVKtbqtnA&t=947.519s)

and beautiful about this yes and it does have to do with some deeply pro-social types of characteristics in humans and um but when you're dealing with very large numbers you don't need a whole lot of a phenomena and so then you start to say well what's the probability that x won't happen this [969](https://www.youtube.com/watch?v=01rVKtbqtnA&t=969.04s)

year then won't happen in the next two years three years four years and then how many people are doing destructive things with lower tech and then how many of them can get access to higher tech that they didn't have to figure out how to build so uh when i can get commercial tech and maybe i don't understand tech very well but [989](https://www.youtube.com/watch?v=01rVKtbqtnA&t=989.68s)

i understand it well enough to utilize it not to create it and i can repurpose it when we saw that commercial drone with a homemade thermite bomb hit the ukraine ukrainian munitions factory and do the equivalent of an incendiary bomb level of damage that's just home tech that's just simple kind of thing and so the question is not [1011](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1011.92s)

what is does it stay being a small percentage of the population the question is does can you bind that phenomena nearly completely and especially now when you as you start to get into bigger things crisper gene drive technologies and various things like that um can you bind it completely long term over what period of time not perfectly though that's the thing i'm trying i'm trying to say that there is some let's call it uh that's uh a random word love that's inherent in that's core to [[human nature]] that's preventing destruction at scale and you're saying yeah but there's a lot of humans there's [1059](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1059.76s)

going to be eight plus billion and then there's a lot of seconds in the day to come up with stuff there's a lot of pain in the world that can lead to a distorted view of the world such that you want to channel that pain into the destruction all those kinds of things and it's only a matter of time that any one individual can do large damage especially as we [1078](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1078.799s)

create more and more democratized decentralized ways to deliver that damage even if you don't know how to build the initial weapon you can but the thing is it seems like it's a race between the cheapening of destructive weapons and the capacity of humans to express their love towards each other and it's a race that [1106](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1106.32s)

so far i know on twitter you it's not popular to say but love is winning okay so what is the argument that love is going to lose here against [[nuclear weapon|nuclear weapons]] and biotech and and ai and uh and drones okay i'm gonna come at the end of this to a how love wins so i just want you to know that that's where i'm oriented [1132](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1132.32s)

that's the end okay but i'm i'm going to argue against why that is a given because it because it's not a given i don't believe and i think this is like a good romantic comedy so you're going to create drama right now but it will end in a happy evening well it's because it's only a happy ending if we actually understand the issues well [1153](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1153.36s)

enough and take responsibility to shift it do i believe like there's a reason why there's so much more dystopic sci-fi than pro-topic sci-fi in the in the some proto-big sci-fi usually requires magic is because or at least magical tech right dilithium crystals and warp drives and stuff because it's very hard to imagine [1173](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1173.84s)

people like the people we have been in the history books with exponential type technology and power that don't eventually blow themselves up that make good enough choices as stewards of their environment and their comments and and each other and etc so like it's easier to think of scenarios where we [1195](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1195.2s)

blow ourselves up than it is to think of scenarios where we avoid every single scenario where we blow ourselves up and when i say blow ourselves up i also i mean the environmental versions the terrorist versions the war versions the cumulative externalities versions um can i and i'm sorry if i'm interrupting your flow of thought [1214](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1214.48s)

but why is it easier is it could it be a weird psychological thing where we either was just more capable to visualize explosions and destruction and then the sicker thought which is like we kind of enjoy for some weird reason thinking about that kind of stuff even though we wouldn't actually act on it it's almost like some [1232](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1232.48s)

weird uh like i love playing shooter games you know uh first person shooters and like especially if it's like murdering zombie and doom you're shooting demons i played one of my favorite games diablo is like slashing through different monsters and the screaming and pain and the hellfire [1251](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1251.28s)

and then i go out into the real world uh to eat my coconut ice cream and i'm all about love so like i can we trust our ability to visualize how all it all goes to as an actual rational way of thinking i think it's a fair question to say to what degree is there just kind of perverse fantasy and [1271](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1271.2s)

morbid exploration and whatever else that happens in our imagination uh but i don't think that's the whole of it i think there is also a reality to the combinatorial possibility space and the difference in the probabilities that there's a lot of ways i could try to put the 70 trillion cells of your body [1293](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1293.2s)

together that don't make you there's not that many ways i can put them together that make you there's a lot of ways i could try to connect the organs together that make some weird kind of group of organs on the on a desk but that doesn't actually make a functioning human and and you can kill an adult human in a second but you can't get one [1312](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1312.0s)

in a second takes 20 years to grow one and a lot of things to happen right i could destroy this building in a couple minutes with demolition but it took a year or a couple years to build it there is uh there is just an example it's not he doesn't mean it there's a there's a gradient where entropy is [1333](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1333.12s)

easier and there's a lot more ways to put a set of things together that don't work than the few that really do produce higher order synergies and so when we look at a history of war and then we look at exponentially more powerful warfare and [[arms race]] that drives that in all these directions and when we look at a history of environmental destruction and exponentially more powerful tech that makes exponential externalities multiplied by the total number of agents that are doing it in the cumulative effects there's a lot of ways the whole thing can break like a lot of different ways [1369](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1369.52s)

and for it to get ahead it has to have none of those happen and so there's just a probability space where it's easier to imagine that thing so what so to say how do we have a protopic future we have to say well one criteria must be that it avoids all of the [[catastrophic risk|catastrophic risks]] so can we understand can we inventory [1390](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1390.159s)

all the [[catastrophic risk|catastrophic risks]] can we inventory the patterns of human behavior that give rise to them and could we try to solve for that and could we have that be the essence of the social technology that we're thinking about to be able to guide bind and direct the new physical technology because so far our physical technology like we were talking about the genghis khans and like that that obviously use certain kinds of physical technology and armaments and also social technology and unconventional warfare for a particular set of purposes but we have things that don't look like warfare like rockefeller and standard oil and it looked like a constructive [1428](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1428.159s)

mindset to be able to uh bring this new energy resource to the world and it did and the second order effects of that are [[climate change]] and all of the oil spills that have happened and will happen and all of the wars in the middle east over the oil that have been there [1450](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1450.32s)

and the massive political and human life issues that are associated with it and on and on right and so it's also not just the orientation to construct a thing can have a narrow focus on what i'm trying to construct but be affecting a lot of other things through second and third-order effects i'm not taking [1472](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1472.24s)

responsibility for and you you often another and another tangent mentioned second third and fourth order effects and the order and and skating which is really fascinating like starting with the third order plus it gets really interesting because we we don't we don't even [1492](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1492.72s)

acknowledge like the second order effects right but like thinking because those it could mat it could get bigger and bigger and bigger in ways we're not anticipating so how do we make those so it sounds like part of the part of the thing that you're thinking through in terms of a solution [1509](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1509.919s)

how to create an anti-fray agile a resilient society is to make explicit acknowledge understand the externalities the second order third order fourth order and the order effects how do we start to think about those effects yeah the war application is harm we're [1532](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1532.4s)

trying to cause or that we're aware we're causing right um the externality is harm that at least supposedly we're not aware we're causing or at minimum it's not our intention right maybe we're either totally unaware of it or we're aware of it but it is a side effect of what our intention is it's not the intention [1548](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1548.4s)

itself there are [[catastrophic risk|catastrophic risks]] from both types the direct application of increased technological power to a rivalrous intent which is going to cause harm for some out group for some in group to win but the out group is also working on growing the tech and if they don't lose completely they [1567](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1567.44s)

reverse engineer the tech upregulator come back with more capacity so there's the [[exponential tech]] [[arms race]] side of in-group out-group rivalry using [[exponential tech]] that is one set of risks and the other set of risks the application of exponentially more powerful tech not intentionally to try and beat an out group but to try to achieve some goal that we have but to produce a second and third order effects that do have harm to the commons to other people to environment to other groups uh that might actually be bigger problems than the problem we were originally trying to solve with the thing we were [1607](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1607.2s)

building when facebook was building a dating app and then building a social app where people could tag pictures they weren't trying to build a democracy destroying app that would maximize time on site as part of its ad model through ai optimization of a news feed to the thing that made people spend most time on site [1632](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1632.32s)

which is usually them being limbically hijacked more than something else which ends up appealing to people's cognitive biases and group identities and creates no sense of shared reality they weren't trying to do that but it was a second order effect and it's a pretty powerful second order effect and a pretty fast one because the rate [1653](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1653.039s)

of tech is obviously able to get distributed to much larger scale much faster and with a bigger jump in terms of total vertical capacity then that's what it means to get to the verticalizing part of an exponential curve so um just like we can see that oil had these second order environmental effects and also social [1672](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1672.72s)

and political effects war and so much of the whole like the total amount of oil used is has a proportionality to total global gdp and that's why we have this you know the petrodollar and um and so the the oil thing also had the externalities of a major aspect of what happened with military industrial [1695](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1695.039s)

complex and things like that so but we can see the same thing with with more current technologies with facebook and google and and other things so i don't think we can run and the more powerful the tech is we build it for reason x whatever reason x is maybe x is [1712](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1712.88s)

three things maybe it's one thing right we we're doing the oil thing because we want to make cars because it's a better method of individual transportation we're building the facebook thing because we're going to connect people socially in a personal sphere but it it intersects with it interacts with [1729](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1729.6s)

complex systems with ecologies economies psychologies cultures and so it has effects on other than the thing we're intending some of those effects can end up being negative effects but because this technology if if we make it to solve a problem it has to overcome the problem the problem's been around for a while it's [1750](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1750.48s)

going to overcome in a short period of time so it usually has greater scale greater rate of magnitude in some way that also means that the externalities that it creates might be bigger problems and you can say well but then that's the new problem humanity will innovate its way out of that well i don't think that's paying [1766](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1766.24s)

attention to the fact that we can't keep up with exponential curves like that nor do finite spaces allow exponential externalities forever and this is why a lot of the smartest people thinking about this are thinking well no i think we're totally screwed and unless we can make a benevolent ai singleton that rules all of us [1786](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1786.159s)

um you know guys like bostrom and and others uh thinking in those directions because they're like how do humans try to do multipolarity and make it work and i i have a different answer of what i think it looks like that does have more to do with the love but some applied social tackle line [1804](https://www.youtube.com/watch?v=01rVKtbqtnA&t=1804.799s)

alignment because i have a bunch of really dumb ideas i prefer to uh i'd like to hear i'd like to hear some of them first you