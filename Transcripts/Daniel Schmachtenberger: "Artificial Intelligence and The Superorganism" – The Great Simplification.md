---
tags: transcript
aliases:
youtube_id: _P8PLHvZygo
published_at: '2023-05-17'
---

<div class="yt-container"><iframe src="https://www.youtube.com/embed/_P8PLHvZygo"></iframe></div>

[[artificial intelligence]] is in the news. we hear 
about chatgpt, making people more efficient,   learning quicker. we hear about ai replacing 
artists and mid-level programmers. we see deep   fakes and fake beautiful baby peacocks that are 
much cuter than real baby peacocks. and lots of   people are debating about the benefits 
and risks of [[artificial intelligence]].   but today's guest is my colleague 
and friend, daniel schmachtenberger,   who is back for a deep dive on how artificial 
intelligence accelerates the [[superorganism]] dynamic   with respect to extraction, climate and many 
of the [[planetary boundary]] limits that we face.
  i have not heard this angle on 
[[artificial intelligence]] before.   i think it's really important to have this 
conversation and throughout this talk with daniel,   we talked about ai, but underpinning it all was 
what is intelligence and how has intelligence   in groups in human history out-competed wisdom 
restraint of different cultures and different   [78](https://www.youtube.com/watch?v=_P8PLHvZygo&t=78.84s)

groups of humans? this is an intense, dense 
three and a half hour conversation and we   weren't even done. we'll be back in the next 
month or so to have the follow on questions.   it's probably one of the better conversations 
i've ever had on the great simplification,   and i think it's really important to merge 
the environmental consequences of ai into   our cultural discourse. here's my 
friend daniel schmachtenberger.
  hello my friend.
hey nate. good to be back with you.
  i prepare a lot for my podcasts. i read people's 
stuff, i prepare questions, i think about it,   but with you, i'm like, i got a appointment with 
daniel at 4:00 pm. i go for a bike ride, i go play   with my chickens and i just show up and we have a 
conversation. so i'm hoping this'll work because   this conversation actually is the culmination of 
how our relationship started a couple, three years   [153](https://www.youtube.com/watch?v=_P8PLHvZygo&t=153.06s)

ago. remember we came to washington, dc for a 
five or six day meeting where i wanted to discuss   energy, money, technology and how this combined 
into a [[superorganism]]. and you were focused   on [[existential risk|existential risks]] and particularly oncoming 
innovation in [[artificial intelligence]] and how that   led to a lot of potential unknown destabilizing 
risks for society. and now we've educated each   other after a couple years. and today rather than 
continue our bend versus break series, i thought   we would merge these two lines of thought on 
[[artificial intelligence]] and the [[superorganism]].
  you and i have done five parts so far in 
this bend versus break series. given all   the things that are in the public attention 
on ai, we decided to do this one. i imagine   some of the people will have heard that series 
and we can reference the concepts for anyone   who hasn't. do you want to give a quick recap 
on [[superorganism]] so we can relate it and maybe   [[superorganism]] and [[metacrisis]]? and those are kind 
of frames we've established that we're going to   be bringing into thinking about ai now?
sure. so humans are a social species and in   the modern world, we self-organize as family 
units, as small businesses, as corporations,   as [[nation state|nation states]], as an entire global economic 
system around profits. profits are our goal   and profits lead to gdp or gwp globally. and 
what we need for that gdp is three things. we   need energy, we need materials, and we need 
technology or in your terms, information.   and we have outsourced the wisdom and the decision 
making of this entire system to the market.   and the market is blind to 
the impacts of this growth.   we represent this by money, and money is a claim 
on energy. and energy from fossil hydrocarbons is   incredibly powerful, indistinguishable from magic 
effectively on human time scales. it's also not   [322](https://www.youtube.com/watch?v=_P8PLHvZygo&t=322.02s)

infinite. and as a society we are drawing down the 
bank account of fossil carbon and non-renewable   inputs like cobalt and copper and 
neodymium and water aquifers and forests,   millions of times faster 
than they were sequestered.
  so there is a recognition that we're impacting 
the environment and all of the risk associated   with this, you label the [[metacrisis]] or 
the poly crisis or the human predicament,   but they're all tied together. the system fits 
together, human behavior, energy, materials,   money, climate, the environment, governance, 
the [[economic system]], et cetera. so right now,   our entire economic imperative as nations and as 
a world is to grow the economy partially because   that's what our institutions are set up to 
do, partially because when we create money   primarily from commercial banks, increasingly from 
central banks, when governments deficit spend,   [399](https://www.youtube.com/watch?v=_P8PLHvZygo&t=399.3s)

there is no biophysical tether and the interest is 
not created. so if the interest is not created, it   creates a growth imperative for the whole system 
and we require growth. now so far the market has   dictated this growth, but suddenly there's a new 
kid on the block which is [[artificial intelligence]]   created by prior intelligence, by humans. and 
that's what we're going to talk about today.
  that's a good frame.
i'm an educator. i've recently been a college   professor. my whole role today is to inform and 
lightly inspire humans towards self-governance,   better decisions, better pathways forward. 
and my trade deals with science and facts   and systems ecology. and i'm afraid that ai 
will spell the end of what we know is true.   and on both sides we won't know what's true and 
there will be things that people can grab on   the internet that many of which are fake 
or influenced by [[artificial intelligence]]   that destroys the social discourse. so that's one 
thing i'm worried about ai. the other is will ai   accelerate [[climate change]] because 
it will make the [[superorganism]],   it would be like playing super mario or donkey 
kong or something like that and pressing the turbo   button and it makes processes more efficient 
and it's just speeds things up, which means   more carbon either directly or indirectly 
more efficiency. it feeds jevons paradox.
  another one of my worries is a lot of jobs are 
going to disappear from ai and how does that   factor into the [[superorganism]]? so it seems 
to me that ai both simultaneously makes the   [[superorganism]] hungrier and more voracious, 
but also runs the risk of killing the host   in several ways. so these are just some of 
my naive questions, but i think before we get   into [[artificial intelligence]], maybe we'll 
just start with intelligence and humans,   i think previous conversation you and i had, 
that's what differentiates us from the rest of   the biosphere is our ability to problem solve and 
use intelligence to grow the scale of our efforts   that is coupled with energy and materials 
always. so maybe you could just unpack how   you see the historical role of intelligence 
before we get to [[artificial intelligence]].
  so i might start in a slightly different place, 
which is to actually start with a couple cases of   ai that are obvious and then we'll go back 
to intelligence. the relationship between   intelligence and the [[superorganism]] itself. why 
human intelligence has made a [[superorganism]] that   is different than animal and natural intelligence 
made in terms of the nature of ecosystems   and then how [[artificial intelligence]] relates 
to those types of human intelligence,   not just individually but collectively as 
you mentioned, mediated by markets or larger   [624](https://www.youtube.com/watch?v=_P8PLHvZygo&t=624.24s)

types of human [[collective intelligence]] systems. 
and then get to what has to guide direct bind   intelligence that it is in service of something 
that is actually both sustainable and desirable.   so let's talk about just [[artificial intelligence]] 
for a moment to give a couple examples because   people have heard since, and the reason it's 
up so much since [[artificial intelligence]] was   kind of innovated in the fifties and 
some could argue precursors before that.
  the reason it is in the conversation so much 
currently is the deployment of large language   models publicly. and we're starting with gpt-3 and 
the speed of the deployment of those relative to   any other technologies. gpt-3 getting a hundred 
million users in, forget exact exactly what   it was now six weeks or something, which was 
radically faster than tiktok's adoption curve,   facebook's, youtube's, cell phones, anything 
which were already radically faster than the   [690](https://www.youtube.com/watch?v=_P8PLHvZygo&t=690.12s)

adoption curve of oil or the plow or anything 
else. so world changingly powerful technologies   at a speed of deployment, which then led to 
other companies deploying similar things,   which led to people building companies on top 
of them, which leads to irretractability.
  and so the speed of what started to happen between 
the corporate races, the adoption curves and the   dependencies is of course understandably 
changed the conversation and brought it   into the center of mainstream conversation 
where it had been only in the domain of people   paying attention to [[artificial intelligence]] or 
the risks or promises associated previously.   so when people talk about ai risk or ai 
promise of which it has a lot of both,   there's a few things about cognitive bias worth 
addressing here first, which is a topic you always   address on why people come to misunderstand the 
[[superorganism]] and get kind of choice making wrong,   get [[sensemaking]] wrong.
thank you. that means you   actually have watched-
of course i have watched and   read your things, this is why we're friends.
you're so damn busy that i'm like, hey daniel,   watch this. and you're like, oh, i 
will. but you and i have not talked   about cognitive biases, but you're right, 
i do talk about them a lot. so carry on.
  so let's take, there are clusters of 
cognitive biases that go together to   define default worldviews. and they're not 
a single cognitive bias, they're a kind of   bunch of them. and you don't even have 
to think of it as bias. it's just like,   i mean it's a strong sounding word, though it's 
true. it's a default basis for the [[sensemaking]]   and [[meaning making]] on new information people are 
likely to do first. and so one of them that i   think is really worth addressing when it comes to 
ai is a general orientation to techno optimism or   techno pessimism, which is a subset of a general 
orientation to the progress narrative. and i would   argue, and we'll not spend too long on this, so 
it actually warrants a whole big discussion.
  i would argue that there are naive versions of 
the progress narrative. capitalism is making   everything better and better. democracy 
is, science is, technology is. don't we   all like the world much better now that there's 
novocaine in antibiotics and infant mortality's   down and so many more total people are fed 
and we can go to the stars and blah, blah,   blah. obviously there are true 
parts in everything i just said,   but there is a naive version of that that does 
not factor all the costs that were associated   adequately. and there's a naive 
version of techno pessimism.   [862](https://www.youtube.com/watch?v=_P8PLHvZygo&t=862.68s)

so first on the naive version of techno 
optimism, when we look at the progress narrative,   there's so much that has progressed that if you 
want to cherry pick those metrics, you can write   lots and lots of books about however everything's 
getting better and better and nobody would want to   be alive at any other time in human history.
there's two things that the naive progress is   missing. one is the costs like [[climate change]] 
and the oceans and insects and the other is the   one time subsidy of non-renewable energy and 
inputs and the source capacity of the earth,   and those are not finite. so those are the 
two blind spots i think in that narrative.
  so we could say the costs and the sustainability 
of the story. and so if you talk about the story   of progress, particularly like the post 
modernity version of science technology   and the associated social technologies, not 
just [[physical tech]] because capitalism and   democracy and international relations are 
all kind of coordination systems that we   can call a social technology, a way of applying 
intelligence to achieving goals and doing things   of which you can consider language in early 
social technology, which it is. if you ask   the many, many indigenous cultures who were 
genocided or extincted or who have just remnants   of their culture left, or if you ask all of the 
extincted species or all of the endangered species   or all of the highly oppressed people, their 
version of the progress narrative is different.
  and just like the story of history written by 
winners or losers. but if you add all of those up,   the totality of everything that was not the 
winner's story is a critique on the progress   narrative. and so one way of thinking about it 
is that the progress narrative is there are some   things that we make better. maybe we make things 
better for an in-group relative to an out-group.   [998](https://www.youtube.com/watch?v=_P8PLHvZygo&t=998.46s)

maybe we make things better for a class relative 
to another class for a race relative to another   race for our species relative to the biosphere 
and the rest of species. or for some metrics like   whatever metric our organization is 
tasked with upregulating or gdp or   something relative to lots of other metrics 
that we are not tasked with optimizing.
  or for our generation versus 
future generations.
  exactly. short term versus long term. and so 
the question is where it is not a synergistic   satisfier where there are zero sum dynamics that 
are happening, the things that are progressing   are at the cost of which other things. and we're 
not saying that nothing could progress in this   inquiry we're saying are we calculating that 
well? and if we factor all of the stakeholders,   meaning not just the ones in the in group but all 
of the people, and not just all the people but   [1052](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1052.88s)

all the people into the future, and not just all 
the people but all the other life forms and all   of the definitions of what is worthwhile and then 
what is a meaningful life, not just gdp, then are   the things that are creating progress, actually 
creating progress across that whole scope.
  so i have two replies to that. my first is 
amen. and my second is you're advocating for   a wide boundary definition of progress 
as opposed to a narrow boundary one.
  and the definition between wide boundary and 
narrow boundaries. very related to the topic   of intelligence too. are our goals narrow 
goals or are they very inclusive goals?   if we have a goal to improve something,   for whom? is it for a small set of stakeholders? 
is it for a set of stakeholders for a small period   of time? is it measured in a small set of metrics 
where in optimizing that, in being effective at   [1109](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1109.1s)

goal achievement, we can actually externalize 
harm to a lot of other things that also matter.   and whether we're talking about technology itself 
or [[nation state]] [[decision making]] or capitalism or   whatever, we can talk about something where all of 
the problems in our world we could say have to do,   the human induced problems have to do 
do with the capacity to innovate at   goal achieving decoupled from picking 
long-term wide definition good goals.
  and that doesn't mean that there is nothing 
good about the goal. it means that the goal   achievement process is fragmented the world 
enough that, and sometimes it's not even perverse,   right? i'm going to get ahead economically and 
i'm going to fuck the environment and the people   doing slave labor in the mines and i know it and 
i'm just a sociopath. so i do it. sometimes it's   not that. sometimes it's we are, the world is 
complex, nobody can focus on the whole thing.   [1171](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1171.02s)

so we're going to make say a government that has 
different branches that focus on different things   so they can specialize in specialization and 
division of labor allow more total capacity.   and so this group is focused on [[national security]] 
of this type or focused on whatever it is, let's   say focused on if it's the un, world hunger.
now is it possible to have a solution to world   hunger - that is where now my organization 
has specific metrics, how many people are fed,   et cetera, and whether how much of the budget we 
get next year to be able to do stuff and whether   we get appointed again or elected again, have 
to do with specific metrics where it is possible   to damage the top soil. it's possible to use 
fertilizers and pesticides that will harm the   environment and cause [[dead zone|dead zones]] and oceans and 
destroy pollinators that advance harm metric. but   if we don't, there is actually no way to continue 
within that power structure. this is an example   [1232](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1232.16s)

where it's not even necessarily perverse 
in a knowing way, but the structure of it,   the institutional choice making architecture 
is such that what is being measured for and   optimized and tasked can't not prioritize some 
things over others. and that with increasing   capacity to goal achieve what is externalized 
to the goal is increasingly problematic.
  so is the narrow boundary focus versus the wide 
boundary focus, could that itself be a basic   fundamental difference between intelligence and 
wisdom? and then building on that, if an entity,   a tribe, a nation, a culture focuses on the narrow 
boundary goals, won't that out-compete a nation   tribe, a culture that focuses just on the wide 
boundary, broader multi-variable things, like   fairness or environment or future generations.
so let's come back to that definition of wisdom   and the relationship between 
wisdom and intelligence.   [1311](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1311.3s)

but let's address that we were saying earlier, 
there's a naive version of the progress narrative   or the kind of techno capital optimist narrative. 
there's also naive version of the techno pessimist   narrative. the techno pessimist narrative over 
focuses on all of the costs and externalities   who lost in that system and basically 
orients in a luddite way and is like,   no, fuck tech and new things. it was better 
before. there's various versions. one is there   was more wisdom before. and this is a dissent 
from wisdom in terms of cleverness that is   somewhere between less wise and evil.
the benefits that come from this will be   more like [[hypernormal stimuli]] that actually 
cause more net harm that we're moving towards   tipping points of catastrophic boundaries for 
the planet, et cetera. so let's just not tech   and extreme versions of that look like the amish. 
but unabomber wrote a lot of things on this topic   [1368](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1368.9s)

and they were not dumb things. he was doing a 
real critique of the advancement of technology   and then being like, how do we not destroy 
everything if we keep it on this track? now,   and we'll also see that there have been indigenous 
perspectives that wanted to keep indigenous ways,   that wanted to resist certain kinds of adoption 
of things that would, as far as technological   implementation is considered a non-embrace 
of progress. now, you were just mentioning   if tech is associated with goal achieving and 
some goals have to do with how to upregulate   the benefits of an in-group relative to 
an out-group, doesn't tech mean power?
  yes. doesn't a group that rejects some of it 
mean less power in the short term? so where   those competitive interests come, particularly 
if the other side both has tech and the mindset   to use it, does that end up meaning that 
that doesn't forward? and we can see that   [1436](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1436.1s)

when china went into tibet and it was kind 
of the end of tibetan culture as it had been,   was that because tibet was a less good culture, 
meaning provided less fulfilling life for all of   its people than china and nature was selecting 
for the truly good thing for the people or the   world or no, we can see that whether we're talking 
about genghis khan's intersection with all of the   people he intersected with or alexander 
the greats or whatever that...
  but that was tech too.
yes, yes. that those who innovated   in the technology of warfare, the technology 
of extraction, the technology of surplus,   the technology of growing populations, 
coordinating them and being able to use those   growing coordinated populations to continue 
to advance that thing relative to others.   there were cultures that might have lived in more 
population sustainability with their environment,   [1497](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1497.54s)

maybe more long-term harmony maybe said, 
let's make all our decisions factoring   seven generations ahead and they were 
just going to lose in war every time.   and so the naive techno negative direction just 
chooses to not actually influence the future. it's   going to say, i'm going to choose something 
because it seems more intrinsically right,   even if it guarantees, we actually have no 
capacity for enactment of that for the world.   and that's why i'm calling it naive.
i don't understand that.
  if...
that last thing. could you give an example?
  yeah. if someone says, i think a particular 
advancement of, i think advancement of tech   in general focuses on the upsides that are easy 
to measure. cause we intended it for that purpose   doesn't focus on all the long-term, second, 
third, fourth order downsides that are going   [1549](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1549.86s)

to happen. i don't want to do that. we want to 
have a much slower process that pays attention   to those downsides. only incorporates the things 
with the right use and guidance and incentives.   it will lose in a war, it will lose in 
an economic growth to the other cultures   that do the other thing. if you want 
to take a classic example and go back   to and in, it didn't happen exactly this way 
anywhere because it happened in such a different   ways in the fertile crescent and in india and 
whatever. but as a kind of [[thought experiment]]   illustration, the plow emerges animal 
husbandry for being able to use the plow.
  now we have to domesticate a bull, or a buffalo, 
turn it into an oxen. and that involves all the   things it does. it involves castrating it involves 
having a whip and you stand behind it to get it   to pull the plow for row cropping, whatever. so 
certain animistic cultures were like, i don't want   [1605](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1605.84s)

to do this. we'll hunt a buffalo, but we also will 
protect the baby buffaloes. we'll make sure that   our body goes into the ground to become grass 
for the future ones. we're part of a circle of   life. we believe in the spirit of the buffalo.
i can't believe in the spirit of the buffalo   and beat one all day long and do things to it 
where i wouldn't want to trade places with it.   but the culture that says now we're not going to 
do that thing is not going to get huge caloric   surplus. it's not going to grow its population 
as much. it's not going to make it through the   hard weather times as well. and so when the 
new technology emerges, those who use it,   if the technology confers competitive advantage, 
it becomes obligate because whoever doesn't use it   or use at least some comparable technologies loses 
when you get into rival risk interactions.
  let me take a brief rabbit hole side step here. 
but while it's fresh in my mind, i think this   [1660](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1660.56s)

dynamic that you're talking about now, and i know 
we're going to get to [[artificial intelligence]],   but in my public discussions, people are 
recognizing the validity of the systemic   risk that i'm discussing and that we're headed 
for at least potentially a great simplification.   simplification is the down slope of a century plus 
of intensive complexification based on energy.   but those communities, and you could 
talk about countries that simplify first   because it's the right long-term thing to do. in 
the meantime, they're going to be out-competed by   communities that don't because those communities 
will have more access to government stimulus and   money and technology and other things. but it 
almost becomes a tortoise and the hare sort   of story. had a podcast a few weeks ago with 
antonio turiel from spain and he said europe   is in much worse shape than the united states.
cause the united states has 90% of its own energy.   [1737](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1737.54s)

so europe is going to face this simplification 
first in a worse way. so the united states   has another decade. so you guys are off 
the hook. and i was thinking to myself   really because yes, the united states is mostly 
energy independent, but europe will be forced to   make these changes first and maybe they will have 
some learnings and adaptations that will serve   them in the longer run when we just ride high 
in the [[superorganism]] for a while longer. i mean   that's really a complicated speculation, but what 
do you think about all that? is that relevant?
  so this is why we talk about the   need to be able to make agreements that get 
out of the [[race to the bottom]] type dynamics,   the [[multipolar trap]], the social trap. because 
of course if anybody starts to cost resources   properly, price resources properly, meaning pay 
for what it would cost to produce that thing   [1804](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1804.38s)

renewably via recycling and whatever it is, and 
not produce pollution in the environment through   known existing technology, they would price 
themselves out of the market completely relative   to anyone else not doing that. so either everybody 
has to or nobody can, right? and whether we're   talking about pricing carbon or pricing copper or 
pricing, anything as you say, well we price things   at the cost of extraction plus a tiny margin 
defined by competition. and that was not what it   cost the earth to produce those things or the cost 
to the ecosystem and other people of doing it.
  so the proper pricing at pricing is really 
very deep to the topic of [[perverse incentive]].   and yet if we talked about how do we ensure 
that in our, and this is core to the progress   narrative, right? because the thing that 
we're advancing that drives the revenue   or the profit is the progress thing, the cost to 
the environment of that we're extracting something   [1860](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1860.3s)

unrenewably that is going to cap out, that we're 
turning it into pollution and waste on the other   side. and we're doing it for differential 
advantage of some people over other people   and affecting other species in the process.
if you were to, the stakeholders that benefit, you   get a progress narrative. the stakeholders that 
don't benefit you get a non progress narrative.   but until industrial tech, like it's 
important to get that before industrial tech,   we did extinct species, right? we over 
hunted species in an area and extincted   them. we did cut down all of the trees and 
caused desertification that then changed   local ecosystems led to flooding, ruined 
the top soil we did over farm areas. so   environmental destruction causing the end of 
civilizations is a thousands of year old story,   but it could only be...
it's just now global.
  [1919](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1919.4s)

so until we had industrial tech, we could not 
actually, we just weren't powerful enough to mess   up the entire biosphere. so how powerful we are is 
proportional to our tech because we can see that   a polar bear cannot mess up the entire biosphere 
no matter how powerful it is. corporally, right?   the thing that can mess up the entire biosphere is 
our massive [[supply chain]], technologically mediated   things starting with industrial tech. and so given 
that we are for the first time ever running up on   the end of the [[planetary boundary|planetary boundaries]] because 
we figured out how to extract stuff from the   environment much faster than it could reproduce 
and turn it into pollution and waste much faster   than it could be processed. and we're hitting 
[[planetary boundary|planetary boundaries]] on both sides of that,   on almost every type of thing you can think 
about in terms of biodiversity, in terms of   trees, in terms of fish, in terms 
of pollinators, in terms of energy,   [1975](https://www.youtube.com/watch?v=_P8PLHvZygo&t=1975.32s)

in terms of physical materials, in terms of 
the chemical pollution, [[planetary boundary]].   so the things that are getting worse are 
getting very near tipping points that were   never true before. those tipping points will make 
it to where the things that are getting better,   won't matter even for the stakeholders they're 
intended. and that's a key change to the story is,   it can no longer be that the winners can win 
at the expense of everybody else. it is that   we're actually winning at the expense of the 
life support systems of the planet writ large.   and when that cascade starts, obviously you can't 
keep winning that way, which is optimized narrow   goals at the expense of very wide values.
you've described the naive progress optimist   and the naive progress pessimist. is there 
such a thing as a progress realist?
  yes. so i am a techno optimist, meaning 
there are things that i feel hopeful about   [2032](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2032.2s)

that require figuring out new ways to do things. 
new technae both [[social tech]] and [[physical tech]].   but i'm cognizant that the market versions of 
that tech are usually not the best versions   because of the incentive landscape. in the same 
way that if facebook hadn't had an ad model, it   would've been a totally different thing, right? if 
we're just talking about the technology of being   able to do many to many communication, but you had 
something that was not a market force driving it,   could you have had something that was much 
better that was not trying to turn people into   a commodity for advertisers, which means 
behaviorally nudge them in ways that manufacture   demand and drive the emotions that manufacture 
demand, maximize engagement which causes the   externality of every young person having 
body dysmorphia and ubiquitous loneliness and   confusion about [[base reality]] and polarization.
could we have done it where rather than drive   engagement, the goal was to actually look 
at metrics of cognitive and psychological   development and interconnectedness across 
ideological divides and do that thing?
  yeah, of course.
so the same technology can be applied to   wider goals rather than more narrow goals, and you 
get a very different thing. so the base technae,   it's the technology and the motivational 
landscape that develops its application space   we have to think about together. so there are 
ways that we can repurpose existing technologies   and develop new ones, both social and physical 
technologies that can solve a lot of problems,   but it does require us getting this narrow goal 
definition versus wide goal definition. and   if intelligence guiding technology is as powerful 
as it is and actually exponentially powerful,   and we're defining intelligence here as the 
ability to achieve goals, what is it that   [2154](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2154.12s)

defines what good enough goals are that being 
able to optimize them exponentially is not   destructive? that's how you would get a progress 
narrative that is post naive and post cynical.
  in contrast, i'm probably a techno pessimist or 
at least a mild one because i see how technology   is acted as a vector for more energy and more 
climate, co2 and degradation of nature at the same   time, i think it's how we choose what technology 
we use. like a golden retriever is probably the   best technological invention ever of our species, 
even though it's really more of a co-evolution.   but you know what i mean? it's something that 
we came about and sexually selected and for   companionship and they give us the complete suite 
of evolutionary neurotransmitters for not a lot   of resource input. and there's lots of other 
technologies that are appropriate that help us   meet basic needs and give us wellbeing that don't 
destroy the biosphere. but this gets back to,   [2226](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2226.66s)

i don't think individuals chose...
wait, this is important.   the [[superorganism]] thesis that you put forward 
shows that the [[superorganism]] is oriented on a path   that does kill its host and thus itself, 
right? it does destroy the space reality,   the substrate that it depends upon. the meta 
crisis narrative that i put forward says a   similar thing is why we did this whole five series 
to kind of show the relationships. and so i would   say as long as the axioms of that thing are still 
in place, yes, i am a techno pessimist, meaning i   think that the good things that come from the new 
tech don't outweigh the fact that the new tech   is in general more often than not, accelerating 
movement towards catastrophic outcomes factoring   the totality of its effects. but this is why 
i said there is a post naive and post cynical   version in which i'm a techno optimist, but it 
requires not being on that trajectory anymore.   [2294](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2294.82s)

it requires that the technology is not being 
built by the [[superorganism]] in service of itself,   but is being built by something different 
in service of something different.
  well, in that way i'm also a techno optimist 
because after growth ends and after super-organism   is no longer in control, efficiency will 
no longer be a vector for jevons paradox   because then efficiency's going to save our 
vegetarian bacon. because as the economy is   shrinking, efficiency's going to be really 
important and innovation. just right now,   it's feeding more energy and 
stuff into the hungry maw.
  for the people who haven't heard the previous 
stuff on jevons paradox, will you do that briefly?   why efficiency? because obviously ai can 
cause radical efficiencies which can help   the environment. that's part of the story of why 
it's an environmental hope. so would you explain   [2353](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2353.5s)

why as long as jevons paradox is the case?
yeah, so humans get smarter on how we use energy   around 1.1% a year. so we get more energy 
efficient every year because we're smarter.   coal plants use less coal to generate the same 
amount of electricity. we invent solar panels,   our televisions are a little bit more energy 
efficient and our laundry machines and one   would think on the surface that that would allow 
us to use less energy. but what ends up happening   is that money savings gets spent on other things 
that use energy and writ large new innovation ends   up system-wide requiring a lot more energy.
since 1990, we've had a 36% increase in energy   efficiency at over the same time we have a 
63% increase in energy use. so as long as   growth is our goal and our cultural aspiration 
is profits in gdp, more energy efficiency will   paradoxically unfortunately result in 
more energy and environmental damage.   [2428](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2428.38s)

that's called jevons paradox. it was based 
after a 19th century economist who observed   this walter stanley jevons who observed this in 
steam engines, that steam engines wouldn't reduce   our energy use but they would scale because 
they helped everyone and were so useful.
  so let's talk about first versus second, third, 
nth order effects here. because jevons paradox   is, it's important to understand that.
daniel, what are the odds that we   actually don't get to artificial 
intelligence on this conversation?
  low.
okay,   keep going. first, second, 
third, order. go for it.
  so if we create a new technology that 
creates more energy efficiency on something,   whether it's a steam engine or a more energy 
efficient energy generation or transportation   [2481](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2481.72s)

or storage technology, the first order 
effect of it is we use less energy.   the second order effect is now that it takes less, 
now that we have more available energy and energy   costs less, there's a bunch of areas where there 
was not positive energy return, profit return that   now become profitable. and so now we open up 
a whole bunch of new industries and use more   total energy, but it's a second order effect or 
even a third order effect because it makes some   other technology possible that does that.
this is one of the asymmetries that we have   to focus on in the progress narrative is the 
progress narrative is and technology in general,   when we make a new technology, and by technology 
i mean a physical technology or even say a law   or a business to achieve a goal, we're generally 
making something that is trying to have a first   order effect on a narrow goal that is definable 
in a finite number of metrics for a small set   [2540](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2540.28s)

of stakeholders. the stakeholders are called 
the total addressable market of that thing   and very rarely is the total 
addressable market everything. and so   we're making things, whatever it is. so i'm 
using technology in the broadest sense of   human innovations towards goal achievement 
here. we're making technologies to achieve   first order effects, meaning direct effects 
for a defined goal, for a defined population.   even if we're talking about a nonprofit trying 
to do something for coral, it's still focused on   coral and not the amazon and everything else. 
and so it can optimize that at the expense of   something else through in terms of the second, 
third order effects of whatever putting that thing   through does. and so we put out a communication to 
appeal to people to do a thing politically. well,   it appeals to some people, it really disappeals 
to other people. one of the second order   [2606](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2606.76s)

effects is you just drove a counter response.
the counter response is people who think that the   thing you're benefiting harms something they care 
about and now you just up-regulated that. is that   being factored? and so the progress narrative, 
the technology narrative and all the way down to   the science narrative, and this is where we get to 
the human intelligence versus wisdom and then how   this relates to [[artificial intelligence]] is it is 
easier to think about a problem this way. here's   a definable problem. it affects these people or 
these beings. it is definable in these metrics.   we can measure the result of this and we can 
produce a direct effect to achieve it. we did,   we got progress. awesome. and the progress 
was more gdp. the progress was people who   could communicate faster. the progress was 
less dead people in the er, the progress   was less starving people. the progress was 
whatever the thing was that we were focused on,   [2658](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2658.12s)

even if it seems to be a virtuous goal.
but that same thing that you did maybe polarized   some people who are now going to do other 
stuff. that is a second order and maybe third   order effect, maybe it had an effect on supply 
chains. so the second, third, nth order effects   on a very wide number of metrics that you don't 
even know what they are to measure on a very wide   number of stakeholders that you don't even know 
how to factor is harder in kind to think about.   so it is logically, cognitively easier as 
we talk about intelligence to figure out how   to achieve a goal than it is to make sure 
that goal doesn't fuck other stuff up.
  so efficiency too has a narrow boundary and a 
wide boundary lens with which to be viewed.
  yes.
but here's one of the challenges though.   it's easy for a group of humans or a full culture 
to optimize one thing. it's very difficult to   [2719](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2719.2s)

optimize multiple things at once. multi-variable 
inputs and outputs are incredibly complex.   so optimizing dollar profits tethered to energy, 
tethered to carbon, combining technology,   materials and energy, that is a thing that was 
very easy akin to the maximum power principle.   so what do you think about that?
i think that optimization is actually   the wrong framework. when you think about 
everything that matters, you're not thinking   about optimization anymore, you're thinking 
about a different thing. so optimization is...   now let's come back to what is distinct to human 
intelligence. why did that cause a [[metacrisis]] or   a [[superorganism]]? how does ai relate to that? and 
then what it would take other than intelligence is   also relevant to ensure that the intelligence is 
in service to what it needs to be in service to.   so we're not saying that humans are the only 
intelligent thing in nature. obviously not. nobody   [2791](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2791.74s)

reasonably would say that, but there is something 
distinct about human intelligence. so how do we   define intelligence? it's fascinating. go look up 
on a bunch of encyclopedias and you'll see that   there are a lot of different schools of thought 
that define intelligence differently. some do   it in terms of formal logic and reason.
some do it in terms of pragmatics, which   is the ability to process information to achieve 
goals. some do it just kind of from a information   theoretic point of view of the ability to intake 
information, process it and make sense of it.   and all of these are related. i'm not going to try 
to formalize it right now, but i'm going to focus   on the applied side because it ends up being 
the thing that's selected for and it ends up   being the thing that wins short term goals and 
that obviously we're building ai systems for.   so there we can say intelligence is the ability to 
figure out how to achieve goals more effectively.
  [2847](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2847.3s)

or we can just say the ability to achieve goals. 
we can see that a slime mold has the ability   to achieve goals and it will figure out and 
reconfigure itself. a termite colony figures out   how to achieve goals and it reconfigures itself. 
there's some element of learning and when you   watch a chimpanzee figuring out using this stick 
versus this stick to get larvae out of a thing,   you can watch it innovating and learning how to 
achieve goals. so all of nature has intelligence.   what is unique about human intelligence 
relative to other things? relative to other,   let's just say animals. we could 
talk about plants, funguses,   all the kingdoms, but that gets harder. 
so let's just stick with other animals.   well first we realize that we can't talk about 
this properly because from an evolutionary   perspective, there were things between the other 
animals that we look at now and humans meaning   [2909](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2909.4s)

earlier hominids and so where do we 
call humans in that distinction?
  so since they're not around, we can mostly talk 
about sapiens versus everything else on the planet   that we're aware of at this point. but we can 
say that the thing we're calling human starts   before homo sapien probably with homo habilis or 
australopithecus or somewhere around there.
  we are the ninth homo. and perhaps 
the last, we don't know.
  so people might question what kind of weird 
anthropocentrism is it that would have you   say that you know that you have some kind of 
intelligence the whales don't have or the chimps   don't have or whatever. and i think it's very 
fair to say what it is like to be a whale, we   really don't know. and what about the experience 
of whaleness? the quality of the sentience of it   might even be deeper than ours, might be more 
interesting in some ways. totally. right. that's   [2971](https://www.youtube.com/watch?v=_P8PLHvZygo&t=2971.32s)

a harder problem in kind to address. but we can 
in observing their behavior, say there are types   of goal achieving that they clearly don't have 
the ability to do in a prima facie evidenced way   that we obviously have the ability to 
do. they have not figured out how to   innovate the tech that makes them work 
in all environments the way we have.
  they have not even figured out how to stay 
away from boats that are wailing boats.   and so from their most obvious evolutionary 
motive, how to figure out that thing is not a   thing that they've really done. and so we can see 
in a prima facie sense that they're not innovating   in technology and changing their environment, 
making the equivalent of an anthropocene in a   similar way, even when we see the way that 
beavers make beaver dams and changes their   environment or ants do, they do it roughly the 
same way they did it 10,000 years ago. humans   [3026](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3026.52s)

don't do it roughly the way we did it 10,000 
years ago. so we can see something unique about   humans in our behavior related to the innovation 
in technology and environment modification.
  in whales' defense, they don't have 
opposable thumbs and they're underwater.   but i'm with you. keep going.
well this is not putting whales down.   i think opposable thumbs are pretty significant 
to the story. i think there are things about the   evolution of homo sapiens that probably have to 
do with the combination of narrower hips from   uprightness that allowed us to de-weight the hands 
that allowed them to be more nimble and opposable   with the larger heads that involved neotenous 
birth. and there's this whole complex of things.   and so in no way does saying that humans have 
more of this particular kind of innovative   intelligence mean have a more meaningful right 
to exist. those are totally separate things,   [3082](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3082.44s)

right? doesn't mean have a deeper experience 
of the world. those are different things.
  so let me get back to something you just said a 
minute ago, unless this is where you were heading.   but you said intelligence is problem solving 
on route to a goal. and most animals in nature,   their goal is, well it's security and 
mating and reproduction, but energy   return is a primary goal in nature to invest 
some energy and get a higher amount back because   that enables all sorts of other optionality. 
energy, calories in nature are optionality for   organisms. so the problem with humans isn't 
the intelligence per se, it's the goal?
  i don't even want to call it a problem yet. 
i want to call it a difference. we'll get to   the problem in a minute. so i want to say 
something, because this is related here,   about modeling because human intelligence, 
all forms of intelligence have something to   [3149](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3149.76s)

do with modeling. they can take in information 
from the environment and be able to forecast   what happens if they do something enough to be 
able to inform their next choice. which choice   is more likely to achieve some future goal, 
even if the future goal's a second, right?
  so let me interrupt there. does that differentiate 
humans that if we model something that we have the   perception and ability to consider time and 
how does time factor into intelligence?
  we're not the only animal that has a relationship 
to time, but we definitely have the ability to   have abstractions on time that seem to be 
unique from what we can tell. and we also   have the ability to have abstractions on space 
and abstractions on other agents. and there's   something about the nature of abstraction itself 
that is related to the what is novel in human   intelligence, the type of recursive abstraction. 
but there's reason, to talk about modeling.   [3209](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3209.46s)

a model of reality is taking a limited amount 
of information from reality and trying to   put together a proxy of that that will inform 
us for the purpose of forecasting and ultimately   choice making, ultimately goal achieving insofar 
as the model gives us accurate enough forecasts   that it informs actions that achieve our 
goal. we consider it useful. that doesn't   mean that it is actually comprehensively, 
right. the models end up being that they're   optimizing for a narrow set of [[sensemaking]] 
just like what we were talking about before,   that we optimize for a narrow set of goals.
and the reason i bring this up is because all of   our models that can be useful, even in trying to 
understand the [[metacrisis]] and whatever themselves   can also end up blinding us to being able to 
perceive outside of those models. so when laozi   started the tao te ching with the tao that is 
speakable in words or understandable conceptually   [3277](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3277.2s)

is not the eternal tao. it was saying keep your 
sensing of [[base reality]] open and not mediated   by the model you have of reality. otherwise 
your sensing will be limited to your previous   understanding and your previous understanding 
is always smaller than the totality of what is.   i would even argue that thou shall have no false 
idols, a model of reality that says here's how   reality works, is the false idol which messes 
up our ability to be in direct perception of new   things where our previous model was inadequate.
so i say this because there are places where a   particular thing we're going to say is useful, 
but it is not the whole story and it's important   to get where it's not the whole story. so for 
instance, if we talk about energy that doesn't   include the parts about materiality and the 
parts about intelligence. and even if we talk   about those three, that's actually not the whole 
story. so it's useful, but i'm wanting to call   [3334](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3334.86s)

this out. if an animal is eating, are they eating 
only for energy? no, they're also eating for   minerals and for enzymes and for vitamins and for 
proteins and for fats, and not just fats that will   get consumed as energy, but that will become part 
of the phospholipid membrane of cells and they're   eating for materiality as well, right? and so 
it's not true that all energy is fungible. i   can't really feed an elephant meat product well, 
even though there's plenty of energy in it.
  so hold on a second. so elephants also 
have what i refer to as the trinity,   which is energy, materials and technology. they 
try to get acacia trees, they use their trunk   or some other tool. and in the acacia leaves 
are energy, the photosynthesis from the sun,   but also as you said, atoms, minerals, materials. 
so it's the same for animals as humans?
  and they're non-fungible. there's a reason why-
how so?
  [3398](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3398.734s)

i can't make certain amino acids from other 
amino acids. i can't make some minerals from   other minerals no matter how much calcium i 
get, i get no magnesium from them and i need   a certain amount of magnesium. so of course if 
i don't get enough dietary input of vitamin d,   we get rickets and die even if we get plenty 
of b vitamins and vitamin c and other things.   and so those nutrients are non-fungible to 
each other and you need all of them. you   need the whole suite that a thing needs, which 
is why there's very interesting health studies   that show people who are dying of obesity are 
actually dying of diseases of malnutrition.
  because we have a diet that has basically 
optimized for calories while stripping all   the micronutrients out of them. so you can 
be eating tens of thousands of calories a   day and actually becoming profoundly deficient 
in minerals and phytochemicals and other things   [3451](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3451.08s)

like that where then your body is wanting to 
keep eating because it's actually starving and   you continue to give it something that creates 
a neurochemical stimulus that says you ate and   satiates the hunger thing for a moment, but what 
you're actually starving for is not in that food.
  and so i don't want to oversimplify 
that energy is a part of the story.   everything we say is a part of the story, but the 
totality of the story is more complex than however   we talk about it, right? there's something 
so important and sacred about that because   what is wrong about our narrow goal achieving is 
what's upstream from that is our narrow modeling   of reality and what even equals progress. who 
is worth paying attention to? how is it all   connected? when i make a model that separates it 
all, then i can up-regulate this, harm something   else, but i don't even realize i'm harming 
something else because that's not in my model.   [3508](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3508.56s)

i don't even realize that that thing that 
i'm optimizing for isn't the actual thing   or is only a part of the whole thing.
and then probably by definition we choose models   or inputs into the models that kind of confirm 
our own built identity up until that moment.
  and, or, win in some game theoretic way. so if 
it achieves, if the model damages lots of things,   but makes me win the war, that model will 
probably win. and you notice how it's like,   okay, so we get the inquisition, we get 
the crusades, we get some fucking gnarly,   violent, cruel like figuring out 
how to optimize torture stuff   in the name of the guy who said, let he who 
has no sins among you cast the first stone   and you're like, how the fuck did we go 
from principles of forgiveness and let he   who has no sins cast the first stone into 
this version that says the inquisition is   [3573](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3573.0s)

the right way to do that. what you see 
is the interpretations - there's lots   of interpretations - the interpretations that 
lead to kill all your enemies and proselytize   win, end up winning in short term warfare. not 
because they're more true or more good, but   because they orient themselves to get rid of all 
of the enemies and have more people come into them   and have nobody ever leave the religion 
because they're afraid of hell and whatever.   and this is an example of there are models that 
win in the short term but that actually move   towards comprehensively worse realities and, or 
even self extinction. evolutionary cul-de-sacs.   and i would argue that humanity is in the process 
of pursuing evolutionary cul-de-sacs where the   things that look like they are forward are forward 
in a way that does not get to keep forwarding.   and at the heart of that is optimizing for 
narrow goals. and at the heart of that is   [3625](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3625.74s)

perceiving reality in a fragmented way and then 
getting attached to subsets of the metrics that   matter. models. which leads to us wanting 
to optimize those models and those metrics.   and now i would start to define the distinction 
between intelligence and wisdom here.   and that wisdom is related to holes and 
wholeness. intelligence is related to the   relevance realization of how do i achieve a goal?
a goal will be a narrow thing for a narrow set of   agents bound in time, modifying a fixed number 
of parameters. and so then i would say if   human intelligence distinct from other types of 
animal intelligence where we don't just have the   ability to work within a range of behaviors 
that are in capacities that are mostly built   into where the primary physical technology is our 
bodies, the animals evolved to have claws, to have   blubber for the cold, to have whatever it 
was that was a technological innovation to be   [3696](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3696.12s)

effective within its environment. and it can't 
become radically more of that thing by choice,   by its own understanding. it becomes more of 
that thing through genetic selection, which   is super slow and it doesn't control. and the 
mutation that makes the giraffe have the slightly   longer neck or the cheetah a little faster or 
whatever it is, is happening as the rest of the   environment is going through similar mutations.
so the cheetah's getting a little faster,   but so are the gazelles and there's co-selective 
pressure so that if the cheetah gets a little   faster first and eats the slower gazelle's, then 
what's left is the faster gazelle's whose genes   inbreed. so you have tiny changes happening 
across the whole system and co-op regulating   each other. so there're cemeteries in the rivalry 
that lead to the entire system still maintaining   its meta stability, not stability, meaning a fixed 
equilibrium, a homeo-dynamic, not a homeostasis,   [3748](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3748.86s)

that continues to increase in complexity over 
time, but that meta stability is the result   of that type of corporeal evolution. but then 
humans' adaptive capacity is not mostly corporeal,   it's mostly extra corporeal. you call it 
extra somatic, meaning outside of just our   body and it's both, we can use a lot of calories 
outside of our body, which started with fire.
  fire was the beginning of us being able 
to warm ourselves and all of a sudden make   new environments possible and make foods edible 
that weren't edible before. but that's calories,   right? that's extra somatic calories. then our 
ability to get more calories from the environment,   meaning gather more stuff, kill more 
things, involve the innovation of tools,   right? those spears, those stone tools allowed a 
little group of primates to take down a mastodon.   the combination of their coordination technologies 
with each other because a single person couldn't   [3814](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3814.86s)

do it and their stone tools together was able 
to do that. you get more caloric surplus. the   agricultural revolution really advanced that, 
the oil revolution really advanced that. but at   the heart of how did we figure out how to get 
oil and how to use it was intelligence. it was   this kind of recursive intelligence that figures 
out i can use this in service of my goals.
  i wonder if 30,000 years ago some neanderthal 
interbreeding with a human could ever imagine that   30,000 years later there would be a brain 
evolved like daniel schmachtenberger's.
  well check this out. tyson yunkaporta. i don't 
know if he's been on your show yet or not.
  two weeks from now.
okay, so great. then you   can follow this conversation up with some things 
i learned from tyson and other indigenous people   who hold some indigenous wisdom, knowledge have 
told me similar things that one, date back more   [3876](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3876.48s)

than the standard current archeological narrative 
of when humans knew certain shit but also with   kinds of wisdom that have definitely been lost in 
the progress narrative. and one of the things that   samantha sweetwater told me originally, and 
then tyson said something similar was that many   of the indigenous cultures had a story that 
when humans developed the first stone tools,   the [[apex predator]] of the environment, samantha's 
version was the sabertooth, came to the human.
  obviously this is a story, right? but you get 
what it would mean that the early people had   made the story and they said we were the ones 
who were taking care of and maintaining the   complex diversity of the whole system in this 
kind of [[apex predator]] role you now clearly are   and we're turning over the mantle of stewardship 
of the whole to you, your job. because now you   have the ability to destroy the whole ecosystem. 
you must be the steward of the whole thing.
  [3934](https://www.youtube.com/watch?v=_P8PLHvZygo&t=3934.92s)

and imagine that even recognizing because maybe 
those stone tools were 2 million years ago   and maybe we already had 
killed a bunch of megafauna,   extincted them, extincted some of our 
other hominoid cousins through gruesome,   kind of inter-species genocidal tribal warfare, 
destroyed some environments and already had time   to learn those mythos and be like, no, no, no.
destroyed some environments and already had time   to learn those myths and be like, no, no, no, 
we're not going to maximum power principle kale,   take everything. we're going to live in 
sustainability. think seven generations   ahead. and there was wisdom about appropriate use 
of technology and restraint 40,000 years ago.
  that was my question is, we are 
homo sapiens, our appellation,   which is wise man, but any small percentage 
of tribes or individuals or [[nation state|nation states]]   or warring clans that pursued a narrow 
boundary goal would have out-competed   those tribes with wisdom.
not any of them...
  and here we are with the [[superorganism]].
any of them above a certain threshold, right?
  what do you mean?
let's say that we had   a number of tribes in an area that had 
all developed some kind of wisdom by which   they bound intelligence. and i'm not saying we 
don't do that today. it's called law, right?
  right.
and it's supposedly also what religion is about,   the development of wisdom of what is the 
good life, what is worth pursuing and not   pursuing in which you get things like religious 
law. you're not going to work on the sabbath.   you're going to take that day to do 
different things. for instance, if you   [4043](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4043.04s)

want to think about that as a example, 
you could think about the sabbath as   an example of law binding a [[multipolar trap]], 
associated with a naive version of progress.   if you don't have a sabbath, some people will work 
seven days a week in the short term before they   burn out. they'll get ahead. they will get so much 
ahead because they'll be able to keep investing   that differential advantage in rent-seeking 
behavior that anyone who doesn't will have no   relevance to be able to guide their own lives.
and now you have a world where no one spends any   time with their kids. no one reflects on the 
religion, nobody enjoys their life. everything   sucks for everyone because somebody did that thing 
and the name of progress because they moved ahead   faster. we say, "no, no, no, you're going to 
have a day where you don't progress stuff."   that's actually the gist. you're not going to be 
focused on external progress in the world. and so   [4094](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4094.7s)

there's 27 or 29 ways in leviticus that you can 
violate the sabbath and you'll be killed if you   violate it. which seems like just whackadoodle 
religious nonsense. but if you're like, wait, no,   you never have to actually do that. if you hold 
that law that extremely, and it's everyone's like,   "all right, we're not going to with the 
sabbath. now what do i get to do that day?"
  i get to reflect rather than achieve goals, 
i reflect on what are good goals. so i get   to spend time with my family, i get to spend 
time with nature, i get to read the scripture,   i get to meditate and i don't get to achieve 
goals. i get to experience the fullness of life   outside of goal achieving. and i get to reflect on 
what goals are truly worthwhile and in doing so,   binding the [[multipolar trap]] that i don't have 
to because everybody else is rushing ahead.
  that would be an example of the way religions 
were supposed to have something like wisdom   [4144](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4144.02s)

that created something like law and 
restraint to bind naive versions of   progress in a way that was actually 
better for the whole long term.
  two comments there. one, when i was much younger, 
i had some jewish friends and i didn't ridicule   them, but i was kind of, "ha ha, you guys have 
sabbath today, i'm going to go to the arcade or   go on a boat ride or go fishing or whatever." 
but now as i'm older, everything you just said   about the good life and spending time with family 
and reading and spending time in nature and not   using the internet on a saturday or whatever 
sounds freaking wise and makes sense and   appealing to me. maybe with age and maturity, 
i'm flipping from intelligence to wisdom.
  and then the second thing, the implication, 
and maybe this is where you're heading,   is to muzzle or forestall the risk singularity 
that is coming from the [[superorganism]]. we have to   have some sabbath equivalent applied to ai.
it's not just sabbath equivalent, but almost   all of law is about restraint, right? things 
that you don't do in the presence of having   incentive to achieve narrow goals. what are the 
things that for the collective wellbeing, which   also means the capacity for your own individual 
wellbeing for all individuals into the future,   what are the things that we say we don't do? if 
you have samantha on this is a topic she'll talk   about cares a lot about, which is there's 
no definition of wisdom worth anything that   is not bound to the concept of restraint.
yeah, i don't know how our culture approaching   a biophysical wiley coyote moment 20 
or 30 or 40 years ago before all this   over leverage and different systemic 
risk, we could have added restraints,   but now our restraint would almost default create 
this rubber band snapback in the [[economic system|economic systems]].   but we can talk about that another time.
well, so this is where you end up having   the [[embedded growth obligation]] of a system,   the embedded continuity of a system, 
the kind of institutional momentum that,   a partner zach was writing something recently, a 
couple of people on the team were contributing,   and in the beginning it was talking about where 
do we find ourselves now? and it says we find   ourselves in the relationship between the life 
giving nature of the biosphere and the life giving   nature of the civilizational system.
yeah.
  and the unique point in time at which the 
latter is threatening the ongoing continuity   of the former upon which it depends. 
and that what it takes to maintain that   civilizational system will destroy the biosphere 
the civilizational system depends on, so we must   [4327](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4327.2s)

remake the civilizational system fundamentally.
we do need civilizational systems. we do need   technological systems, but we need ones that don't 
have embedded [[exponential growth]] obligations.   we do need ones that have restraint. we do need 
ones that don't optimize narrow interests at the   expense of driving arms, races and externalities. 
we do need ones where the intelligence in the   system is bound by and directed by wisdom.
right. which is the equivalent of   sabbath plus law plus emergence.
now come back to for a moment to the "ha ha,   what idiot's" reaction you had when you were 
young. i had a similar one and lots of young   people, probably even many people who were 
raised jewish have a similar one before.   they understand the full depth of it. so let's 
talk about chesterton's fences for a moment.
  never heard of that.
i don't know actually the   [4381](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4381.32s)

history of why i've got that name, but there's a 
[[thought experiment]] in philosophy that says called   chesterton's fence, which is there's a fence up 
and you think, the purpose of that fence was x.   that's no longer here. the fence is ugly and in 
the way, let's cut, let's take the fence down.   is there a chance that the purpose for the fence 
included several other things that you don't know   and you don't know that you don't know it? 
and before you take the fucking fence down,   you better make sure you actually understand why 
it was put up. and now this comes to a very deep   intuition. we were talking about biases earlier in 
the progress narrative. progressives, it's funny   how right now that is somehow associated with left 
in some weird way. but progressive and traditional   is a deep dialectic and neither one 
are supposed to be the one you choose.
  it's a dialectic. you're supposed 
to hold them in balance, right? and   [4432](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4432.62s)

very much in the same way. this is an important 
point and it relates to wholes and wisdom versus   narrow goals. narrow value sets are as 
bad as narrow goals. they're part of it.   any value that is a real value exists 
in a dynamic tension with other values.
  a dialectical one oftentimes, but other 
values. where if you optimize the one at   the expense of everything else, you optimize 
it, you get these reductoabsurdum, right?   meaning the optimization of any value by itself 
can end up looking like evil. so if i want to   optimize truthfulness and all i'm going to do is 
speak the truth all the time, then when the nazis   come and ask me are there any jews inside? i say, 
"yes,"... no! i don't - truthfulness is not the   only value at that point to the preservation of 
life and kindness and other things are." we can   even see an example where it's like if someone 
in a naive sense says, my value is honesty,   [4488](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4488.6s)

there's a bunch of places where you can see 
a person who in the name of honesty is just   an asshole. and they just say kind of mean 
things and say it's in the name of honesty.
  we can see that in the name of say kindness. 
people will lie to say flattering things and   avoid painful things. we can see that 
if you hold them in dialectical tension,   you actually get a truth that is more truthful 
and you get kindness that is more effective   because the kindness that doesn't want to 
tell the person they're an addict when they   are and nobody does or tell the emperor they 
have no clothes or say anything painful that   is necessary feedback isn't even kind. sometimes 
for that value to even understand itself fully,   there's this kind of dialectical 
relationship that helps that come about.
  now here's where i'm coming to chesterton's 
fence and then i want to hear.
  [4545](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4545.6s)

okay.
there is a dialectic between   a traditional impulse and a progressive 
impulse. the traditional impulse basically says,   i think there were a lot of wise people 
for a long time wise and smart people   who thought about some of these things more 
deeply than i have who fought and argued.
  and that the systems that made it through 
evolution that made it through, made it   through for some reasons that have some embedded 
wisdom in it that i might not understand fully   and it makes sense for me to have as my null 
hypothesis, my default trusting those systems,   they wouldn't have made it through if they 
weren't successful, didn't work. and likely   the total amount of embedded intelligence in 
them is more than i've thought about this thing.   without knowing it, that's the traditional 
intuition. the progress intuition is,   [4594](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4594.02s)

[[collective intelligence]] is advancing, built on all 
that we have known. we're discovering new things   and we're we're moving into new problem sets where 
the previous solutions could not possibly be the   right solutions because we have new problems. 
we need to have fundamentally new thinking.   obviously these are both true. now on the 
traditional side, the chesterton's fence thing is   i might have as a kid, or you might have as a kid 
thrown out the sabbath and said, that's dumb.
  before we actually understood it because 
we understood a straw manned version of it,   said it was stupid and threw it out. and so when 
we're talking about wisdom and restraint and all   like that, there is something around are we 
seeking to, because in the name of progress   there will always be something that is focused on 
restraint that seems like it's up that progress   i could get. and if i don't understand all the 
reasons for the restraint that factor second,   [4650](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4650.24s)

third, fourth, nth order effects long into the 
future in the short term, i should do the thing.   in the short term, no, of course i should 
advance the ai applied to genomics to solve   cancer without thinking through the fact that 
the fourth order effects might involve increasing   [[bioweapon|bioweapons]] capability for everyone in destruction 
of the world, so even the cancer solutions don't   matter in the course of those people's lives.
and so this is there a whole enough perspective   to be able to see how the things that are actually 
wise from a narrow perspective look stupid?
  two questions, one, is the metaphor the same 
as when i was younger, i thought those things   were stupid and now i recognize the validity of 
them. that's where we are as a culture? we are   the younger version of nate in the intelligence 
versus wisdom dynamic? i'm just speculating. i   think that's probably the case. and then 
two, you and all the people that i know,   [4711](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4711.68s)

i know a lot of smart people, you're certainly up 
there, but you also have wisdom and i don't know   as many people that have both intelligence and 
wisdom and you in my sphere rank near the top. but   is it in our genome? is it in the human behavioral 
repertoire to hold more than those single values   to hold multiple values and wide boundary views 
of the world? what do you think about that?
  we said that there is something unique about 
the types of recursive intelligence that lead   to technology innovation and the anthrop scene, 
the [[superorganism]] et cetera in human intelligence   relative to other species. let's talk about the 
genetic predisposition and what the predisposition   is actually for and the nature nurture thing a 
little bit i would say. and again, everything   i'm going to say here will be at a high level that 
is hopefully pointing in the right direction, but   totally inadequate to a deeper analysis 
of all the topics. our nature in terms of   [4793](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4793.22s)

the genetic fitness of humans, homosapiens, 
it would be fair to say that our nature   selected for being more quickly and recursively 
changeable by nurture than anything else.
  as individual humans?
  in the individual human is not the unit of 
selection and evolution, the tribe is.
  right? well, both.
the tribe or the band, the group of humans.
  both, sometimes individuals, sometimes tribes.
i don't think there is much of a case for   individual humans surviving in the 
early [[evolutionary environment]] by   themselves very well. and the behavior of them as 
individuals separate from social behavior leading,   there are certainly some animals that are 
largely solitary and they have a different set   of selection criteria than primarily social 
animals. humans are a primarily social-
  [4859](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4859.46s)

but i mean this was eo wilson, david sloan 
wilson's paper that selfish individuals out   compete within groups and cooperative groups, out 
compete selfish groups. i think both are hardwired   in us, but let's not get detracted by that.
actually what i'm saying holds with this,   the individual, there are some selection of 
an individual within a social environment,   but there's no selection of an individual 
outside of other sapiens, right?
  right.
and so the unit of selection that is   driving the dominant feature for sapiens, the unit 
of selection is a group. that's actually a really   important thing to think about. as opposed to that 
the unit of selection's an individual because we   have such an individualistically focused culture 
today, and we think in terms of individual focus   way excessively to the actual evolutionary 
fitness of an individual outside of a tribe   [4921](https://www.youtube.com/watch?v=_P8PLHvZygo&t=4921.86s)

was dead in almost every environment for most of 
history. a set of behaviors that made you alienate   the tribe was not an evolutionary strategy for 
most of the evolutionary basis of humans.
  and the problem now is our tribe is 8 billion 
strong pursuing profits tethered to carbon-