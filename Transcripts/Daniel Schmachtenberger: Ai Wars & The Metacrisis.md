---
tags: transcript
aliases:
youtube_id: g7WtcTATa2U
---

<div class="yt-container"><iframe src="https://www.youtube.com/embed/g7WtcTATa2U"></iframe></div>

as of today, we are in a war that has moved
the atomic clock closer to midnight than it has ever been. we're dealing with nukes and ai and things
like that. we could easily have the last chapter in that
book if we are not more careful about confident, wrong ideas. this is a different sort of podcast. not only because it's daniel schmadenberger,
one of the most requested guests, who by the way, i'll give an introduction to shortly, but also because today marks season 3 of the
theories of everything podcast. each episode will be far more in-depth, more
challenging, more engaging, have more energy, more effort, and more thought placed into
it than any single one of the previous episodes. [38](https://www.youtube.com/watch?v=g7WtcTATa2U&t=38.48s)

welcome to the season premiere of season 3
of the theories of everything podcast with myself, curt jaimungal. this will be a journey of a podcast with several
moments of pause, of tutelage, of reflection, of surprise appearances, even personal confessions. this is meant for you to be able to watch
and re-watch or listen and re-listen. as with every toe! podcast, there are timestamps in the description
and you can just scroll through to see the different headings, the chapter marks. i say this phrase frequently in the theories
of everything podcast, this phrase, just get wet, which comes from wheeler, [88](https://www.youtube.com/watch?v=g7WtcTATa2U&t=88.19s)

and it's about how there are these abstruse
concepts in mathematics and you're mainly supposed to get used to them, rather than attempt to bang your head against
the wall to understand it the first time through. it's generally in the re-watching that much
of the lessons are acquired and absorbed and understood. while you may be listening to this, so either
you're walking around and it's on youtube or you're listening on spotify or itunes, by the way, if you're watching on youtube,
this is on spotify and itunes, links in the description, i recommend that you at least watch it once
on youtube, so you periodically check in, [116](https://www.youtube.com/watch?v=g7WtcTATa2U&t=116.24s)

because occasionally there are equations being
referenced, visuals. i don't know about you, but much or most,
in fact, of the podcasts that i watch, i walk away with this feeling like i've learned something, but i actually haven't, and the next day if
you ask me to recall, i wouldn't be able to recall much of it. that means that they're great for being entertaining
and feeling like i'm learning something, that is the feelings of productivity, but if i actually want to deep dive into a
subject matter, it seems to fail at that, at least for myself. therefore, i'm attempting to solve that by
working with the interviewee, for instance, [148](https://www.youtube.com/watch?v=g7WtcTATa2U&t=148.13s)

we worked with daniel, to making this episode and any episode that
comes out from season 3 onward, from this point onward, to make it not only a fantastic podcast, but
perhaps in this small, humble way to evolve what a podcast could be. you may not know this, but in addition to
math and physics, my background is in filmmaking, so i know how powerful certain techniques
can be with regards to elucidation, how the difference between making a cut here
or making a cut here can be the difference between you absorbing a lesson or it being
forgotten. by the way, my name is curt jaimungal, and
this is a podcast called theories of everything, [181](https://www.youtube.com/watch?v=g7WtcTATa2U&t=181.47s)

dedicated to investigating the versicolored
terrain of theories of everything, primarily from a theoretical physics perspective, but also venturing beyond that to hopefully
understand what the heck is fundamental reality, get closer to it, can you do so, is there
a fundamental reality, is it fundamental, because even the word fundamental
has certain presumptions in it. i'm going to use almost everything from my
filmmaking background and my mathematical background to make toe the deepest dive, not
only with the guest, but we'd like it to be the deepest dive on
the subject matter that the guest is speaking about. it's so supplementary that it's best to call
it complementary, as the aim is to achieve [218](https://www.youtube.com/watch?v=g7WtcTATa2U&t=218.97s)

so much that there's no fat, there's just
meat. it's all substantive, that's the goal. now, there's some necessary infrastructure
of concepts to be explicated prior in order to gain the most from this conversation with
daniel, so i'll attempt to outline when needed. again, timestamps are in the description,
so you can go at your own pace, you can revisit sections. there will also be announcements throughout
and especially at the end of this video, so stay tuned. now, daniel schmattenberger is a systems thinker,
which is different than reductionism, primarily [245](https://www.youtube.com/watch?v=g7WtcTATa2U&t=245.59s)

in its focus. so systems thinkers think about the interactions,
the end-to-or-greater interactions, the second order or third order. and daniel, in this conversation, is constantly
referring to the interconnectivity of systems and the potential for unintended consequences. we also talk about the risks associated with
ai. we also talk about their boons, because that's
often overlooked. plenty of alarmist talk is on this subject. when talking about the risks, we're mainly
talking about its alignment or misalignment with human values. [277](https://www.youtube.com/watch?v=g7WtcTATa2U&t=277.539s)

we also talk about why each route, even if
it's aligned, isn't exactly salutary. about a third of the way through, daniel begins
to advocate for a cooperative orientation in ai development, where the focus is on ensuring that ai systems
are designed to benefit and that there are safeguards placed in, much like any other technology. you can think about this in terms of a tweet,
a recent tweet by rob miles, which says, it's not that hard to go to the moon, but
in worlds that manage it, saying that these astronauts will probably die is responded
with a detailed technical plan showing all the fail-safes, testings, and procedures that
are in place. [307](https://www.youtube.com/watch?v=g7WtcTATa2U&t=307.36s)

they're not met with, hey, wow, what an extraordinarily
speculative claim. now, this cooperative orientation resonates
with the concept of nash equilibrium. a nash equilibrium occurs when all players
choose their optimal strategy given their beliefs about other people's strategies, such that no one player can benefit from altering
their strategy. now, that was fairly abstract, so let me give
an instance. there's rock, paper, scissors, and you may
think, hey, how the heck can you choose an optimal strategy in this random game? well, that's the answer. it's actually to be random, so a one-third
chance of being rock or paper or scissors. [344](https://www.youtube.com/watch?v=g7WtcTATa2U&t=344.03s)

and you can see this because if you were to
choose, let's say, one-half chance of being rock, well, then a player can beat you one-half
of the time by choosing their strategy to be paper, and then that means that you can improve your
strategy by choosing something else. in [[game theory]], a move is something that you
do at a particular point in the game or it's a decision that you make. for instance, in this game, you can reveal
a card, you can draw a card, you can relocate a chip from one place to another. moves are the building blocks of games, and
each player makes a move individually in response [373](https://www.youtube.com/watch?v=g7WtcTATa2U&t=373.699s)

to what you do or what you don't do or in response to something that they're thinking,
a strategy, for instance. a strategy is a complete plan of action that
you employ throughout the game. a strategy is your response to all possible
situations, all situations that can be thrown your way. and by the way, that's what this upside-down,
funny-looking symbol is. this means for all in math and in logic. it's a comprehensive guide that dictates the
actions you take in response to the players you cooperate with and also the players that
you don't. a common misconception about nash equilibria
is that they result in the best possible outcome [411](https://www.youtube.com/watch?v=g7WtcTATa2U&t=411.389s)

for all players. actually, most often, they're suboptimal for
each player. they also have social inefficiencies. for instance, the infamous prisoner's dilemma. now, this relates to ai systems, and as daniel
talks about, this has significant implications for ai risk. do we know if ai systems will adopt cooperative
or uncooperative strategies? how desirable or undesirable will those outcomes
be? what about the [[nation state|nation states]] that possess
them? will it be ordered and positive, or will it
be chaotic and ataxic, like the intersection [439](https://www.youtube.com/watch?v=g7WtcTATa2U&t=439.82s)

behind me? although it's fairly ordered right now, it's
usually not like this. the stability of a nash equilibrium refers
to its robustness in face of small changes, perturbations, and payoffs or strategies. an unstable nash equilibrium can collapse
under slight perturbations, leading to shifts in player strategies, and then consequently
a new nash equilibrium. in the case of ai risk, an unstable nash equilibrium
could result in rapid and extreme harmful oscillations in ai behavior as they compete
for dominance. and by the way, this isn't including that
an ai itself may be fractionated in the way that we are as people, [478](https://www.youtube.com/watch?v=g7WtcTATa2U&t=478.599s)

with several selves inside us vying for control
in a jungian manner. generalizations also have a huge role in understanding
[[complex system|complex systems]]. so what occurs is you take some concept, and
then you list out some conditions, and then you relax some of those conditions. you abstract away. through the recognition of certain recurring
patterns, we can construct frameworks, we can hypothesize, such that hopefully it captures not only this
phenomenon, but a diverse array of phenomenon. the themes of theories of everything of this
channel is what is fundamental reality, and like i mentioned, we generally explore
that from a theoretical physics perspective, [511](https://www.youtube.com/watch?v=g7WtcTATa2U&t=511.81s)

but we also abstract out and think, well,
what is consciousness? does that arise from material? does it have a relationship to what's fundamental
reality? what about philosophy? what does that have to say in metaphysics? so that is, generalizations empower prognostication,
the discerning of patterns, and they streamline our examination of the
environment that we seem to be embedded in. now, in the realm of quantum mechanics, generalizations
take on a specific significance. now, given that we talk about probability
and uncertainty, both in these videos, which you're seeing
on screen now, and in this conversation with [543](https://www.youtube.com/watch?v=g7WtcTATa2U&t=543.19s)

daniel, thus it's fruitful to explore one powerful
generalization of probabilities that bridges classical mechanics with quantum
theory called quasi-probability distributions. born in the early days of quantum mechanics,
a quasi-probability distribution, also known as a qpd, bridges between classical
and quantum theories. there's this guy named eugene wigner, who
around 1932, published his paper on the quantum corrections
of thermodynamic equilibriums, which introduces the wigner function. what's notable here is that both position
and momentum appear in this analog to the wave function, [589](https://www.youtube.com/watch?v=g7WtcTATa2U&t=589.92s)

when ordinarily you choose to work in the
so-called momentum space, or position space, but not both. to better grasp the concept, think of quasi-probability
distributions as maps that encode quantum features into
classical-like probability distributions. whenever you hear the suffix "-like", you
should immediately be skeptical, as space-like isn't space, and time-like isn't
the same thing as time. in this instance, classical-like isn't classical. there's something called the kalmogorov axioms
of probability, and some of them are relaxed in these quasi-probability
distributions. for instance, you're allowed negative probabilities. [624](https://www.youtube.com/watch?v=g7WtcTATa2U&t=624.089s)

they also don't have to sum up to one, and
doing so with the wigner function reveals some of the more peculiar aspects
of quantum theory, like superposition and entanglement. the development of qpds expanded with the
glauber-sedartian p-representation, introduced by sedartian in 1963, and refined
by glauber and houssoumis q-representation in 1940. qpds play a crucial role in quantum tomography, which allow us to reconstruct and characterize
unknown quantum states. they also maintain their invariance under
symplectic transformations, preserving the structure of phase-space dynamics. [660](https://www.youtube.com/watch?v=g7WtcTATa2U&t=660.74s)

you can think of this as preserving the areas
of parallelograms formed by vectors in phase-space. nowadays, qpds have ventured beyond the quantum
realm, inspiring advancements in machine learning
and [[artificial intelligence]]. this is called quantum machine learning, and
while it's in its infancy, it may be that the next breakthrough in lowering
compute lies with these kernel methods and quantum
variational encoders. by leveraging qpds in place of density matrices, researchers gain the ability to study quantum
processes with reduced computational complexity. for instance, qpds have been employed to create
quantum-inspired optimization algorithms, like the quantum-inspired genetic algorithm,
qga, [700](https://www.youtube.com/watch?v=g7WtcTATa2U&t=700.21s)

which incorporates quantum superposition to
enhance search and optimization processes. quantum variational autoencoders can be used
for tasks such as quantum states compression and quantum generative
models, also quantum error mitigation. the whole point of this is that there are
new techniques being developed daily, and unlike the incremental change of the past,
there's a probability, a low one but it's non-zero, that one of these will remarkably and irrevocably
change the landscape of technology. so, generalizations are important. for instance, spin and gr, so general relativity, [735](https://www.youtube.com/watch?v=g7WtcTATa2U&t=735.13s)

is known to be the only theory that's consistent
with being lorentz invariant, having an interaction, and being spin-2, something
called spin-2. this means if you have a field and it's spin-2
and it's not free, so there's interactions, and it's lorentz invariant, then general relativity
pops out, meaning you get it as a result. now, this interacting aspect is important,
because if you have a scalar, so if you have a spin-0 field, then what happens
is it couples to the trace of the energy momentum tensor, because there's nothing else for it to couple
to, and it turns out that does reproduce newton's
law of gravity. however, as soon as you add an interacting
relativistic matter, [767](https://www.youtube.com/watch?v=g7WtcTATa2U&t=767.75s)

then you don't get that light bends. so then you think, well, let's generalize
it to spin-1, and then there are some problems there, and you think, well, let's generalize it to
spin-3 and above, and there's some no-go theorems by weinberg
there. by the way, the problem with spin-1 is that
masses will repel, for the same reason that in electromagnetism,
that if you have same charges, they repel. okay, other than just a handful of papers,
it seems like we've covered all the necessary ground, and when there's more room to be covered,
i'll cover it spasmodically throughout the [793](https://www.youtube.com/watch?v=g7WtcTATa2U&t=793.57s)

podcast. there'll be links to the papers and to the
other concepts that are explored in the description. most of the prep work for this conversation
seems to be out of the way, so now, let's introduce daniel schmattenberger. welcome, valued listeners and watchers. today, we're honored to introduce this remarkable
guest, an extraordinary, extraordinary thinker, who
transcends conventional boundaries, daniel schmattenberger. so, what are the underlying causes that everything
from nuclear war to environmental degradation, to animal rights issues, to class issues,
what do these things have in common? [824](https://www.youtube.com/watch?v=g7WtcTATa2U&t=824.95s)

as a multidisciplinary aficionado, daniel's
expertise spans [[complex system|complex systems]] theory, evolutionary dynamics, and [[existential risk]],
topics that challenge the forefront of academic exploration. seamlessly melding different fields such as
philosophy, neuroscience, and sustainability, he offers a comprehensive understanding of
our world's most pressing challenges. really, the thing we have to shift is the
economy, because perverse economic incentive is under
the whole thing. there's no way that as long as you have a
for-profit military-industrial complex as the largest block of the global economy,
that you could ever have peace. there's an anti-incentive on it as long as
there's so much money to be made with mining, [864](https://www.youtube.com/watch?v=g7WtcTATa2U&t=864.19s)

etc. like, we have to fix the nature of economic
incentives. in 2018, daniel co-founded the consilience
project, a groundbreaking initiative that aims to foster
societal-wide transformation via the synthesis of disparate domains promoting
collaboration, innovation, as well as something we used to call wisdom. today's conversation delves into ai, consciousness,
and morality, aligning with the themes of the toe podcast. it may challenge your beliefs. it'll present alternative perspectives to
the ai risk scenarios, [895](https://www.youtube.com/watch?v=g7WtcTATa2U&t=895.649s)

by also outlining the positive cases which
are often overlooked. ultimately, daniel offers a fresh outlook
on the interconnectedness of reality. say, let's get the decentralized collective
intelligence of the world having the best frameworks for understanding
the most fundamental problems as the center of the innovative focus of the
creativity of the world. so, you toe watchers, you, my name is curt
jeymungel. prepare for a captivating journey as we explore
the peerless, enthralling world of daniel schmattenberger. enjoy. i do not know with what weapons world war
iii will be fought, [944](https://www.youtube.com/watch?v=g7WtcTATa2U&t=944.959s)

but world war iv will be fought with sticks
and stones. all right, daniel, what have you been up to
in the past few years? past few years? trying to understand the unfolding global
situation and the trajectories towards existential and
global [[catastrophic risk]] in particular, the solutions to those that involve control
mechanisms that create trajectories towards dystopias, and the consideration of what a world that
is neither in the attractor basin of catastrophe or [[dystopia]] looks like, a kind of [[third attractor]]. what would it take to have a civilization
that could steward the power of exponential technology much better than we have stewarded all of
our previous technological power? what would that mean in terms of culture and
in terms of political economies and governance and things like that? so, thinking about those things and acting
on specific cases of near-term [[catastrophic risk|catastrophic risks]] that we were hoping to ameliorate and helping
with various projects on how to transition institutions to be more intelligent and things
like that. what are some of these near-term catastrophic
risks? [1040](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1040.01s)

well, as of today, we are in a war that has
moved the atomic clock closer to midnight than it has ever been. and that's a pretty obvious one. and if we were to write a book about the folly
of the history of human hubris, we would get very concerned about where we
are confident, about where we're right, where we might actually be wrong, and the consequences
of it. and as we're dealing with nukes and ai and
things like that, we could easily have the last chapter in that book. if we are not more careful about confident,
wrong ideas. so, what are all the assumptions in the way
we're navigating that particular conflict [1095](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1095.95s)

that might not be right? what are the ways we are modeling the various
sides? and what would an end state that is viable
for the world and that just at minimum doesn't go to a global [[catastrophic risk]]? that's an example. if we look at the domain of synthetic biology
as a different kind of advanced technology, [[exponential tech]], and we look at that the cost of things like
gene sequencing and then the ability to synthesize genomes, gene printing, are dropping faster
than moore's law in cost. well, open science means that the most virulent
viruses possible studied in context that have [1143](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1143.19s)

ethical review boards getting open published, then that's a situation where that knowledge
combined with near-term decentralized gene printers is decentralized [[catastrophe weapon|catastrophe weapons]]
on purpose or even accidentally. there are heaps of examples in the environmental
space if we look at our [[planetary boundary|planetary boundaries]]. [[climate change]] is the one people have the
most awareness of publicly. but if you look at the other [[planetary boundary|planetary boundaries]]
like mining pollution or chemical pollution or nitrogen [[dead zone|dead zones]] in oceans or biodiversity
loss or [[species extinction]], we've already passed certain tipping points. the question is how runaway are those effects? there was an article published a few months
ago on pfos and pfas, the fluorinated surfactants, [1198](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1198.34s)

forever chemicals as they're popularly called, that found higher than epa allowable standards
of them in rainwater all around the world, including in snowfall in antarctica because
they actually evaporate. and we're not slowing down on the production
of those in their endocrine disruptors and carcinogens. and that doesn't just affect humans, but affects
things like the entirety of ecology and soil microorganisms. it's kind of a humongous effect. so those are all examples. and i would say right now i know the topic
of our conversation today is ai. [1233](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1233.79s)

ai is both a novel example of a possible catastrophic
risk through certain types of utilization. it is also an accelerant to every category
of [[catastrophic risk]] potentially. so that one has a lot of attention at the
moment. so that makes ai different than the rest that
you've mentioned? definitely. and are you focused primarily on avoiding
disaster or moving towards something that's much more heavenly or positive, like a shangri-la? so we have an assessment called the [[metacrisis]]. there's a more popular term out there right
now, the polycrisis. we've been calling this the [[metacrisis]] since
before coming across that term. polycrisis is the idea that the global catastrophic
risk that we all need to focus on and coordinate on is not just [[climate change]] and is not just
wealth inequality and is not just kind of the breakdown of pax americana and the possibility
of war or these [[species extinction]] issues, but it's lots of things. there's lots of different global catastrophic
risks. and that they interact with each other and
they're complicated and there can even be cascades between them, right? we don't have to have [[climate change]] produce
total venusification of the earth to produce a global [[catastrophic risk]]. it just has to increase the likelihood of
extreme weather events in an area. [1318](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1318.799s)

and we've already seen that happening. statistics on that seem quite clear. and it's not just total [[climate change]], deforestation
affecting local transpiration and heat in an area can have an effect on and total amount
of pavement laid and whatever can have an effect on extreme weather events. but extreme weather events, i mean, we saw
what happened to australia a couple years ago when a significant percentage of a whole
continent burned in a way that we don't have near-term historical precedent for. we saw the way that droughts affected. the migration that led to the whole syrian
conflict that got very close to a much larger-scale [1354](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1354.97s)

conflict. the australia situation happened to hit a
low population density area, but there are plenty of high population density areas that
are getting very near the temperatures that create total crop failures, whether we're
talking about india, pakistan, bangladesh, nigeria, iran. and so if you have massive human migration,
the un currently predicts hundreds of millions of climate-mediated migrants in the next decade
and a half, then it's pretty easy under those situations to have resource wars. and those can hit existing political fault
lines and then technological amplification. and so in the past, we obviously had a lot
less people. [1398](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1398.22s)

we only had half a billion people for the
entirety of the history of the world until the [[industrial revolution]]. and then with the green revolution and nitrogen
fertilizer and oil and like that, we went from half a billion people to 8 billion people
overnight in historical timelines. and we went from those people mostly living
on local subsistence to almost all living on dependent upon very complicated supply
chains now that are six-continent-mediated [[supply chain|supply chains]]. so that means that there's radically more
fragility in the life support systems so that local catastrophes can turn to breakdowns
of [[supply chain|supply chains]], economic effects, et cetera, that affect people very widely. [1443](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1443.882s)

so polycrisis kind of looking at all that,
[[metacrisis]] adds looking at the underlying drivers of all of them. why do we have all of these issues? and what would it take to solve them, not
just on a point-by-point basis, but to solve the underlying basis? so we can see that all of these have to do
with [[coordination failure|coordination failures]]. we can see that underneath all of them, there
are things like perverse economic interests. if the cost of the environmental pollution
to clean it up was something where in the process of the corporation selling the pfas
as a surfactant for waterproofing clothes or whatever, it also had to pay for the cost
to clean up its effect in the environment [1481](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1481.59s)

or the oil cost had to clean up the effect
on the environment. so you didn't have the [[perverse incentive]]
to externalize costs onto nature's balance sheet, which nobody enforces. obviously, we'd have none of those environmental
issues, right? that would be a totally different situation. so can we address [[perverse incentive]] writ
large that would require fundamental changes in what we think of as economy and how we
enact that, so political economy? so we think about those things. so i would say with the [[metacrisis]] assessment,
we would say that we're in a very novel position with regard to [[catastrophic risk]], global catastrophic
risk, because until world war ii, there was no technology big enough to cause a global
[[catastrophic risk]] as a result of dumb human choices or human failure quickly. and then with the bomb, there was. it was the beginning. and that's a moment ago in evolutionary time,
right? and if we reverse back a little bit before
the bomb, until the [[industrial revolution]], we didn't have any technology that could have
caused global [[catastrophic risk]] even cumulatively. the industrial technology, extracting stuff
from nature and turning it into human stuff for a little while before turning it into
pollution and trash, where we're extracting stuff from nature in ways that destroy the
environment faster than nature can replenish [1559](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1559.73s)

it and turning it into trash and pollution
faster than it can be processed and doing exponentially more of that because it's coupled
to an economy that requires [[exponential growth]] to keep up with interest. that creates an [[existential risk]]. it creates a [[catastrophic risk]] within about
a few centuries of cumulative effects. and we're basically at that few-century point. and so that's very new. all of our historical systems for that, you
know, our historical systems for thinking about governance in the world didn't have
to deal with those effects. we could just kind of think about the world
as inexhaustible. [1594](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1594.23s)

and then, of course, when we got the bomb,
we're like, all right, this is the first technology that rather than racing to implement,
we have to ensure that no one ever uses. in all previous technologies, there was a
race to implement it. it was a very different situation. but since that time, a lot more catastrophic
technologies have emerged, catastrophic technologies in terms of applications of ai and synthetic
biology and cyber and various things that are way easier to build the nukes and way
harder to control. when you have many actors that have access
to many different types of catastrophic technology that can't be monitored, you don't get mutually
assured destruction and those types of safeties. [1635](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1635.179s)

so we'd say that we're in a situation where
the [[catastrophic risk]] landscape is novel. nothing in history has been anything like
it. and the current trajectory doesn't look awesome
for making it through. what it would take to make it through actually
requires change to those underlying coordination structures of humanity very deeply. so i don't see a model where we do make it
through those. it doesn't also become a whole lot more awesome. and that's why we say the only other example
is to control for catastrophes, you can try to put very strong control provisions. okay, so now, unlike in the past, people could
or pretty soon have gene drives where they [1672](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1672.35s)

could build pandemic weapons in their basement
or drone weapons where they could take out infrastructure targets or now ai weapons even
easier. we can't let that happen. so we need ubiquitous surveillance to know
what everybody's doing in their basement, because if we don't, then the world is unacceptably
fragile. so we can see catastrophes or dystopias, right? because most versions of ubiquitous surveillance
are pretty terrible. and so if you can control decentralized action,
if you don't control decentralized action, the current decentralized action is moving
towards [[planetary boundary|planetary boundaries]] and conflict and etc. [1711](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1711.01s)

if you control it, then who, what are the
checks and balances on that control? sorry, what do you mean control decentralized
actions? so when we look at what is what causes catastrophe,
so when we're talking about environmental issues, there's not one group that is taking
all the fish out of the ocean, or causing [[species extinction]] or doing all the pollution,
there's a decentralized incentive that lots of companies share to do those things. so nobody's intentionally trying to remove
all the fish from the ocean, they're trying to meet an economic incentive that they have
that's associated with fishing, but the cumulative effect of that is overfishing the ocean, right? so if you try to – if there's a decentralized
set of activity where the lack of coordination [1753](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1753.28s)

of everybody doing that, everybody pursuing
their own near-term optimum creates a shitty term global minimum for everybody, right? a long-term bad outcome for everybody. if you try to create some centralized control
against that, that's a lot of centralized power. and where are the checks and balances on that
power? otherwise, how do you create decentralized
coordination? and similarly, if you're looking at things
like in an age where terrorism can get exponential technologies and you don't want exponentially
empowered terrorism with [[catastrophe weapon|catastrophe weapons]] for everyone, to be able to see what's being
developed ahead of time, does that look like [1795](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1795.64s)

a degree of surveillance that nobody wants
to be able to control those things not happening, right? do you know what i mean? so if you – how to prevent the catastrophes
if the catastrophes are currently the result of the human motivational landscape in a decentralized
way, if the solution is a centralized method powerful enough to do it, where are the checks
and balances on that power? so a future that is neither cascading catastrophes
nor controlled dystopias is the one that we're interested in. and so, yes, i would say the whole focus is
that this is now ai comes back into the topic because a lot of people see possibilities
for a very pro-topian future with ai where [1836](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1836.88s)

it can help solve coordination issues and
solve lots of resource allocation issues. it also – and it can – it can also make
lots of things. the catastrophe is worse and [[dystopia]] is worse. it's actually kind of unique in being able
to make both of those things more powerful. can you explain what you mean when you say
that the negative externalities are coupled to an economy that depends on exponential
growth? yeah. so it's – if you think about it just in
a first principle way, the idea is supposed to be something like there are real goods
and services that people want that improve their life that we care about. [1881](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1881.899s)

and so the services might not be physical
goods directly. they might be things humans are doing, but
they still depend upon lots of goods, right? if you are going to provide a consultation
over a zoom meeting, you have to have laptops and satellites and power lines and mining
and all those things. so you can't separate the service industry
from the goods industry. so there's physical stuff that we want. and to mediate the access to that and the
exchange of it, we think about it through a currency. so it's supposed to be that there's this physical
stuff and the currency is a way of being able to mediate the incentives and exchange
of it. [1921](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1921.769s)

but the currency starts to gain its own physics,
right? so we make a currency that has no intrinsic
value, that is just representative of any kind of value we could want. but the moment we do something like interest,
where we're now exponentiating the monetary supply independent of an actual automatic
growth of goods or services to not debase the value of the currency, you have to also
exponentiate the total amount of goods and services. and everybody's seen how compounding interest
works, right? because you have a particular amount of interest
and then you have interest on that amount of interest, so you do get an exponential
curve. [1962](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1962.85s)

obviously that's just the beginning. financial services as a whole and all of the
dynamics where you have money making on money mean that you expand the monetary supply on
an [[exponential curve]], which was based on the idea that there is a natural [[exponential curve]]
of population anyways and there is a natural growth of goods and services correlated. but that was true at an early part of the
curve that was supposed to be an s curve, right? so we have an [[exponential curve]] that inflects,
goes into an x curve, but we don't have the s curve part of the financial system planned. the financial system has to keep doing exponential
growth or it breaks. [1999](https://www.youtube.com/watch?v=g7WtcTATa2U&t=1999.89s)

and not only is that key to the financial
system, because what does it mean to have a financial system without interest? say it's a very deeply different system. that formalizing that was also key to our
solution to not have world war iii, right? the history of the world in terms of war does
not look great, that the major empires and major nations don't stay out of violent conflict
with each other very long. and world war i was supposed to be the war
that ended all wars, but it wasn't. we had world war ii. now this one really has to be the war that
ends all major superpower wars because of the bomb. [2043](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2043.251s)

we can't do that again. and the primary basis of wars, one of the
primary bases had been resources, which was a particular empire wanted to grow and get
more stuff. and that meant taking it from somebody else. and so the idea of if we could exponentially
grow global gdp, everybody could have more without taking each other's stuff. it's so highly positive sum that we don't
have to go zero sum on war. so the whole post-world war ii banking system,
the [[bretton woods]] monetary system, et cetera, was part of the how do we not have world war
along with mutual assured destruction, the un and other international intergovernmental
organizations. [2080](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2080.01s)

but that let's exponentially grow the monetary
system also meant that if you have a whole bunch more dollars and you don't have more
goods and services, the dollars become worth less and it's just inflation and debasing
the currency. so now you have an artificial incentive to
keep growing the physical economy, which also means that the [[materials economy]] has to have
an exponential amount of nature getting turned from nature into stuff, into trash and pollution
in a linear [[materials economy]]. and you don't get to exponentially do that
on the finite biosphere forever. so the economy is tied to interest and that's
at the root of this of what you just explained, not at the root of every catastrophe. interest is the beginning of what all of the
financial services do, but there's an [[embedded growth obligation]] of which interest is the
first thing you can see on the [[economic system]]. the [[embedded growth obligation]] that creates
exponentiation of it tied to the physical world where [[exponential curve|exponential curves]] don't get to
run forever is one of the problems. there are a handful. this is where we're thinking [[metacrisis]]. what are the underlying issues? this is one of the underlying issues. there's quite a few other ones that we can
look at to say if we really want to address the issues, we have to address it at this
level. what's the issue with transitioning from something
that's exponential to sub-exponential when [2160](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2160.14s)

it comes to the economy? what's the issue with it? well i mean, there's a bunch of ways we could
go. there is an old refrain from the hippie days
that seems very obvious i think as soon as anyone thinks about it, which is that you
can't run an [[exponential growth]] system on a finite planet forever. that seems kind of obvious and intuitive. because it's so obvious and intuitive, there's
a lot of counters to it. one counter is we're not going to run it on
a finite planet forever. we're going to become an interplanetary species,
mine asteroids, ship our ways to the sun, [2204](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2204.25s)

blah, blah, blah. i don't think that we are anywhere near close,
independent of the ethical or aesthetic argument on if us obliterating our planet's carrying
capacity and then exporting that to the rest of the universe is a good or lovely idea or
not, independent of that argument. the timelines by which that could actually
meet the humanity [[superorganism]] growing needs relative to the timelines where this thing
starts failing don't work. so that's not an answer. that said, the attempt to even try to get
there quicker is a utilization of resources here that is speeding up the breakdown here
faster than it is providing alternatives. the other answer people have to why there
could be [[exponential growth]] forever is because digital, right? that more and more money is a result of software
being created, a result of digital entertainment being created, and that there's a lot less
physical impact of that. and so we can keep growing digital goods because
it doesn't affect the physical plan and physical [[supply chain]]. so we can keep the [[exponential growth]] up forever. that's very much the kind of silicon valley
take on it. of course, that has an effect. it does not solve the problem. and it's pretty straightforward to see why,
which is for – let's go ahead and say [2296](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2296.76s)

software in particular. does software have to run on hardware where
the computer systems and server banks and satellites and et cetera require massive mining,
which also requires a financial system and police and courts to maintain the entire cybernetic
system that runs all that? yes, it does. does a lot more compute require more of that,
more atoms, adjacent services, energy? yes. but also, for us to consider software valuable,
it's either because we're engaging with what's – we're engaging with what it's doing
directly. so that's the case in entertainment or education
or something. [2348](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2348.03s)

but then it is interfacing with the finite
resource called human attention, of which there is a finite amount. or because we're not necessarily being entertained
or educated or engaging with it, but it's doing something for us to – again, to consider
valuable. it is doing something to the physical world. so the software is engaging, say, [[supply chain]]
optimization or new modeling for how to make better transistors or something like that. but then it's still moving atoms around using
energy and physical space, which is a finite resource. if it is not either affecting the physical
world or affecting our attention, why would [2389](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2389.68s)

we value it? we don't. so it still bottoms out on finite resources. so i can't just keep producing an infinite
amount of software where you get more and more content that nobody has time to watch
and more and more designs for physical things that we don't have physical atoms for or energy
for. you get a diminishing return on the value
of it if it's not coupled to things that are finite. the value of it is in modulating things that
are also finite. so there's a coupling coefficient there. [2421](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2421.31s)

you still don't get an [[exponential curve]]. so what we just did is say the old hippie
refrain, you can't run an exponential economy on a finite planet forever. the alt, the counters to it don't hold. what about mind uploading or some computer
brain interface to allow us to have more attention exponentially? yeah, so it's another, it's kind of, that's
almost like the hybrid of the other two, right? which is get beyond this planet and do it
more digitally. so get beyond this brain and become digital
gods in the singularity universe. i again, i think there are pretty interesting
arguments we can have ethically, aesthetically, [2470](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2470.94s)

and epistemically about why that is neither
possible nor desirable. but independent of those, i don't think it's
anywhere close. and same like the multi-planetary species,
it is nowhere near close enough to address any of the timelines we have by which economy
has to change because the growth imperative on the economy as is, is moving us towards
catastrophic tipping points. so if it were close, would that change your
assessment or you still have other issues? if it were close, then we would have to say,
first, that is implying that we have a good reason to think that it's possible, right? that it's possible to, and that means all
the axioms that consciousness is substrate independent, that the consciousness is purely
a function of compute, strong computationalism [2538](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2538.22s)

holds, that we could map the states of the
brain and or if we believe in embodied cognition, the physiology adequately to represent that
informational system on some other substrate, that that could operate with an amount of
energy that is and substrate that's possible, blah, blah, blah. so first we have to believe that's possible. i would question literally every one of the
axioms or assumptions i just said. we're going to get to that. we would say, is it desirable and how do we
know? how ahead of time? and now you get something very much like how
do i know that the ai is sentient, which for [2583](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2583.68s)

the most part on all ai risk topics, whether
it's sentient or not is irrelevant. whether it does stuff is all that matters. but how do you tell if it's sentient and all
of the chalmers, p-zombie questions or whatever are actually really hard? because what we're asking is how can we use
third-person observation to infer something about the nature of first-person given the
ontological difference between them? so how would we know that that feature is
desirable? are there safe to fail tests and what would
we have to test to know it to start making that conversion? but i don't think we have to answer any of
those questions because i don't think that [2628](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2628.36s)

anybody that is working on whole brain emulation
thinks that we are close enough that it would address the timeline of the economy issues
that you're addressing. let's attempt to address one of the questions
about substrate independence. what are your views? is consciousness something that our biological
brains do or that requires a development from an embryonic stage? whatever it is that produced us, there's something
special about us or animals or is it something that can be transferred or started up, booted
up from scratch into what's not us, like decidedly not us, a computer? okay. so this is now much more a proper theory of
everything conversation than the topic that we intended for the day, which is about ai
risk. so what i will do is say briefly the conclusion
of my thoughts on this without actually going into it in depth, but i would be happy to
explore that at some point. i think that how i come to my position on
it to try to do a kind of proper construction takes a while. so briefly i'll say... i'm not a strong computationalist, meaning
don't believe that mind, universe, sentience, qualia is purely a function of computation. i am not an emergent physicalist that believes
that consciousness is an epiphenomenon of [2731](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2731.109s)

non-conscious physics that in the same way
that we have weak emergence, more of a particular property through certain kind of combinatorics
or strong emergence, new properties emerging out of some type of interaction
where that hadn't occurred before, like a cell respirating or none of the molecules
that make it up respirate. i believe in weak emergence. that happens all the time. you get more of certain qualities. it happens in metallurgy when you combine
metals where the combined tensile strength or shearing strength or whatever is more than
you would expect as a result of the nature of how the molecular lattices form. [2772](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2772.78s)

you get more of a thing of the same type. i believe in strong emergence, which is you
get new types of things you didn't have before, like respiration and replication out of parts,
none of which do that. but those are all still in the domain of third
person accessible things. the idea of radical emergence, that you get
the emergence of first person out of third person or third person out of first person,
which is idealism on one side and physicalism on the other, i don't buy either of. i think that idealism and physicalism are
similar types of reductionism where they both take certain ontological assumptions to bootload
their epistemology and then get self-referential dynamics. so i don't think that if a computational system
gets advanced enough, automatically consciousness pops out of it. that's one. two, i do think that the process of a system
self-organizing is fundamentally connected to the nature of experience of selfness. and things that are being designed and are
not self-organizing, where the boundary between the system and its environment that exchanges
energy and information and matter across the boundary is a auto-poetic process. i do believe that's fundamental to the nature
of things that have self-other recognition. and on substrate independence, i do believe
that carbon and silicon are different in pretty [2864](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2864.819s)

fundamental ways that don't orient to the
same types of possibilities. and i think that that's actually pretty important
to the ai risk argument. but – so i'll just go ahead and say those
things. i also don't think – i believe that embodied
cognition in the damasio sense is important and that a scan of purely brain states is
insufficient. i also don't think that a scan of brain states
is possible even in theory. and – sorry to interrupt. i know you said you don't believe it's possible. what if it is? [2915](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2915.359s)

and you're able to scan your brain state and
body state. so we take into account the embodied cognition. sure. so
i think that – okay. it's not simply a matter of scanning the brain
state. we need to scan the rest of the central nervous
system. no, we also have to get the peripheral nervous
system. no, we have to get the endocrine system. no, all of the cells have the production of
and reception of neuroendocrine-type things. we have to scan the whole thing. [2952](https://www.youtube.com/watch?v=g7WtcTATa2U&t=2952.9s)

does that then extend to the microbiome, virome,
et cetera? i would argue yes. does it then extend to the environment? i would argue yes. where does that stop its extension is actually
a very important question. so i would take the embodied cognition a step
further. the other thing is stuart kaufman's arguments
about quantum amplification to the mesoscopic level that quantum mechanical events don't
just fully cancel themselves out. at the subatomic level and at the level of
brains, everything that is happening is straightforwardly classical, but that there is quantum mechanical,
i.e., some fundamental kind of indeterminism [3004](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3004.68s)

built in phenomena. they end up affecting what happens at the
level of molecules. now then, one can say, well, does that just
mean we have to add a certain amount of a random function in, or is there something
else? this is a big rabbit hole, i would say, for
another time, because then you get into quantum entanglement and coherence, so you get something
that is neither perfectly random, meaning without pattern. you get a born distribution even on a single
one, but it's also not deterministic or with hidden variables. so do i think that what's happening in the
brain-body system is not purely deterministic, [3047](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3047.349s)

and also as a result of that means you could
not measure or scan it even in principle, in a kind of heisenberg sense? yes, i think that. have you heard of david wolpert and his "limits
on inference machines"? i have not studied his work. okay. well, anyway, he echoes something similar,
which says that you can't have laplace's demon even in a classical world. you can't have laplace's demon. so let me talk about the economy. [3079](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3079.43s)

which only on your podcast would happen. why is it that if somehow this exponential
curve starts to get to where the s is, the top of the s, that the halting or the slowing
down of the economy is something that's so catastrophic and calamitous, rather than something
that would mutate? and if we need to just, at that point, as
it starts to slow down, we make minor changes here and there? is this something that's entirely new? like, will they all come crashing down? um, okay. so... [3113](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3113.6s)

let me make the question clear. it sounds like, look, the economy is tied
to [[exponential growth]]. we can't grow exponentially. virtually no one believes that. so at some point, and let's just imagine it's
three decades, just to give some numbers. so at some point, three decades from now,
this [[exponential curve]] for all of the economy will start to show its legs and start to weaken
and we'll see that it's nearing the s part. so what? does that mean that there's been fire in the
streets, that the buildings don't work, that the water doesn't run anymore? [3142](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3142.329s)

like, what will happen? okay. so people often make jokes about physicists
in particular starting to look at biology and language and society and modeling in particularly
funny reductionist ways because they try to map the entire economy through the second
law of thermodynamics or something like that. and because what we're really talking about
is the maximally complex and anterocomplex thing and embedded complexity we can because
we're talking about all of human motives. and how do humans respond to the idea that
there is fundamentally limits on the growth possible to them or there's less stuff possible
for them or that there – whether it's issues that are associated with environmental extraction. [3200](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3200.19s)

so here's one of the classic challenges is
that the problems, the [[catastrophic risk|catastrophic risks]], many of them in the environmental category
are the result of cumulative action long-term where the upsides are the result of individual
action short-term. and the asymmetry between those is particularly
problematic. that's why you get this collective choice-making
challenge, meaning if i cut down a tree for timber, i don't obviously perceive a change
to the atmosphere or to the climate or to watersheds or to anything. but my bank account goes up through being
able to sell that lumber immediately. and the same is true if i fish or if i do
anything like that. but when you run the kantian categorical imperative
across it and you have the movement from half [3246](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3246.98s)

a billion people doing it to pre-industrial
revolution to 8 billion and you have something like in the industrial world 100x resource
per capita consumption just calorically measured today than at the beginning of the industrial
revolution. then you start realizing, okay, well, the
cumulative effects of that don't work. they break the planet, and they start creating
tipping points that auto-propagate in the wrong direction. but no individual person or even local area
doing the thing recognizes their action as driving that downside, and how do you get
global enforcement of the thing? and if you don't get global enforcement, why
should anyone let themselves be curtailed when other people aren't being curtailed? [3292](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3292.02s)

and that'll give them game theoretic advantage. so this is actually, there's a handful of
asymmetries that are important to understand with regard to risk. all right. we've covered plenty so far. and so it's fruitful to have a brief summary. we've talked about the faulty foundation of
our monetary system. daniel argues that post-world war ii especially,
our [[economic system]] has not only encouraged but been dependent on exponential monetary
growth, and this can't continually occur. we've also talked about the digital escape
plan and how this is an illusion, at least [3319](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3319.92s)

in daniel's eye. he believes that digital growth has physical
costs because their hardware, their human attention limits their finite resources, linear
resources as he calls them, though i have my issues with the term linear resource because
technically anything is linear when measured against itself. we've also talked about how moving to mars
won't save us, us being civilization. daniel believes that the idea of becoming
an interplanetary species to escape resource limitations is unrealistic, perhaps even ethically
questionable. we've also talked about how mind uploading
is not what it's cracked up to be. it may not occur. [3354](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3354.49s)

and even if it does, it's not the answer because
it's either unfeasible, but even if it's feasible, daniel believes it to be undesirable. another resource as we expand our digital
footprint is the privacy of our digital resources. you can see this being recognized even by
openai as they recently announced an incognito mode. and this is where our sponsor comes in. do you ever get the feeling that your internet
provider knows more about you than your own mother? it's like they're in your head. they can predict your next move. [3380](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3380.2s)

when i'm researching complicated physics topics
or checking the latest news or just in general what i want privacy on, i don't want to have
to go and research which vpn is best. i don't want to be bothered by that. well, i and you can put those fears to rest
with private internet access. if you have a vpn provider that's got your
back with over 30 million downloads, they're the real deal when it comes to keeping your
online activity private. and they've got apps for every operating system. you can protect 10 of your devices at once,
even if you're unfortunate enough like me to love windows. and if you're worried about strange items
popping up in your search history, don't worry. [3412](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3412.74s)

i'm not judging. private internet access comes in here as they
encrypt your connection. they encrypt your ip address so your isp doesn't
have access to those strange items in your history. they make you a ghost online. it's like batman's cave before you're browsing
history. with private internet access, you can keep
your odd internet searches, let's say, on the down low. it's like having your own personal confessional
booth, except you never need to talk to a priest. [3441](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3441.89s)

so why wait? head over to piavpn.com slash toe, t-o-e,
and get yourself an 82, an 82% discount. that's less than the price of a coffee per
month. and let's face it, your online privacy is
worth way more than a latte. that's piavpn.com slash t-o-e now and get
the protection you deserve. brilliance is a place where there are bite-sized
interactive learning experiences for science, engineering, and mathematics. [[artificial intelligence]] in its current form
uses machine learning, which uses neural nets, often at least. and there are several courses on brilliance's
website teaching you the concepts underlying [3475](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3475.799s)

neural nets and computation in an extremely
intuitive manner that's interactive, which is unlike almost any of the tutorials
out there. they quiz you. i personally took the course on random variable
distributions and knowledge and uncertainty because i wanted to learn more about entropy, especially as there may be a video coming
out on entropy, as well as you can learn group theory on their website, which underlies physics, that is su3 cross
su2 cross u1 is the standard model gauge group. visit brilliant.org slash toe, t-o-e, to get
20% off your annual premium subscription. as usual, i recommend you don't stop before
four lessons. [3513](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3513.2s)

you have to just get wet. you have to try it out. i think you'll be greatly surprised at the
ease at which you can now comprehend subjects you previously had a difficult time grokking. the bad is the material from which the good
may learn. so this is actually there's a handful of asymmetries
that are important to understand with regard to risk. one is this one that i'm saying, which is
you have risks that are the result of long-term cumulative action, but that you actually have to change individual
action because of that. [3549](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3549.68s)

but the upside, the benefit, the individual
making that action realizes the benefit directly. and so this is a classic tragedy of the commons
type issue, right? the tragedy of the commons at a not just local
scales, but at global scales. and some of the other asymmetries are particularly
important is people who focus on the upside, who focus on opportunity, do better game theoretically for the most
part than people who focus on risk when it comes to new technologies and advancement
and progress in general. because if someone says, hey, we thought vioxx
or ddt or any number of things were good ideas, they ended up – or leaded gasoline, they
ended up being really bad later. we want to do really good long-term safety
testing regarding first, second, third order [3597](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3597.75s)

effects of this. they're going to spend a lot of money and
knock it first to market and then probably decide the whole thing wasn't a good idea
at all. or if they do decide how to do a safe version,
it takes them a very long time. the person says, no, the risks aren't that
bad. let me show you. does a bullshit job of risk analysis as a
box checking process and then really emphasizes the upsides is going to get first mover advantage,
make all the money. they will privatize the gains, socialize the
losses. then when the problems get revealed a long
time later and are unfixable, that will have [3627](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3627.99s)

already happened. so these are just examples of some of the
kind of choice-making asymmetries that are significant to understand the situation. i only partly answered your question. sure. are you having in mind a particular corporation
currently? totally. not a particular corporation, but a particularly
important consideration in the entire topic. one view is that google is not coming out
with something that's competitive. like bart is not competitive. [3661](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3661.28s)

i think even google would admit that. and so one view is that, well, they're highly
testing. another one, i've spoken to some people behind
the scenes and they say google doesn't have anything. they don't have anything like chat gpt. it's bs when they say so. even openai doesn't know why chat gpt works,
like gpt-4 works as well as it does. they just threw so much data at it and it
was a surprise to them. and in some ways they got lucky. so do you see what's happening right now between
microsoft and google as google is actually [3685](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3685.67s)

the more cautious one and microsoft is the
more brazen one and perhaps should be a bit more circumspect? i have heard a lot of things about the choices
that both companies made to not release stuff and safety studies that they did and then
what influenced the choices to release stuff inside of microsoft and openai and how google's
handling it. i don't know that these stories are the totality
of information on it that's relevant. do i think that economic forcing functions
have played a role in something that affected the safety analysis totally? do i think that that is an unacceptably dumb
thing on a topic that has this level of safety risk associated totally? [3747](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3747.529s)

so now getting into what is unique about ai
risk, right? what is unique about it relative to all other
risks? do we – people are saying things like we
need an fda for ai right now, which i would argue is both true and a profoundly inadequate
analogy because a single new chemical that comes out is not an agent. it is not a dynamic thing that continues to
respond differently to huge numbers of new unpredictable stimuli. so how you do the assessment of the phase
space of possible things is totally different. it would probably be good to dive into what
is the risk space of ai, why is it unique, and how given all of the differences of concern,
how to framework and think about that properly. [3794](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3794.88s)

what else is unique about it? why can't we have an fda or a un version of
an fda for ai? and when i say un, sorry, what i mean is global. yeah. well, obviously, you bring up un and say global
because you have to have global regulation on something like that, right? in the same way that when people talk about
climate regulation, if we were – if any country, if any group of countries was to
try to price carbon properly, meaning what does it take to renewably produce those hydrocarbons
and what does it take to in real time fix all of the effects, both sequester the co2,
clean up the oil spills, whatever it is. [3848](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3848.9s)

the price of oil would become high enough
with those costs internalized that oil then as an input to industries, literally every
industry would be non-profitable. and so even if any country was to try to make
some steps in the direction of internalizing cost and other ones didn't, then the other
ones who continue to externalize their costs get so much further ahead in terms of gdp
that can be applied to militaries and surplus of many different kinds and advancing exponential
tech that insofar as those are also competing entities for world resources and control,
it's not a viable thing. this is true for ai as well, and this then
starts to hit this other issue, which is if you can't regulate something on a purely national
level because it's not just how does it affect the people in the nation, but how does it
affect the nation's capability to interact [3902](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3902.34s)

with other nations. now you get to the – and so the creation
of the un was kind of the recognition in the existence in the emergence of world war ii
that nation-state governance alone was not adequate to prevent world war. obviously, that's why the league of nations
came after world war i and it was not strong enough to prevent world war ii. now you get to the topic of why so many people
are super concerned about global government and don't want global government, and they'll
say things like the risks are being exaggerated and blown out of proportion to be able to
drive control paradigms. and the people who want to have a one-world
government or a powerful nation government [3939](https://www.youtube.com/watch?v=g7WtcTATa2U&t=3939.539s)

exaggerate the risks so that they can drive
control paradigms where they will be the one in the control side. this can be excessive paranoia, but it's also
a really realistic and founded consideration, which is are there any radical asymmetries
of power where the side that had all the power used it really well historically? it doesn't look that good, right? and so we see a reason to be concerned about
something like a one-world government that has no possible checks and balances. but there's also a concern about not having
anything where you get some type of [[global governance]], if not government, meaning some
unified establishment that has monopoly of violence. at least governance, meaning some coordination
where everyone is not left in a [[multipolar trap]] saying we can't bind our behavior because
they won't, and if they won't, then we have to race ahead. we can't stop overfishing because the fish
will all get killed because they're doing the thing anyway. so not only will we not stop, we will actually
race to do it faster than them so they don't get more resource relative to us, those types
of issues. so obviously with regard to the environment,
we call it a tragedy of the commons. with regard to the development of possible
military technology, we call it an [[arms race]]. both of them are examples of social traps
or [[multipolar trap|multipolar traps]]. briefly, why do you call it a [[multipolar trap]]? social trap is a term used in the social sciences
quite a lot to indicate a [[coordination failure]] of this type where each agent pursuing their
own near-term rational interest creates a situation that moves the entire global situation
long-term to a suboptimal equilibrium for everybody. and there's a lot of work in various fields
of social science on social traps. the first time i'm aware of the term multipolar
trap entering the conversation was in the great article called meditations on moloch
by scott alexander where he – i believe he's the one who coined the term multipolar
trap there, and it's pretty close to a social [4070](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4070.839s)

trap. if i was going to define a distinction, it
might be something like in a classic social trap, it's a social trap. tragedy of the commons scenario where everyone
is utilizing a common wealth resource like say fishing or cutting down trees in a forest
or whatever. you're not necessarily in the situation where
everyone is racing to do it faster than the other person to destroy it, just them simply
not curtailing their own behavior, and yet you have a resource per capita consumption
growth and a total population growth such that the environment can't deal with it. you still end up getting environmental devastation. [4119](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4119.54s)

but as soon as you kind of move over into,
hey, even if i don't cut down the trees or i don't fish, the other side is going to,
so i literally don't have the ability to protect the forest, but i do have the ability to cut
down some of it, benefit myself or our people or tribe or nation or whatever it is. and if i don't, the other guys will break
it down anyways, but they'll also use the economic advantage of that against us in whatever
the next rivalrous conflict is. so not only do i have to keep doing it, but
i have to race to do it faster than they do. i actually have to apply innovation now, and
so this is where you get an accelerating dynamic. and if you don't just have two actors doing
this, but you have many actors doing this where it's very hard to be able to bind it
because how do you ensure that all the actors [4162](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4162.02s)

are keeping the agreement? you have to make some nonproliferation agreement. you have to have some way of ensuring that
they're all keeping it, and you have to have some enforceable deterrent if anyone violates
it. those happen, but it's not trivial. it's not trivial to enact those, and it's
particularly – so let's say we've achieved that when it comes to nukes in some ways,
though at the beginning of the current post-world war ii system, there was only two superpowers
with nukes, and now there's roughly nine countries with them. there are not 100 countries with them. [4197](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4197.44s)

there aren't even 30 because we've done a
really intense job of ensuring that iran and many countries that want nukes don't get them,
right? and why? because there are not uranium mines everywhere. you can see where they are. uranium enrichment takes massive capability
that you can literally see from space, right? there's radioactive activity associated. so it's somewhat easy to monitor that that's
happening. this is not true at all with the newer technologies
that provide more catastrophic capability. so obviously with ai right now and the regulation
of it, there are conversations about like [4231](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4231.52s)

we need to monitor all large gpu clusters
or something like that, which to some degree can be done. but in terms of applications, it takes a very
large gpu cluster to develop an llm. it takes a very small one to run that llm
afterwards, right? and then can you run it for destructive purposes? and it takes a very large capability to advance
something like a crispr or a new type of synthetic bio knowledge. it doesn't take that much to be able to reverse
engineer it after it's been developed. so this brings up this very important point
of when the technology is built, there's this general refrain that all technology is dual
use, right? [4285](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4285.96s)

meaning that if it wasn't, sometimes it's
developed for military purpose first and then becomes used for civilian normal market purposes. but if it's being developed for some non-military
purpose, there's probably a militarized application. that's what's meant with dual use is military
versus non-military. so it's not the same as this is a double edged
sword. it's positive and negative. it's not the same as that. yeah, it is. what it's saying is you're developing this
for some purpose, but it has other purposes too, right? [4315](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4315.219s)

and it has purposes that can be used for violence
or conflict or destruction or something. and while that is historically mostly used
with the concept of has a military application can be used to advance war and killing and
things like that, whether by a state actor or a non-state actor, so you call it terrorist
activity. sorry, when i was thinking of military, i
was also thinking in terms of pure defense, not just defense that also can be something
that can attack. yeah. yeah, the pure defense only military – it
starts becoming part of most military doctrines that viable defense requires things that look
like escalation. but that's another topic as well. [4362](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4362.0s)

so it's not just that all technologies are
dual use. it's that they have many uses. you develop the technology and i think a good
way to think about – so now this is a little bit of theory of tech. did we close [[multipolar trap]]? well, you mentioned that it first came up
in scott erickson's or – alexander. yeah. and so basically the concept is you have many
different agents who all of them pursuing their own rational interest and maybe they
can't even avoid it because it would be so [4394](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4394.99s)

irrational. it would be so bad for them game theoretically
that the effect of each of the agents pursuing their own rational interest produces a global
effect that is somewhere between catastrophic or at least far from the global optimum if
they could coordinate better. so this is basically a particular type of
multi-agent [[coordination failure]]. and we see this all over in the tragedy of
the commons as an example. a market [[race to the bottom]] like happens in
marketing and attention currently is an example, and an [[arms race]] is another example. those would all be examples of a kind of multipolar
trap [[coordination failure]]. this is why if you have – say one nation
is advancing [[bioweapon|bioweapons]] or advancing ai weapons, either ai applied to cyber or applied to drones
or applied to autonomous weapons of various kinds. if any country is doing that, it is such an
obvious strategic advantage that every other country has to be developing the same types
of things plus the whole suite of counters and defenses to those types of things. and so you could just say, well, the world
in which everybody has autonomous weapons is such a worse world than this world that
we should just all agree not to do it. except how do i know the other guy is actually
keeping the agreement? well, with the nukes we can tell because we
can see if they're mining uranium and enriching it because it takes massive facilities and
they're radioactive and stuff like that. [4476](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4476.79s)

but if we're talking about things like working
with ai systems or synthetic biosystems that don't require a bunch of exotic materials,
exotic mining that don't produce radioactive tracers, et cetera, and they can be done in
a deep underground military base, how do we know if they're doing it or not? so if we don't know if the other side is doing
it or not, then the [[game theory]] is you have to assume they are because if you assume they
are, you're going to develop it as well. and then if they do have it and use it, you
aren't totally screwed, whereas the risk on the other assumption that they aren't, if
you were wrong, you're totally screwed. so under not having full knowledge, the game
theory orients to worst case scenario and being prepared against the worst case. [4523](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4523.23s)

but what that means is all sides assume that
of each other. we don't know that the other guys are keeping
the agreement. therefore, we have to race ahead with this
thing. and so this is why you're saying when it comes
to things like ai, do we need something that is not just an fda thing but a un thing? is this the kind of thing that would require
an international agreement? and obviously when there was the question
of creating a pause on a six-month pause or whatever, one of the first things people brought
up is won't that let china race ahead? and isn't this a us-china competitiveness
issue? and we can see with the chips act and trying
to ban asml downstream-type gpus to china, [4559](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4559.27s)

and we can see with the pressures over taiwan
and tsmc that there is actually a lot of us-china great power play competition related to computation
and ai in specific. and so it's a classic situation that if you
can't put certain types of control mechanisms in internationally, you will probably fail
at being able to get them nationally as well. so about this competition where the tragedy
of the commons such that like, well, the competitiveness plus tragedy of the commons accelerates the
tragedy of the commons. why is it not much more simple, religiously
simple, ethically simple, where we go back and we say, hey, what i'm going to do is outputting
something negative. i don't care that if you do it, you're going
to get ahead. i don't care if you're going to eliminate
me. [4612](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4612.52s)

i would rather die for your sins rather than
contribute my own sins. so the selflessness. why isn't that sort of ethic? like we say we don't want to be luddites,
but why isn't that a solution? i mean, you're bringing up a great point,
which is can there be a long range thinking about the kind of world we want to live in
and a recognition of the kind of beings we have to be, the behaviors we would have to
do and not do for that world to come about where we bind ourselves, right? where we have some kind of whether the ethics
reduces to law, meaning there's a monopoly of violence that backs up the thing or not. [4647](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4647.56s)

can we at least self-police in some way towards
it? and the answer is the long term answer must
involve that, i would argue. past examples have involved it, but let's
talk about where it's limited. one could argue that the sabbath and the punishments
for violating the sabbath is an example of binding a [[multipolar trap]]. so you're not going to work on the sabbath,
and if you do, there's 29 different reasons laid out why you can be killed for working
on the sabbath. it seems to secular people not thinking about
the chesterton fence deeply, it seems like a ridiculous, wacky religious idea not grounded
in anything with a ridiculous amount of consequence. now, your theory of justice is, is it only
a personal or is it a collective theory of [4704](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4704.8s)

justice? some theories of justice are your punishment
is not based on just what was right for that one person but creating an adequate deterrent
for the entire population because if you don't, what happens? so a classic example is singapore's drug policy
is pretty harsh, right? life in prison for just possession of drugs. well, that was following the devastating effect
that the british had on the chinese with the opium wars and recognizing how as a kind of
population-centric warfare, the british were able to influence, like, catastrophic damage
on china. they're like, we don't want that here and
we know that there are external forces that [4747](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4747.93s)

will push to do that kind of thing and it's
not just personal choice once there are asymmetric forces trying to affect the most vulnerable
people in the most vulnerable ways. so we're going to make it to where the deterrent
on drug use is so bad nobody will do it. so if you say that, you actually have to lock
somebody up forever for smoking pot, which feels very unfair to them. but you probably only have to do that like
a few times before nobody ever will fucking touch it because the deterrent is so bad and
they believe it will be enforced. and if the net effect on the society as a
whole is that you don't have black markets associated with drugs and gangs and the violence
that's associated and you don't have ods and you don't have the susceptibility to population-centric
warfare and whatever, they might argue a utilitarian [4795](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4795.57s)

ethical calculus that the harsh punishment
was radically less harm to the total situation than not having it. so you have a strong deterrent. so that's just – i'm not saying that i think
that is an adequate theory of justice, but it is a theory of justice, right? so let's say that the sabbath was something
like this, and i'm not saying that the rabbis that were creating it at the time thought
this, though many people suggest that that's probably what they thought. some very competitive people wanting to get
ahead will work every day. they'll work seven days a week, and as a result,
they will be able to get a little bit more [4838](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4838.62s)

grain farming, whatever, then other people
get more surplus, start turning that into compounding benefits. and if anyone does, it will create a competitive
pressure where everyone has to. so nobody spends any time with their family. nobody spends any time connecting to what
binds the culture together, the religious idea, et cetera. so we're going to make a sabbath where no
one is even allowed to work, and there's such a harsh punishment against it that we're binding
the [[multipolar trap]], right? because even though it would make sense in
that person's rational interest to work that extra day a few times to get ahead, the net
effect on the society cumulatively is actually [4874](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4874.98s)

a shittier world. so we're going to bind it because people having
that time off to be with their family, each other, and studying ethics is a good idea. i would argue that religion has heaps of examples
like this of how do we bind our own behavior to be aligned with some ethic. but i would also argue – because that was
the question you were asking, right? is there some kind of religious bind to the
[[multipolar trap]]? and i think the sabbath is a good example. i think we can also show how well that didn't
work for tibet when china invaded, right? which is we want to be nonviolently oriented. [4916](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4916.5s)

we have a religion that's oriented towards
nonviolence. and we can see that there were – if you
think about it, the time of genghis khan or alexander the great or whatever where you
have a set of worldviews that doesn't constrain itself in that way. and it's going to go initiate conflict with
those people who didn't do anything to initiate it and don't want it. but the worldview that orients itself that
way also develops military capability and maximum extraction for the surplus to do that
thing. the other worldviews don't make it through. they get wiped out because – so there are
indigenous cultures and matriarchal cultures [4952](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4952.77s)

and whatever that we just don't even have
anymore. don't even have the ideas around it because
it just got wiped out by warring cultures. and so does that produce the long-term world
we want? no. it doesn't either. and so there has been this kind of multipolar
trap on that the natural selection if you want to call it that of worldviews that make
it through are selected by their ability to grow their population, have outsized influence
on other population and win wars. and basically things that don't necessarily
map to a good world long-term. but the things that might map to a good world
long-term might not ever get to the long-term [4995](https://www.youtube.com/watch?v=g7WtcTATa2U&t=4995.34s)

because they get wiped out in the short-term. yeah, i don't buy that. so i'm not saying this as someone who's religious
or from a religious perspective. well, this is a religious perspective, sorry. but i'm not saying this as someone who's advocating
for a certain religion. the most dominant religion in the world is
christianity. and that's the story of someone who had the
government against him and he said, no, i'm not going to fight back. in fact, if you want to persecute me, go ahead. i will come to you. [5022](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5022.39s)

and one of the most striking stories, literally
striking in the bible to me is the story of jesus the captor and peter, his friend, cut
off the captor's ear. the guy was going to take jesus to kill jesus. and jesus said, no, no, no, no, don't do that
and took the ear and healed his captor. so think about this, though. yes, jesus is the guy who said, let he who
has no sins cast the first stone and they brought mary magdalene and all those things. but we somehow did the crusades in his name
and the inquisition in his name and the dark ages in his name. right. that's some weird ass mental gymnastics. but the scenes, the versions that we're going
to stay peaceful and not do crusades, how many do you see around and how much power
did they get? so what happens is you have a bunch of different
interpretations, the interpretations that orient themselves to power and to propagation,
propagate and make it through the interaction between the memes. so memes engage in a kind of competitive selection
like genes do, but not individual memes, meme complexes. so if we have a religion that says, be humble,
be quiet, listen to people and don't push your ideas on anybody. [5094](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5094.78s)

and then you have another one that says, go
out and proselytize, convert everyone to your religion and kill the infidels. which one gets more people involved? right. and so the ones that have propagation and
that have conflict ideas built right in. so, of course, then the meme sets evolve over
time. right. the religious interpretations don't stay the
same. and the meme sets that end up winning through
how they reduce themselves to the behaviors that affect war and population growth and
governance, et cetera, are all part of it. [5127](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5127.73s)

so the fact that the dude who said, let he
who has no sins among you cast the first stone got to be the religion that became dominant
through the crusades and through violent expansionism and then through radical torturous oppression
is fascinating. right. and it shows you that you have like a real
philosophy and then you have politics of power and you have fusions of those things that
you have to understand both of when you're studying religion. to me, and i don't mean to harp on this point,
but it doesn't have to be a choice between, hey, let me do good and let me not push my
views on anyone and proselytizing slash killing. because you can also proselytize and say your
ideas and hopefully people will hopefully [5168](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5168.09s)

maybe there is something in us. maybe there's something cosmically in us. i don't know. that says, hey, you know what? i like that. i don't like that killing. i don't like where that will lead. it resonates with me that the sins get passed
down or that the violence gets passed down and amplified. but i need to be told that. [5186](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5186.64s)

so i do need to hear that because i can't
come up with that on my own. so that's why i'm saying the proselytizing
is a part of it, whether proselytizing is explicit or it's lived and you just see how
someone lives and then you inquire, hey, what are your views? and why are you so happy when you have nothing? and i'm so miserable and i have everything. i just don't see it as a choice between you
do good locally and don't tell anyone about it or you can tell people about your ethical
system, but also oppress them. well, to make the [[thought experiment]], we picked
both extremes. so we can see that the mormons proselytize,
but they don't kill everyone who disagrees [5215](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5215.5s)

in the crusading kind of way. they have not expanded as much as the crusades. they've not got as much global dominance or
total population as a result. but they have not got nowhere. we can see that the ones that say, hey, if
someone is interested, we'll share, but we're not going to proselytize because we have a
certain humility of how much we don't know and a respect for everyone's choice. the mystery schools stay pretty small. and, again, when we were talking about asymmetries,
those who are more focused on the opportunity and downplay the risk move ahead, get the
investment capital, et cetera, and those who [5259](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5259.55s)

are focused on the risk heavily don't. there's a similar thing here, which is like
there's an asymmetry in the ideas that hit evolutionary drivers even if perverse forms. like in the [[evolutionary environment]], it was
where there was actual food scarcity. we evolved dopaminergic, dopamine opioid responses
to salt, fat, and sugar, which were hard to get and useful. as soon as we got to the point where we could
produce lots and lots of salt, fat, and sugar and there was no scarcity on those things,
our genetics didn't change. and so the fact that it felt really good when
you ate that and incentivized you to get more of it where that little bit of surplus might
mean you make it through the famine versus [5305](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5305.84s)

not, it was an adaptive response. then we create a anthropocene where we have
hostess and mcdonald's giving amounts of salt, fat, sugar that are – and the combinations
of them with the kind of optimized palatability where it is – not only is it not evolutionarily
useful to get it anymore, it is actually the primary cause of disease in the environments
where that's available. it doesn't mean that the dopaminergic signal
changed. so we're able to kind of take an evolutionary
signal and hijack it. and this is obviously what fast food does
to the evolutionary programs around food. it's what [[social media]] does to the impulses
for social connectivity. it's what porn does for the impulses to sexual
connection associated with intimacy and procreation [5348](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5348.679s)

and all like that is to extract the hypernormal
stimuli from the rest of what makes it actually evolutionarily fit. same thing can happen with religion, right? you can offer people an artificial sense of
certainty and offer them an artificial sense of belonging and security and various things
like that and without much actual deep philosophic consideration or necessarily even deep numinous
experience. and that similarly has the ability to scale
more quickly than something where you want people to actually understand deeply, discover
things themselves, have integrated experiences, not just do the right action but for the right
intrinsically emerging reasons. which is why your podcast doesn't have one
of the – as many views as the most trending [5404](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5404.06s)

tiktok videos that require less work and are
shorter and more oriented to [[hypernormal stimuli]]. so i'm not saying we can't work with these
things. i'm saying these are the things we have to
work with. so we're in a situation where the – let's
say that we're in group – in groups and out groups would both cooperate and compete
at different times based on what [[game theory]] seemed to make most sense. and they would typically cooperate while reserving
the right to compete. and to even fully defect if they need to,
right? resource scarcity or something. or just a sociopath coming into leadership,
which totally happens. [5453](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5453.04s)

so the combination of the worldviews – everybody
needs to believe our religion. if they don't, they are bad and so we're going
to convert them or whatever, right? or everyone needs to be – have a democracy
because that's good and all other forms of governance are bad or whatever it is. there's ideology that orients itself. there's a tech stack that is a part of the
capacity to do that. there are coordination mechanisms that are
part of that. so the full stack of the [[superstructure]], the
worldviews, the [[social structure]] and the infrastructure are what are engaged in in-group, out-group
competitions and that are upregulating largely shaped by those competitions. [5494](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5494.6s)

it just happens to be that the version that
makes it through that shaping process is also orienting us towards a whole suite of global
[[catastrophic risk|catastrophic risks]]. it is basically [[self-terminating]]. and so it has been the case that you have
to win the local [[arms race]] because otherwise you lose. but the [[arms race|arms races]] that are externalizing
harm but on an [[exponential curve]] that have cumulative effects, you don't actually get
to keep externalizing on an [[exponential curve]] or running [[arms race|arms races]] on an [[exponential curve]]
in a finite space forever. so we're at this interesting space where you
can't try to build an alternate world that just loses but you also can't keep trying
to win in the same definition of win. [5540](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5540.29s)

this is the interesting point we're at which
is we have to actually build a version of win that is not for an in-group in relationship
to an out-group but is something that actually allows some kind of omni-win that gets us
out of those [[multipolar trap|multipolar traps]]. and this was all coming from the topic of
you starting with why you brought up the un and that you have to deal with these things
with some kind of sense of how are other people dealing with them and how does that affect
the choice-making process. some people would say, look, we're group selected
and then we can make our group to be the tribe versus another tribe. and one of the solutions is if there was aliens
and then we could bind together as humans and fight something external. [5587](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5587.37s)

it doesn't have to be aliens. the point is that there needs to be something
external. so you're saying there's another option and
that that option, the bind together in order to fight some other out-group, whether the
group is something physical or it could be more abstract, that that's not something that
should be pursued and there's another option. i didn't say that, but it's an interesting
conversation. if we are not binding in groups to fight out-groups,
so this is kind of like machiavelli's enemy hypothesis that people are kind of evolutionarily
tribal and that to unify a lot of people at a much larger than tribal scale, given that
they naturally will find their own differences and conflicts and reasons to otherize somebody
because they have more influence over their [5632](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5632.429s)

own small group or whatever. to unify them works best if you have a shared
enemy that forces them to unify. and so then you eventually – of course this
makes small tribes unify to deal with a larger tribe and then you get kingdoms and nation
states and global economic trading blocks and eventually you get great superpower conflicts. and that if the only way to unify, that if
groups opposing each other in that way ends up being catastrophic for the world, so we
want to get everybody unified in some way, do we need a shared enemy? obviously this has been talked about a gazillion
times. can [[climate change]] or environmental harm be
the shared enemy? not really. even if everyone believed in it, which they
don't, it doesn't hit people's agency bias in the same way and whatever. could we stage a false flag alien invasion
that may unify? of course this has actually been an explored
topic both in sci-fi and reality and it's how deeply explored is a question but yes,
it's a very natural topic to explore that something like a attack from the outside would
allow that kind of unification. because of that, there are people who are
very skeptical and concerned of anything that looks like a presented shared threat that
should create some unified response because then they're like, well, what is the government
that regulate – that will navigate that [5741](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5741.25s)

shared threat and who has any checks and balances
on that if that thing becomes captured or corrupt? and so this is, again, the catastrophes or
dystopias. if you don't have some coordination, you get
these problems of [[coordination failure]]. if your coordination is imposed, you end up
getting oppression dynamics. so how do you get coordination that is global
but that is emergent, that keeps local power from doing things that drive [[multipolar trap|multipolar traps]]
but that also ensures that you don't get centralized power that can be captured or corrupted? a system of coordination has to address both
of those things. and as we move into more people with more
resource consumption per capita and the cumulative [5784](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5784.92s)

tipping points on the biosphere being hit,
but even more than that, exponentially more power available to exponentially more actors. obviously, if we look at the history of how
humans have used power and you put an [[exponential curve]] on that, it doesn't go well. so, yeah, that's one way of thinking about
the coordination issue. when we were thinking about the un or whatever
is this global agency, potentially, the phrase, they have no checks and balances comes up. is there a way of organizing something that
is global and influential that has its own internal checks and balances? i don't understand how the us political system
works. [5831](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5831.85s)

it's my understanding that it's tripartite
and antagonistic. i don't understand the details of it. i'm apolitical, at least consciously. i haven't looked into it. but the point is, i can, that's interesting. i don't know how that works. i wonder how much that doesn't work, how much
that can be accelerated, amplified. well, one point that we bring up is that any
proposed system of coordination, governance, whatever, is not going to work the same way
after it's been running for a long time as when it was initially developed because all
of the systems have a certain kind of institutional [5866](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5866.52s)

decay or entropy built in. it has to be considered because every vested
interest that is being bound has a vested interest in figuring out how to break the
control system or capture or corrupt it or something. and so it's not just how do we build a system
that does that, but it's also how do we build a system that continues to upregulate itself
to deal with an increasingly complex, different world than the one it was originally designed
for and that continues to deal with the fact that wherever there is an incentive to gain,
the system is going to happen. so you have to not only figure out a system
that makes sense currently, but a system that has an adaptive intelligence that is adequate
for the changing landscape. [5912](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5912.639s)

so when you look at the u.s., because leaving
corrupt monarchy was key to the founding here, and so we were going to try to do this democracy,
non-monarchy thing, it was also the result of a change in tech, right? it was a result of the [[printing press]] where
rather than – before a [[printing press]] and everyone could not have textbooks and couldn't
have newspapers and to have access to information, someone had to copy a book by hand, which
meant that there were very few of them who would copy the information by hand so only
the wealthy could have it. the idea of a wealthy nobility class that
got educated enough to make good choices for everyone else where if they were too corrupt,
the people would overthrow them, so there was certain kind of checks and balance that
kind of maybe made sense, right? [5965](https://www.youtube.com/watch?v=g7WtcTATa2U&t=5965.369s)

with the noblesse oblige built in, the obligation
of the nobility class to rule well. i'm not saying it did, but that's at least
the story. but as soon as the [[printing press]] comes and
now everybody could have textbooks and get educated and everybody could have a newspaper
and know what's going on, it kind of debases the idea that you need a nobility class to
make all the choices because everyone else doesn't know what's really going on. and you say, well, maybe we could all get
educated enough to understand how to process information and we could all get news to be
able to understand what's going on and all have a say. and so obviously democracy emerged following
that change in information tech. [6003](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6003.05s)

i'm saying this because, of course, ai is
a radical change in information tech that will also obliterate our existing political
economies and coordination systems and make new ones and changes to culture as well. the difference in the ai case, just briefly,
is that i don't see the ai as democratizing more so than exacerbating the inequality in
terms of like, so if you're extremely bright, the amount of information you can process
is going to be far outpacing someone who either is not so bright or gets access to that ai
three weeks later. so thinking through in the same way that the
[[printing press]] had an effect on central religion through everybody can have a bible and read
it and learn on their own and kind of lutheran revolution and it had an effect on central
government in the form of feudalism. [6052](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6052.699s)

we can then look at kind of mcluhan's insights
of how information tech changes the nature of the [[collective intelligence]] and motivation
and type of mind that everyone is operating with. and as a result, the emergent type of society,
we can look at the way that the internet and digital have already done that. looking at the way [[social media]] has affected
media, for instance, which affects our democratic systems is a pretty obvious one. but then we can look at ai and not just ai,
but different types of ai, different ways it could develop. llm is very different than other kinds of
ai. [6092](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6092.82s)

so we'll come to that in a moment. but let's come back to the other question
because you were asking the checks and balances one. so the idea in the us system was the british
system following the magna carta and the treaty of forest and whatever was supposed to be
the most ideal noble thing around and ended up being in their experience a totally corrupt
thing. so the idea that no matter how you develop
a system, it can be corrupted. that was built in. so how do we make sure that no part gets too
much power and that we have checks and balances throughout was kind of key. [6133](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6133.19s)

so before you even get into the three branches
of government, you already have the separation of the state and the church, which was already
a key part, and you have the separation of the market and the state, which is the – you
have a [[liberal democracy]] that is proposed. so you don't have a pure market function,
but you also don't have that the state is running the entire economy. and so the separation of the market, the state,
the church, there's a few other ways of thinking about separation was already a part of it. and then with regard to the state's function,
the separation of the legislative, the judicial, and the executive were critical. and then within each of those, within the
legislative, a bicameral breakdown was really [6183](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6183.88s)

important. and that then the 10th amendment was to push
as much power, the subsidiary principle, to the states as possible and as little to the
federal. so there were many, many steps of checks and
balances on concentrated power that were built into the system. but of course, everyone who is smart, who
is also agentic, who wants more power, looks for loopholes and or figures out how to write
laws and to get them passed, right, doing legislation and lobbying. and of course, corporations can pay for a
lot more lawyers than an average citizen can or than a nonprofit group that doesn't have
a revenue stream associated. [6226](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6226.77s)

so the group that is trying to turn commons
into commodities versus one that's trying to protect the commons will inherently have
a bigger revenue stream to employ media to change everyone's mind or to employ campaign
budgets or to employ lobbyists or whatever. so you end up seeing that there is a progressive
kind of loophole-finding corruption because the underlying incentive systems, invested
interests, are still there, right? baudrillard simulation and simulacra that
discusses the steps of the degradation from a new system to how it eventually devolves
into mostly a simulation of what it originally was is a good analysis on this we could discuss. but – so that's a little bit on kind of
the history of checks and balances on power. but i don't think anybody looks at our current
u.s. system and says it's doing a great job [6283](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6283.6s)

of that. and there's a bunch of reasons in addition
to the one that i said about how there is a natural process of figuring out how to influence
this. like everyone who – okay, there's one other
part that's actually worth saying. so you have a state. you have a market. and you have the people as members of a democratic
government, meaning their function in state, not their function in market. so a government of, for, and by the people. the people might not all be representatives,
but they can all speak to their representative, [6328](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6328.599s)

decide how it votes, those types of things,
right? so there's supposed to be a check and balance
between these three that the main reason that there is law is to prevent some people or
groups of people from doing things that they have an incentive to do that would suck for
everybody else. obviously, whether it's individual stealing
or murder or whatever, or it's a corporation cutting down the national forest or polluting
the waterways too much, there is – somebody has an incentive to do something. and in a democracy where the idea is supposed
to be that the – we all want and value different things, but the collective will of the people
as determined through some voting process gets instantiated into law where a monopoly
of violence can back that up. [6378](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6378.849s)

that's kind of core to the idea of a liberal
democracy, right? i'm not arguing that it is a good system,
but i'm arguing for the core logic of it. and it's because the recognition that if we
just had a pure market system, the reason why there wasn't just a pure kind of laissez-faire
system even though the people building this understood, at least their expressed reason,
is in a pure type market dynamic, as you were mentioning with ai, some people are way better
at it than other people. and as a result, we'll just end up getting
a lot more money that they can convert to more land, resources, employees, et cetera,
and you end up getting a power law distribution on wealth, which is a power law distribution
on everything, and these people's interests end up determining the whole society and these
people's interests are pretty determined for [6424](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6424.52s)

them. and so if you want to create protections for
these people at all, and that was basically the king george situation and the inspiration
for the declaration of independence and leaving, which was there was too much concentrated
power and it was kind of fucked, so how do we make that not happen? well, since we know that the market is going
to kind of naturally do that, let's create a state that is more powerful than any market
actor. and let's make sure that the state reflects
the values of all the people. so the little guys get to unify themselves
through a vote, right? and then you get to have a representative
that represents everybody. [6460](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6460.41s)

it's the only one given a monopoly of violence
and it gets to make sure that any more powerful actors are checked. that's kind of the idea. yeah. so, so far, this is an account of how it's
been like a history lesson, but you aren't saying this is how it should continue to be,
nor this is how it's operating in its ideal sense currently. are you just saying that this was the reasoning
behind it? one key part of how it broke down. so the idea is that the market, people will
have incentives to do things that are good [6484](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6484.54s)

for them that might suck for the environment
or others. and so others have the ability to agree upon
laws that will bind those actors to not do that thing, right? so the state is supposed to check the market,
let the market do its thing, do resource distribution productivity, let it do that because it's
good, but check the particularly fucked applications. and in order for the state to check the market,
the people are supposed to check the state and ensure that the state is actually doing
the thing that it's supposed to do and that the representatives aren't corrupt and taking
back-end deals and all those kinds of things, right? and then there's a way in which the market
kind of checks the people, meaning that the [6526](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6526.21s)

people can't – the accounting checks them. they can't vote themselves more rights than
they're willing to take responsibility for. they can't make the economics of the whole
situation not work, right? they can't vote themselves a bunch. they can't – if the people all say, yes,
we should all get no taxes but lots of social services, then the accounting is what actually
checks the people, right? so that's the idea of how you have this kind
of self-stabilizing thing. but of course the people stopped checking
the market once we were out of kind of the sense of an eminent need for revolution. then the people have a lot of shit to do other
than really pay attention to government in [6570](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6570.45s)

detail. and there's a bunch of other reasons beyond
the scope of this conversation why the people stopped checking the government, in which
case the market is continuously trying to influence the government through lobbying
and legislation and campaign finance and all those other things. and so then you end up getting regulatory
capture rather than regulatory effectiveness, right? so when you put those checks and balances,
it's going to change. when everyone is scared of concentrated power
following a revolution, it's different than four generations later where nobody actually
feels that fear anymore and is busy doing [6601](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6601.87s)

other shit, right? so it's not just how you build your system. it's how do you build a system where the initial
people that went through the difficult thing to build it when they die, you didn't just
pass on the system but the [[generator function]] of the kinds of insights needed to keep updating
and evolving the system under an evolving context. so when you ask the question about could such
a thing be built at an international level where there are checks and balances, the answer
is it's super hard. but yes. but it's not just can you design it properly
up front. [6640](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6640.06s)

it's also can you factor how that system then
even if well intended at first, it's kind of like all technologies dual use. so you build the gene editing for immuno-oncology
but then it can be used for [[bioweapon|bioweapons]]. you have to not just think about what you're
building it for but all the things that will happen having created that thing. same thing with government. you have to think about not just who you're
building it for right now but as the landscape changes, culture changes, can this thing be
corrupted? can it be captured in future different contexts
and how do you build in immune systems to that? and that sort of thinking seems to be missing
with the development of ai. and it reminds me, i've said this several
times, like the development of the bomb where feynman and oppenheimer, mainly feynman and
his peers, said they didn't think about what they were creating. they were thinking we're having fun speaking
about these topics. it's even more fun to do research on these
topics. einstein said like i'd burn my hands had i
known that this was what was going to be developed. i wasn't thinking about that. i wasn't thinking about the consequences and
feynman said something similar. we're consumed with the achieving of a goal
and we're not thinking about what would occur [6702](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6702.82s)

as a consequence once we attain it. and you hear this constantly in the ai scene,
channels like two minute papers that say, what a time to be alive. that's like his catchphrase, what a time to
be alive, like encouraging and amazed constantly thinking, what is this going to be like? two papers down the line said enthusiastically,
i see little caution expressed. yes. jeez louise, like what the heck are we building
and should we? just because we could, should we? well, the people who express caution, this
now relates to this asymmetry, said if people [6729](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6729.81s)

are like, hey, this is extremely risky technology,
we need to understand the risk space very deeply first. we need to ensure that the development of
the technology and then its future use by everybody is safe enough to be worth built. those people end up running non-profits because
there's no upside to that. there's no immediate capital upside to that. so they have a hard time getting the capital
to get really good researchers or big enough computers and data sets to try to run stuff
on for trials. and the people that are like, oh, there's
a market application to this have a much easier time getting a massive gpu cluster and a lot
of talent and a lot of data. [6777](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6777.46s)

and so we can see this if you name the names
that are out there and their views and then map them to the types of organizations they
run and the type of motivational or cognitive bias it somewhat maps. right. so. what is. this is what we intended to talk about all
of this has been interesting preface. what is the actual risk space with ai? what do we know? what do we. [6811](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6811.46s)

not know, how should we think about it? how should we proceed, especially given that
ai is a lot of different things? should we dive in there now? sure. i just wanted to point out that although this
seems like a disagreement between you and i on the surface, there's an agreement. so again, i'm not a mormon. but i don't see the mormons failure because
they go and they say, hey, you should whatever they say, act right, be humble, be kind, don't
overconsume and so on. but then their religion doesn't grow. [6840](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6840.41s)

i don't see that as a failure of them because
maybe their religion is larger than just being mormon. it's something about the values that are spread
and then they send out these values and they filter through the community in the same way
that these nonprofits, just because they're not the largest, doesn't mean that the values
that they send out don't influence you and i and influence the people who are listening, who then act differently because of these
values. we have no idea how much the pacifism of tolstoy
has influenced you hugging your father and your brother and the positive sentiment we
have generally speaking in society toward decentralization. [6873](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6873.5s)

and it also reminds me of the cassandras for
people who don't know what a cassandra is. it's someone who makes a prediction. it's a doomsayer. it's akin to a doomsayer. they're the opposite of a self-fulfilling
belief. so self-fulfilling belief is one where you
state it and you create the conditions such that it becomes true. whereas in the best case for the people who
say the world is going to end, well, their success depends on them being self-sacrificing,
depends on us then being able to repudiate them. let the world hasn't ended. we have no idea how much the doomsayers or
the fear mongerers said something that made us straighten up and act right just enough
that it pushed us off of the brink and influenced us to make society live. and so we then have this archetype of the
cautious and contumacious false cravens of the past. it's just not clear that because they have
died doesn't mean that they're unsuccessful. that's what i mean. yeah. so there's this important point. the idea that even though greece didn't continue
its empire relative to rome, that its memes ended up influencing the whole roman empire. and so in some way it won or similar with
judaic ideas or whatever. one of the greatest examples of that that
people talk about right now is tibetan buddhism, which is ok. so from the point of view of tibet as a nation,
the tibetan people, the integrity of that wisdom tradition, it was radically destroyed. but in the process of the world seeing that
and having some empathy engendered for it, even though it didn't protect tibet, did that
actually disseminate buddhist ideals to the [6976](https://www.youtube.com/watch?v=g7WtcTATa2U&t=6976.29s)

whole world radically faster? there's a similar conversation as so many
people become interested in ayahuasca and plant medicines from indigenous cultures that
the economic pressure of that is making what is remnant of those cultures get turned into
tourism and ayahuasca production or shipibo production or whatever it is. that on one hand it actually looks like a
destructive act on those cultures. and the other way it's – are the memetics
of those now becoming dispersed throughout the dominant systems. that is a part of the consideration set that
has to be considered. and now do we see, for instance, that particularly
nonviolent groups like say the jains as an [7022](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7022.56s)

example of maximum nonviolence, that those
memes do become decentralized and affect everybody? it's not a simple yes or no, right? there's a whole bunch of contextual application. so when we look at are there memes from buddhism
that have influenced the world? yes. but are they the ones that are compatible
with the motivation set of the world they influence? so you've got this like buddhist techniques
of mindfulness for capitalists in silicon valley to crush it at capitalism is a very
weird version of the subset of the buddhist stack. and i remember when i first started seeing
kind of the popularization of mindfulness techniques in business so that people could
focus better and crush it at capitalism how fucking hilarious that thing is, right? because in some ways you are distributing
good ideas. in other ways you're actually extracting from
a whole cultural set the part that ends up being a service ingredient to another set. so the topic of what makes it through, i mean
it's a complex topic. what i will say is the idea that a civilization,
which is its [[superstructure]], its worldview values, what is true, good, beautiful, what
it's oriented to, what's a good life. its [[social structure]], meaning its political
economy and institutions, its formal incentives and deterrence and how it organizes collective
agreement and its infrastructure, the [[physical tech]] stack that it mediates all this on. together in a civilizational stack. one of those competing with other ones for
scarce resource and dominance and all of them engaged in that particular competitive thing. no one actually gets to win that. that process of the competition of those relative
to each other does actually create an overall global civilizational topology that self-terminates. but also no one trying to create a good long-term
future can just lose at that game in the short term. so you can neither create the world you want
by just losing at that game nor by trying to win at it. it's something else. it's actually having to abandon that game
to try to change the game dynamics themselves. ah, okay. so i was just watching arrested development. this, that you can't play the game, you can't
frame it in the same way. so buster from arrested development, i don't
know if you've seen it, he's this mama's boy. he wants to go out with this other girl who
happens to have the same name as his mom. but anyway, she's like, i don't want to go
out with you because you're just in love with [7201](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7201.409s)

your mom. then he's like, no, and he just left his mom's
home. he's like 40 years old or 35. he's like, no, no, no, it's the opposite. i'm leaving my mom for you. you're replacing my mom. and then she's like, no. and it's because he's still framing it in
the same way. anyway, we have to abandon the frame. so please, what does that look like? [7221](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7221.1s)

and integrate ai into this answer. so we have not just tried to get the frame
on ai risk yet. and to incorporate that in the what is the
long term solution for civilization as a whole look like, let's actually just kind of do
the ai risk part first and then we can bring it back together. let's try to frame how to think about ai risks,
ai opportunities and potentials, including how ai can help solve other risks, which has
to be factored. i will add as preface that i am not an ai
developer. i don't have background in that. i'm not even an ai risk expert or specialist. [7277](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7277.35s)

i know you're in conversation might have yudkowsky
and other people who really are stuart russell, bostrom. those guys would be great. because of some maybe novel perspectives about
thinking about risk and governance approaches to risk writ large, [[metacrisis]], that's the
perspective that i'm taking into the ai topic. so what is unique about ai risk relative to
other risks? we were talking earlier about environmental
risks and risks associated with large scale war and breakdown of human systems and synthetic
bio and other things. if we look at other technologies that are
that have the potential to do some catastrophic things like nuclear, it's very easy to see
that [[nuclear weapon|nuclear weapons]] don't make better bio weapons. they don't make better cyber weapons. they don't even make better [[nuclear weapon|nuclear weapons]]
directly. and the same is true for biotechnology. it doesn't automatically make those other
things. ai is pretty unique in that you can use ai
to evolve the state of the art in nuclear technology, in delivery technology, and intelligence
technology, and bio and cyber and literally all of them. so it is unique in its omnipurpose potential
in that way. because of course all those other technologies
were developed by human intelligence. [7373](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7373.699s)

human intelligence, agency, creativity, some
unique faculties of human cognitive process. and so where all of the other technologies
are kind of the result of that human process, building a technology that is doing that human
process, possibly much faster and on much more scale, is obviously a unique kind of
case, right? and so there's thinking about what type of
risk does an ai system create on its own. but then there's thinking about how do ai
systems affect all other categories of risk, right? we have to think about both of those. and then in addition to the fact that the
nukes don't automatically make better [[bioweapon|bioweapons]], the nukes don't even automatically make more
nukes, right? [7424](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7424.449s)

they're not pattern replicating. but to the degree that we actually get ai
systems that not only can make all the other things better, but they can make better ai
systems and to the degree that there starts to be something like autonomy in that process,
then the self-upgrading and omnipotential of all the other things. it's also true that there's an exponential
curve in the development of hardware that ai runs on, right? better gpus and all the different kinds of
computational capabilities. there's an [[exponential curve]] in iot systems
for capturing more data to train them on, exponentially more people and money going
into the field. [7470](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7470.6s)

because of the way that shared knowledge systems
work, the kind of exponential development in the software and cognitive architectures,
so we're looking at the intersection of multiple [[exponential curve|exponential curves]], not just a single one. that is also kind of important and unique
to understand about the space. so thinking about the case of ai turning into
agi, an autonomous [[artificial intelligence]] system that we can no longer pull the plug
on that has goals, has objective functions, whatever they happen to be, that is something
that guys like bostrom and yudkowsky have done a very good job of describing why that's
a very risky thing. i think everybody at this point probably has
a decent sense of it, but just make it very quick. [7521](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7521.34s)

when we say a narrow ai system, we mean something
that is trained to be good at a very specific domain, like beating people at chess or beating
them at go or being able to summarize a large body of text. when we say general intelligence, we mean
something that could maybe do all of those things and can figure out how to be better
than humans at new domains it has not been trained on through some kind of abstraction
or lateral application of what it already knows. so if you put us into an environment where
we have to figure out what is even the adaptive thing to do, we will do it. there's a certain kind of general intelligence
that we have. [7561](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7561.73s)

so when we talk about a generally intelligent
[[artificial intelligence]], the idea – and then, of course, because we can develop ai
systems, one of the things it could do is develop ai systems. so if it has more cognitive capability than
us in some ways, it can develop a better ai system, and then that one could recursively
develop a better one, and you get this kind of thought about recursive takeoff in the
power of an ai system. and there are conversations about whether
that would be slow or fast. is there an upper boundary on how intelligent
a system could be, and humans are near the top of that? or are we barely scratching at the beginning
of it, and we could have something millions [7600](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7600.27s)

of times smarter than us? so that's all kind of part of that conversation. but the idea that we could create an artificial
intelligence that is – that could basically beat us at all games, right? that could – which it could think about
economy and affecting public opinion and military as games. and it has faster feedback loops, faster ooda
loops to get better than we do. so if we're trying to deal with it, it's going
to win at newly defined games. and if that thing we can't pull the plug on,
and it can't anticipate our movements and beat us at all games, if it has goals that
are directly antithetical to ours, or not [7641](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7641.469s)

even directly antithetical but the way in
which it fulfills its goals might involve externalizing harm to things that are part of our goal set. that's bad for us, right? so the idea of don't let that thing happen
prevents getting to an unaligned agi. that's that particular category of risk. and so there are arguments around could an
agi like that – is it even possible? that's one question. if it is possible to have such a thing, is
it possible to align it with human interests? what would that take? [7682](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7682.29s)

is it possible – if it is possible to align
it, is it possible to know ahead of time that the system you have will be aligned and will
stay aligned? right? like those are all some of the questions in
the space. and then do our current trajectories of ai
research like transformer tech or just neural networks or deep learning in general, do these
converge on general intelligence? and if so, in what time period? those are all some of the questions regarding
the agi risk space. now i want to talk about that risk, but i
want to talk about other risks using that as an example in the space. [7723](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7723.309s)

any questions or thoughts on that one to begin
with? sure. number one is that we may already have artificial
intelligence in a baby form, like we have hugging face. i don't know if you know what that is. and then there's the paper sparks of artificial
general intelligence. and that's distinguished from something that
updates itself. i just want to make that clear. so there's a lot of questions regarding does
it have to be better than humans at everything to be an [[existential risk]]? we could imagine a von neumann machine that
was self-replicating and self-evolving that was not better at everything but better at
turning what was around it into more of itself and evolving its ability to do so. and just having way faster feedback loops. and we could imagine that becoming an existential
risk with a speed of a particular type of intelligence that does not mean better than
us at everything. yeah, that's a great point. like an asteroid is not better than us at
almost anything, but it can destroy us. and it, yeah, it's not doing it through some
kind of process that involves learning or navigating competitions at all. [7791](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7791.579s)

it's just kinetic impact. this would be a kind of intelligence, but
it could be one that's a lot more like a very bad pandemic, right? and the intelligence of a pathogen than the
intelligence of a god. so talking about if the system is generally
– like what type of intelligence it would need. is it generally intelligent? is it autonomous? is it agentic? is it self-upgrading? [7821](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7821.5s)

they're related concepts, but they're not
identical concepts. so let's go ahead and put the category of
agi risk as one topic in the ai risk space. let's come to a much nearer term set of things,
which is ai empowering bad actors. and we can talk about what of that is possible
with the existing technology. what of that is possible with impending technology
that we're for sure going to get on the current course versus things where we don't know how
long it's going to take or even if we'll get there. so with regard to ai empowering bad actors,
we could say – how one defines bad actor is obviously – because one person's freedom
fighter is another person's terrorist. but – so we can imagine someone who is terrified
about environmental collapse deciding to become [7882](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7882.71s)

an eco-terrorist being a maximally good actor
in their world view. but saying that the only answer is to start
taking out massive chunks of the civilizational system that's destroying the environment. so i'm simply saying that i'm not being simplistic
about what we mean by bad actor, but oriented to from whatever motivational type, whether
it was pure sadism, whether it's nihilistic burn it all down, or whether it's well-motivated
but maybe misguided considerations. but ai for some destructive purpose. so now – this is something we have to address
first. one thing i have found when people think about
how significant ai risk will be and how significant ai upside will be. [7941](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7941.46s)

first on ai upside, it's just important because
if we talk about risk and we don't talk about upside, it will be easy for a lot of people
to say, oh, this is a techno-pessimist luddite perspective and kind of dismiss it at that. so i would like to say there is a – the
upsides of ai, the best case examples that everyone is interested in, everyone is interested
in. they're awesome, right? can we – all the things that we care about
that we use intelligence to figure out where intelligence is rate limiting, figure them
out, rate limiting to figure out the rest of the problems, could we use it to solve
those problems? so could ai make major breakthroughs in cancer
and immuno-oncology? [7980](https://www.youtube.com/watch?v=g7WtcTATa2U&t=7980.63s)

and does anyone who's talking about slowing
down ai, are they factoring all the kids that are dying of cancer right now? and if we could speed that thing up, could
we affect that in our – like, that's a very personal, very real thing, right? so ai applied to curing all kinds of diseases,
and ai applied to psychiatric diseases and scientific breakthroughs and maybe resource
optimization issues that help the environment and maybe the ability to help with coordination
challenges if applied in certain ways. the positive applications, the kind of customized
ai tutoring that could provide marcus aurelius-level education where the best tutors of all of
rome were personally tutoring him in every topic could provide something better than
that to every human, right? [8034](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8034.889s)

could democratize aristocratic tutoring. there was eric hole's essays on aristocratic
tutoring are really good, you should bring them on here, but basically it was something
many people have come to, which is that the great polymaths and super geniuses, the highest
statistical correlator that pops out is that they all had something like aristocratic tutoring
when they were young, or the vast majority of them. that even von neumann and einstein had mathematicians
as governesses before they went to school. and terry tao had paul erdos, there's this
famous image of, i don't know who had edwin though. so that is a – many of the people simply
had parents that were very actively involved, [8074](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8074.4s)

scientists, philosophers, thinkers, you know. but if you think about why marcus aurelius
dedicated the whole first chapter of meditations to his tutors, and if you think about how
the dalai lama was conditioned where you find this three-year-old boy and have the top lamas
in all of tibet to do everything, that is the whole canon of knowledge. of course, if that was applied to everybody,
we'd have a very different world, right? and i think this is a very interesting insight
because it says that the upper boundaries on a lot of what we call [[human nature]] because
it's ubiquitous, it's not nature, it's nature through ubiquitous conditioning, that the
edge cases on human behavior show conditioning in common. [8123](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8123.31s)

and that if you could make that kind of conditioning
ubiquitous, you would actually change the human condition pretty profoundly. but as we move from feudalism to democracy
and wanted to kind of get rid of all the dreadful, unequal aspects of feudalism, looking at the
fact that like you can't learn to be a world-class mathematician by a person who's not a world-class
mathematician the same way you can by one who is. and you don't get a bunch of world-class mathematicians
becoming third-grade or eighth-grade high school teachers or school teachers. so how would you do that? so it's kind of repugnant from a privileged
lack of democratized capability point of view, [8156](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8156.5s)

right? and yet could you have – could i make llm-trained
ais and better than llm ones where i can have von neumann and einstein and gödel all in
a conversation with me about formal logic? where they are not only representing their
ideas but maybe even now have access to all the ideas since then and are pedagogically
regulating themselves to my learning style. that's kind of amazing, right? like – and could they maybe be doing that
based on also psychological development theories? a colleague of mine, zach stein, has been
working on this a lot of how to be evolving not just their cognitive capacity but their
psychosocial, moral, aesthetic, ethical, et cetera, full suite of human capacities. [8209](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8209.51s)

so i'm simply saying ai applied rightly, there's
a lot of things to be excited and optimistic about. so that's a given. and we could do a whole long conversation
on more of those examples. there is a –
when i look at how people orient to the topic of ai risk, one of the things that seems to
be a common kind of where their knee-jerk reactions before understanding all the arguments
pro and con well comes is how much they have a bias towards a kind
of techno-optimism or techno-pessimism, kind of a – where pinker, hans rosling, there
are still problems but they're getting better. the world is getting progressively better,
and it's a result of things like capitalism [8264](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8264.83s)

and technology and science and progress. and so more of that will just keep equaling
better. and yes, there will be problems, but they're
worth it, right? versus – so that's – i would call that
techno-capital optimism. but the naive version that doesn't look at
the cost of that thing, we would call a naive progress dialectic. in the dialectic of progress is good, progress
is not good, or we're really making progress versus we're actually losing critical things
or causing harm or whatever. that's a dialectic. that's on the progress side but a naive version
of it. [8308](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8308.469s)

and so most of the – and just to address
that briefly, so the naive progress story looks at all the things that have gotten better. and you can see this in lots of good books
and pinker's books, diamandis' books, rosling's talks, on and on. and then the extension of that into the future,
diamandis starts to do and we could say kurzweil is kind of an extension of that far out. why naive is if it doesn't look at what is
lost in that process and what is harmed in that process as well as the increase in the
types of risk that are happening. and so i would argue that most of those things
in every kind of rosling presentation is cherry-picking its data out of a humongous set to make a
cherry-picked argument. [8361](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8361.36s)

this is one of the reasons that fact-checking
is not enough is because you can cherry-pick your facts. you can frame them in a particular way and
create a conclusion that the totality of knowledge wouldn't support it all because of that process,
right? i would say there is a naive techno-pessimism
or luddite direction that looks at the real harms tech causes culturally, socially, environmentally
or other things and wants back to the land of movement and organic, natural, traditional,
whatever, various types of – and if it is not paying attention to the types of benefit
that are legitimate, that's naive. but also if it's not paying attention to the
fact that that worldview will simply, as we talked about before, not forward itself because
the one that advances more tech will develop [8409](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8409.971s)

more power and end up becoming the dominant
[[world system]], that also means that it's not actually having a worldview that can orient
towards shaping the world. so we have to – so putting those together,
i would say all of the things that the techno-optimists say tech has made better and all of us like
a world where going to the dentist involves novocain versus not novocain and where we
have painkillers and where we have antibiotics under infection and stuff like that. all of the things that tech has made better
have not come for free. there have been externalized costs, and the
cumulative effect of all of those costs is really, really significant. and so if you look at the progress narrative,
the indigenous people that were genocided [8459](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8459.5s)

don't see it as a progress narrative. the fact that there are more – there's more
biomass of animals in [[factory farm|factory farms]] than there is in the wild today does not see that as
a sign of progress. the animals that live in [[factory farm|factory farms]] or
all the species that are extinct don't see it as progress. the fact that we have many, many different
possibilities of destroying the life support capacity of the planet relative to any previous
time or that almost no teen girl growing up in the industrialized world doesn't have body
dysmorphia where that was not an ubiquitous thing. there's a lot of things where you can say,
damn, those technologies upregulated some [8503](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8503.89s)

things and externalized costs somewhere else. if you factor the totality of that, then you
can say, okay, there are a lot of positive examples any new type of tech can have, but
there's also a lot of externalities and harms it can have. and we want to see how to get more of the
upsides with less of the downsides, and that can't be a rush-forward process, right? that actually requires a lot of thinking about
how to do that. so i'm actually – i actually am a techno-optimist
in a way, meaning i do see a future that is high nature stewardship, high touch, and there's
also high tech. high touch? [8556](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8556.11s)

high touch, yeah, meaning
that the tech does not move us into being disembodied heads mediating exclusively through
a digital world. so i would argue that your online relationships
don't do everything that offline relationships do. they do some additional things like distance
and [[network dynamics]], whatever. but if you're not doing that with your relationships,
they're causing harm. if the online relationships improve your embodied
relationships, not just the create online relationships and debase them, then that's
a different thing, right? so that's what i mean by high touch. so i want to say that naive techno-optimism
is – if we look at the history of corporations [8604](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8604.01s)

that are developing technology, market technology
advancement focused on the upside and not terribly focused on the downside. we look at four out of five doctors choose
camel cigarettes. we look at better living through chemistry
providing ddt and parathion and malathion. we look at adding lead to gasoline in a way
that took a toxic chemical that was bound in ore underneath the biosphere and sprayed
it into the atmosphere ubiquitously and dropped about a billion iq points off the planet and
made everybody more violent in terms of its neurotoxicology effects. trusting the groups that are making the upside
on moving the thing to figure out the risks historically is not a very good idea. [8661](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8661.44s)

and i'm mentioning that in terms of now trusting
the ai groups to do their own risk assessment. and if you think about the totality of risks
well, then you want to say, how do we move the positive applications of this technology
forward in a way that mitigates the really negative applications of it that if one wants
to be a techno-optimist responsibly, they have to be thinking about that well. so what about the ai companies that say we
do third party testing for safety? are you still consider that somehow internal
because they're the ones going out? depends. so when i early in the process of getting
into risk assessment, i had times where corporations asked me to come do risk assessment on a technology
or process. [8708](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8708.52s)

and then when i did an honest risk assessment,
they were not happy because what they wanted me to do was some kind of box checking exercise
that wouldn't cost them very much and wouldn't limit what they were going to do. so they had [[plausible deniability]] to say they
had done the thing and move forward quickly. because what they didn't want was for me to
say, actually, there is no way for you to pursue the market viability of this that does
not cause excessive harms or where you're dealing with those harms messes up your possibility
for margins. without breaking any nda of yours that you
may have signed, are you able to go into what a company did that you disapproved of? and what was the result of it? [8744](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8744.24s)

just as an example to make this more concrete
for people who are listening. yeah, totally. i have seen this in the example of something
like mining technology or a new type of packaging technology that is wanting to say why it's
doing something that addresses some of the environmental concerns. it addresses the environmental concerns it
identified. we identify a bunch of other ones that it
doesn't address well that it moves some of the harm from this area to the other one. that's an example of where some of the problem
would come. but i find this is just as bad in the non-profit
space or in the government space as well, [8780](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8780.13s)

not just in the for-profit space. because they also have – even if it's not
a profit motive, they have an institutional mandate. and their institutional mandate is narrow. they can advance that narrow thing. this is now the same thing as an ai objective,
right? if the ai has an objective function to optimize
x, whatever x is, or optimize a weighted function of x, y, z and metrics, everything that falls
outside of that set, harm can be externalized to that and achieve its objective function. so i remember talking to groups, un-associated
groups. [8813](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8813.01s)

they were working on world hunger, and their
particular solutions involved bringing conventional agriculture to areas in the world that didn't
have it, which meant all of the pesticides, herbicides, and nitrogen fertilizers. and it was a huge increase in nitrogen fertilizer
by a bunch of river deltas where it currently wasn't. it would increase [[dead zone|dead zones]] in oceans from
nitrogen effluent. that would affect the fisheries in those areas
and everything else and the total biodiversity. and when i brought it up to them, they're
like, oh, i guess that's true. but those are not the metrics we're tasked
with. we're tasked with how many people get fed
this year, not how much the environment is [8854](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8854.3s)

ruined in the process. and so the reduction of the totality of an
interconnected world to a finite set of metrics we're going to optimize for, whether it's
one metric called net profit or gdp, or it's the metric of whatever the institution is
tasked with or getting elected or something like that, it is entirely possible to advance
that metric at the cost of other ones. and then it's entirely possible that other
groups who see that create counter responses to that who do the same thing in opposite
directions. and the totality of human behavior optimizing
narrow metrics while both driving [[arms race|arms races]] and externalizing metrics in wide areas is
at the heart of the [[coordination failure|coordination failures]] we face. [8906](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8906.15s)

and so it happens to be that this is already
something that we see with humans outside of ai, but giving an ai an objective function
is the same type of issue. so i was mentioning examples in the nonprofit
space. i think there are examples of how to do ai
safety that can also be dangerous. and so it's important. sure. so somebody proposes an idea like here's a
type of ai that could be good and we should build it, or on the other side, here's an
ai safety protocol that would be good and we should instantiate it in regulation or
whatever. we want to red team those ideas, meaning see
how they break or fail, and violet team them, [8959](https://www.youtube.com/watch?v=g7WtcTATa2U&t=8959.11s)

meaning see how they externalize harm somewhere
else that they didn't intend before implementing them, which just means think through the causal
set beyond the obvious set you're intending it for. so there was this call for a six-month pause
on training large language models bigger than gpt-4. i'm not saying that a pause is a bad idea. i'm saying as instantiated, it's not implementable,
and it's not obviously good. so you saw the pushback as people were like,
all right, so that means that whatever actors are not included in this, which might mean
bad actors, rush ahead, relative. that's a real consideration, and one has to
say, okay, so are we stopping the accumulation [9014](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9014.85s)

of larger gpu clusters during that time? are we stopping the development of larger
access to larger data sets during that time that we'll be able to quickly configure them? are we also stopping – there are plenty
of other types of ai that are not llms being deployed to the public but that are very powerful. blackrock's aladdin played some role in the
fact that it has more assets under management than the gdp of the united states. and there are military application ais in
development. and so can you – so what is the actual risk
space, and are we talking about slowing the whole thing? [9061](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9061.09s)

or are we talking about slowing some parts
relative to other parts where these kind of game-theoretic questions emerge? how would we ensure that the whole space was
slowing? how would we enforce that? those are all things that have to be considered. you mentioned that gdp is not a great indicator,
and – gdp goes up with war and more military manufacturing. it goes up with increased consumerism and
the cost to the environment. it goes up with addiction. addiction is great for lifetime value of a
customer. [9086](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9086.46s)

so – there's something called goodhart's law, so
i'm sure you're familiar with goodhart. is this at the core of what you're saying? it's like, hey – it's one of them. okay. go ahead and explain it. as soon as you have a metric that you try
to optimize for, it ceases to become a good metric. for instance, i think this is from the simpsons,
but it may be real, that there was a town [9105](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9105.38s)

overrun by snakes or rats. i think it was rats, and then you say, hey,
give me rat tails because it implies that you killed a rat. i'll give you a dollar every time you bring
me a rat tail, and we'll reduce the amount of rats. maybe it initially did so, but then people
realized, i can farm rats and then just kill them and give you tails. and thus, i have more total rats. this is a much more general phenomenon. so perhaps if you think twitter followers
are great – [9126](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9126.38s)

okay, yeah. if you incentivize any metric, there are perverse
forms of fulfilling that metric. meaning there's a way to fulfill that metric
that either no longer provides the good, or it provides the good while also affecting
some other bats. right? which basically means you probably thought
of that metric in a specific context. like there's a bunch of wild rats, and the
only way to get a rat tail is to kill a wild rat, not in the context of farmed rats. and so it kind of relates to the topic we
were mentioning earlier about government, that it's not just instantiating a government
that makes sense on the current landscape, [9160](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9160.38s)

but recognizing the landscape is going to
keep changing. and it'll change in a way that has an incentive
to figure out how to control the regulatory systems and how to game the metric systems. so with regard to the topic of ai alignment,
right? because if we tell that ai maximized the number
of rat tails, then it could, like bostrom's [[paperclip maximizer]]. before we continue, it's imperative that we
have a brief overview of bostrom's [[thought experiment]] called the [[paperclip maximizer]]. the [[paperclip maximizer]] scenario, initially
conceived by philosopher nick bostrom in 2003, illustrates the potential dangers associated
with misaligned goals of artificial general [9205](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9205.81s)

intelligence, that is, agi agents. in this hypothetical scenario, an agi is tasked
with the seemingly innocuous goal of maximizing the number of paperclips it produces. however, rather than competence and focus
serving as a salutary quality, it's in fact due to its extreme competence and single-minded
focus that it proceeds to transform the entire planet and eventually the universe into paperclips,
annihilating humanity and all of life in the process. the core ideas to understand from this scenario
are the importance of value alignment, the orthogonality thesis, and instrumental convergence. value alignment is the process of ensuring
that an agi shares our values and goals in [9248](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9248.029s)

order to prevent cataclysmic outcomes, such
as the aforementioned [[paperclip maximizer]]. the orthogonality thesis states that intelligence
and goals can be independent, implying that a highly intelligent agi can have arbitrary
goals. you hear this, by the way, when people say
that we've become more knowledgeable with time, yet our ancestors were wiser. instrumental convergence refers to the phenomenon
where diverse goals lead to similar instrumental behaviors, like resource acquisition and self-preservation. for instance, as marvin minsky points out,
both goals of prove the riemann hypothesis and make paperclips may result in all of the
earth's resources being dismantled, disintegrated, in an effort to accomplish these goals. [9288](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9288.68s)

thus, despite the ultimate goal being different,
for instance, the riemann hypothesis and make paperclips are not the same, there's a convergence
along the way. what's often overlooked in agi development
is something called the value loading problem. this refers to the difficulty of encoding
our moral and ethical principles into a machine. that is, how do you load the values? keep in mind that agi needs to be corrigible
and robust to distributional shifts. agi, or even baby agi, needs to maintain its
alignment even when encountering situations deviating from its training data. additionally, something that we want is that
the agi should be able to recognize ambiguity in its objectives and seek clarification,
rather than optimizing based on flawed interpretations. [9333](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9333.68s)

of course, we as people have ambiguity and
flawed interpretations. the difference is that agi could decidedly
exacerbate our own existing known and unknown flawed nature. another difference is that we can't replicate
on a second to second or millisecond to second basis. at least not yet. one promising approach to this agi alignment
scenario or misalignment scenario is something called reward modeling. this involves estimating a reward function
based upon observing our preferences, rather than us providing predefined objectives. [9370](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9370.011s)

and some of the more hilarious examples i
found of the predefined sort are as follows. the aircraft landing problem was explicated
in 1998 when feldhut attempted to evolve an algorithm for landing aircraft using genetic
programming. the evolved algorithm exploited some overflow
errors in the physics simulator, creating extreme forces that were estimated to be zero
because of the error, resulting in a perfect score without actually solving the problem
that it was intended to solve. another example is the case of the roomba. in a tweet, custard smigley described connecting
a neural network to this roomba to navigate without bumping into objects. the reward scheme encouraged speed and discouraged
hitting the bumper sensors. [9407](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9407.44s)

okay, so think about it. what could happen? well, the roomba learned to drive backward. there are no sensors in the back, so it just
went about bumping frequently and merrily. in a more recent example, a reinforcement
learning agent was trained to play the video game road runner. it was penalized for losing in level two,
so did it just become fantastic at the game? not quite. the agent discovered that it could kill itself
at the end of level one to avoid losing in level two, thus exploiting the reward system
without actually improving its performance. [9438](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9438.62s)

what would happen if it was tasked to keep
us from hitting some tipping point? by the way, is living more valuable than not
living? what's the rational answer to this? this is perhaps the most important fundamental
question. with regard to the topic of ai alignment,
right, because if we tell the ai, maximize the number of rat tails, then it could, like
bostrom's [[paperclip maximizer]], start clear-cutting forests to grow massive [[factory farm|factory farms]] of rats
and whatever. you can do the reducto ad absurdum of a very
powerful system. and so then the question is you say, okay,
well, do the rat tails or the gdp or whatever it is while also factoring this other metric. [9494](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9494.05s)

okay, well, you can do those two metrics and
there's still something armed. what about these three? the question is, is there a finite, describable
set of things that is adequate for something that can do optimization that powerfully? that is a way of thinking, and so it's, is
there a finitely describable definition of good is another way of thinking about it,
right, or in terms of optimization theory. yeah, that's something i think about, the
misalignment problem. is it in principle impossible to make the explicit what's implicit? when we state a goal, it carries with it manifold [9531](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9531.14s)

unstated assumptions. for instance, i say, bring me coffee or bring
me uber food. we imply indirectly, don't run over a pedestrian
to bring me the uber food. don't take it from the kitchen prior to it being packaged. don't break through my door to give it to
me. and we cloak all of that and say, that's just
common sense. common sense is extremely difficult [9551](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9551.83s)

to make explicit. even object recognition is extremely difficult. and then as soon as we can get a robot to do something that is
human-like, then it becomes more and more black box-like. and then you have this huge problem of interpretability
of ai. so it is an extremely difficult problem, and i wonder
how much of the misalignment problem is just that. is it just the fact that we can't make explicit
what's implicit, and we overvalue [9570](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9570.61s)

how much the explicit matters, and implicit
is far more complex. i don't know. this is just something that i'm putting out there
and asking. in other words, to relate this to what you
were saying is, is it finite? and even if it's finite, is it like a tractable amount
of finiteness that either we can handle it or we can design an ai that we feel like we
have a handle over that can understand it? yeah. and if you try to say, okay, can i mine myself,
my brain for all the implicit assumptions [9596](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9596.23s)

and put them all? i think every version of the [[thought experiment]],
you realize you can't. but even if you do, that's only the ones that
are associated with the kinds of context you've been exposed to so far. but there are a heap of things that nobody
has ever done that maybe an ai could do that now also have to be factored
in there that you didn't think to say because it was never something that happened previously
where there is evolved knowledge to say don't do those things. there's also something that because humans
all co-evolved and have similarish [[nervous system|nervous systems]] and all kind of
need to breathe oxygen and want a world that has similar physics and whatever, there's
some stuff where the implicit processing is kind of baked into the [[evolutionary process]]
that brought us that is not true for a silica-based system that is not subject to the same physical
constraints, right? optimize itself in a very different physical environment. and so even the thing that we would call just
kind of an intuitive thing is very different for a
very different type of system. so i would say when it comes to the topic of agi alignment, there
are different positions on alignment. i would say the strongest position is agi alignment
is not – well, first we actually have to discuss what we even mean by alignment, right? because initially, the topic of alignment means can we ensure that the ai is aligned
with human values and human intentions so that when you say bring me a cup of coffee
that you're – all those implicit intentions that you have are not damaged in the process. but if we look at the – all of the animals and [[factory farm|factory farms]] and the extinct species
and the disruption to the environment and the conflict between humans and other humans
and class subjugation and all those things, you can say human intent is not unproblematic. [9729](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9729.84s)

and exponentiating human intent as is is not actually an awesome solution. and so do you want it to be aligned with human
intention? well, it currently looks like human intention
has created a social sphere and a technosphere that is fundamentally misaligned with the
biosphere they depend upon and it is the technosphere social sphere complex is kind of auto-poetically
scaling while debasing the substrate it depends upon. in other words, it's on a self-termination
path. so – and that represents something like the collective intent of humans currently
in this context. [9773](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9773.92s)

so if you ensure that the ai is aligned with intent in the narrow obvious
definition, that is also not a good definition of alignment. so insofar as the humans are not aligned,
their intent is not aligned with the biosphere they depend upon and is not
aligned with the well-being of other humans who will produce counter responses. and most of the time isn't even aligned with
their own future good as is the case with all addictive
behavior. right? sorry to interrupt. i'm so sorry. is this a place where you disagree with yudkowsky or has he also expressed points that are in
alignment with your point about alignment? i don't know if he has. there's nothing that i know of that i disagree
with. i think when he's – i'm sure he's thought about
this. i just haven't read that of him. when he's talking about alignment, he's talking
about this more basic issue [9829](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9829.13s)

of as he tries to give the example, if you
have a very powerful ai and you ask it to do something that would be very hard for us
to do but should be a tractable task for it like replicate the strawberry at a cellular
level, that can you make an ai that could do that that doesn't destroy the world in
the process? even that level, not being clear how to do it at all is the thing he's generally
focused on. i'm sure he has deeper arguments beyond it that if we got that thing down,
what else would we have to get? so one could say – like if we look at all
of the [[social media]] issues that the social dilemma addressed where you can say – facebook
can say we're giving people what they want [9876](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9876.39s)

or tiktok or the youtube algorithm or instagram
or whatever because we're not forcing people to use it except it's saying we're giving
people what we want in the same way that the drug dealer who gives drugs to kids is saying
that, right? which is we can create addiction. we can kind of prey on the lower angels of
people's nature and if they're individual people who don't even know they're in such
a competition and we're talking about a major fraction of a trillion-dollar organization
employing supercomputers and ai in an asymmetric warfare against them to say we are giving
them what they want while engineering what they want. that's – it's a tricky proposition. [9923](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9923.49s)

but we can see how the algorithm that optimizes for – whether it's time on site or engagement,
both have happened. that's a perverse metric, right? because you can get it through driving addiction
and driving tribalism and driving fear and limbic hijacks and all those things. what's important to acknowledge, that's already an ai, right? it's already a type of [[artificial intelligence]]
that is taking personal – collecting personal data about
me and then looking at the totality of content [9950](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9950.55s)

that it can draw from and being able to create
a news feed for me that continues to learn based on what is stickiest for me, right? what i engage with the most. now in this case, the ai isn't creating the
content. it's just choosing which content gets put in front of people. in doing so, it is now incentivizing all content
creators to create the content that does best on those
algorithms. so it's actually in a way farming all human content creators because it's incentivizing
them to do whatever it is that [9986](https://www.youtube.com/watch?v=g7WtcTATa2U&t=9986.04s)

is within the algorithm's bidding. now as soon as we have synthetic media, which
is rapidly emerging, where we can not just have
humans creating whatever the tiktok video is, but we can have deep fake versions of
them that are being created very rapidly. and now you have a curation ai where that
first one's ai was just to curate the stickiest stuff personalized to people and creation
ai's that can be creating multiple things to split test relative to each other and the
feedback loop between those, you can just see that the problem that has been there just
hypertrophies. let alone the breakdown of the epistemic comment and the ability to tell
what is real and not real and all those types [10030](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10030.06s)

of issues. i want to come back to where we were. so obviously the curation algorithm is saying that it's aligned with human intent,
but not really. it's aligned with human intent because it's giving stuff that they
empirically like because they're engaging with it. but most people then actually end up having
regret of how much time they spend on those platforms and wish that they did
less of it. and they don't plan in the day, [10060](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10060.35s)

i want to spend this much time doom scrolling. and so is it really aligned with their intent? and in general, is aligning with intent that
includes the lowest angels of people's nature type intent, is that a good thing? is that a good type of alignment when you
factor the totality of effects it has? so we could say that the solution to the algorithm
issue should be that because the [[social media]] platform
is gathering personal data about me, and it's gathering based on its ability to model my
psyche based on all of who my friends are and what i like and what i don't like and
all those things and my mouse hover patterns. [10105](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10105.149s)

it has an amount of data about me that can
model my future behavior better than a lawyer or a psychotherapist or anybody else could. so there are provisions in law of privileged information, right? if you have privileged information, what are
you allowed to do with it? and there are provisions in law about undue
influence. so we could argue that the platforms are gathering privileged information,
that they have undue influence. as a result, there should be a fiduciary responsibility. this is one of the things that we do when there's a radical asymmetry of power. because if there's a symmetry of power, we
say caveat emptor buyer beware. it's kind of on you to make sure that you
don't get sold a shitty thing or engage with. but if there's a radical asymmetry of power,
can you tell the kid buyer beware about an adult that is playing them? no, you can't, right? [10157](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10157.479s)

and so in that way, can the person who isn't a doctor know that
they really don't need a kidney transplant if the doctor tells them that they do because
the doctor gets paid when they give kidney transplants? well, that's so bad. we don't want that to happen. we make law saying doctors can't do that. there's a hippocratic oath to act not just
in their own economic interest, but they are an agent on behalf of the principal
because the principal cannot buyer beware, [10180](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10180.58s)

right? and so then there is a board of other doctors
who are also at that upper asymmetry who can verify did the person do malpractice
or not. same with a lawyer. if the lawyer wanted to just bill by the 15-minute sections
maximally to drain as much money from me, they could because there's no way i can know
that what they're telling me about law is wrong because they have so much asymmetric
knowledge relative to me that we have to make that illegal. we have to make sure that the lawyer is an
agent on behalf of me as the [10213](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10213.17s)

principal. so with lawyers and doctors and therapists
and financial advisors, we have this fiduciary principal agent binding thing,
right? and it's because there's such an asymmetry that there cannot be self-protection, right? if i'm engaging with them and giving them this privileged information and they wanted
to fuck me, they could, right? and so but for my own well-being, i have to engage with
them and give them this information. so we have to have some legal way of binding that. but of course, in the case to bind it where the lawyers all have some practice law that
they can be bound by, they can be shown they did malpractice and same with doctors, that
requires a legal body of lawyers or a body of doctors that can assess if what that doctor
or lawyer did was wrong. so somebody else that has even higher asymmetry, right? the group of the top things. this becomes very hard when it comes to ai. [10269](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10269.27s)

so let's start by saying the rather than the
ai being a rivalrous relationship with me, when i'm on [[social media]],
and it is actually gathering the information about me not to optimize my well-being, but
to optimize ad sales for other for the corporation that is the platform and the corporations
that are its actual customers. right? in which case it has the incentive to prey on the lowest
angels of my nature and then be able to say it was my intent and i had free choice. so we could say that should be a violation
of the principal agent issue. [10309](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10309.63s)

and because there's undue influence, we can
show there's undue influence. consilience project, we wrote some articles
on this. there's one on undue influence that makes this argument more deeply. and because you can show it's gathering privileged information, it should be in fiduciary relationship
where it has to pay attention to my goals and optimize aligned with my goals rather
than i'm the product and it's optimizing with the goals of the corporation or its customers. right? in order to do that, that would change its business model. it couldn't have an ad model anymore. i would either have to pay a monthly fee for it, or the state or some
commons would have to pay for it and everybody had access to it or some other thing. that seems like a very good step in the right
direction. and that is an alignment issue thing. right? the principal agent issue is a way of trying [10362](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10362.23s)

to solve the alignment, which is to say that
this more powerful ai system here, the curatorial ai, [[social media]], would be aligned with my
interest in bound in some way. and maybe that would be a way to, maybe we would extend that
to all types of ai. well, of course, in the agi case where it becomes fully autonomous
and becomes more powerful than any other systems, what other system can check it to see if what
it is doing is actually aligned or not? there isn't a group of lawyers that can check that
lawyer. right? so that becomes a big issue. and if it really becomes autonomous, as opposed
to empowering a corporation, which is what it is, it's different. and so this is one part on the topic of alignment
and alignment with our intention or well-being. you can do superficial alignment with our intention, which the [[social media]] thing already
does, but it's not aligned with our actual well-being because an asymmetric agent is
capable of exploiting your sense of intentionality. so, and similarly, when you say there's a
common sense that says, don't bring the [10428](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10428.109s)

door down, you're bringing me coffee, there
should be a common sense that says, don't overfish the entire ocean and cut all the
damn trees down and turn them into forests in the process of growing gdp. and there is clearly not. right? and so we can see that the current, without ai, human [[world system]]
already actually doesn't have that kind of check and balance in it, in all the areas
that it should, just so long as the harms are externalized somewhere far enough that
we don't instantly notice them and change them. so the question of what do we, if we have
radically more powerful optimizer than we already have, what do we align its goal
with? if we just say align it with our intention, but it can change our intention because it
can behavior mod me and we can't possibly deal with that because of the asymmetry, that's
no good as in the facebook case. if we try to align it with the interest of
a [[nation state]] that can drive [[arms race|arms races]] with other ones and other [[nation state|nation states]] and war
or drive it, align it with the current economy that's misaligned with the biosphere, that's
not good. so the topic of alignment is actually an incredibly deep topic. [10505](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10505.09s)

and this now gets to what you've probably
addressed on your show in other places. it gets to a very philosophic issue, which
is the kind of is-ought issue. which is science can say what is, it can't
say what ought, right? and that kind of distinction by mill and others historically, and that
the applied side of science is technology and engineering can change what is, but what
ought to be, what is the ethics that is somehow compatible with science is a challenge. the best answer we have had, arguably, that came from the mind that created both
a lot of our nuclear technology and our foundations [10544](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10544.22s)

of ai, von neumann, was [[game theory]], right? that the idea that is good is the idea that
doesn't lose. and we can arguably say that that thing instantiated
by markets and [[national security]] protocols has actually been the dominant definition
of ought that ends up driving the power of technology. because if science says, we can't say what
ought, we can't. we can only say what is. but we're really fucking powerful at saying
what is in a way that reduces to technology that changes what is, where we can optimize some
metrics and say it's good, even if we externalize a lot of harm to other metrics or optimize
in groups with expensive outgroups or whatever it is, right? but we say that not only do we not have an
ought, but that any system of ought is not the philosophy of science. so is, insofar as that's concerned, out of
scope or gibberish, well, then what ends up guiding the power
of technology? markets do. and to some extent, [10612](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10612.21s)

[[national security]] does. in other words, rival risk game theoretic
kind of interests. and so what gets researched, the thing that
has the most market potential. and so then, again, it is what is actually
developing the technology? because as einstein said, like, i was developing science not knowing
it was going to do that application, didn't want that application, wanted science
for social responsibility. but what ends up, for the most part, the research that gets
funded is r&d towards something that ends up either [10646](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10646.54s)

advancing the interest of a [[nation state]] or
the interest of a corporation or whatever the metric set, the game theoretic metric set of the
group of people that is doing the thing, right? and so what i would say is that as we get
to more and more powerful is, more and more powerful science that creates more and more powerful
tech and exponentially powerful tech, especially as we're already hitting fragility of the planetary
systems. and when we say more powerful, we mean like exponentially more powerful, not iteratively
more powerful. [10674](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10674.701s)

you have to have a system of ought powerful enough to guide, bind, and
direct it. because if you don't, it is powerful enough to, in whatever it is optimizing for, destroy
enough that what it optimizes for doesn't matter anymore. now, this is a fundamentally deep metaphysical
philosophical issue. and of course, when we talk about regulation, law, the basis
of law is jurisprudence, right? and is ought questions, [10711](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10711.33s)

right? applied ethics that get institutionalized
for exactly this reason. and so when we say if we have tech that is powerful enough to do pretty
much fucking anything, what should be guiding that and what should be binding it? and if we don't answer those well, what is
the default of what will be guiding it and binding it currently? and what does that world look like? so this is super cheerful conversation. [10741](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10741.22s)

what is the call to action? okay, we're quite a far ways away from that. let me try to expedite a couple other parts. when we were mentioning the ai risk, we said ai empowering
bad actors. so you can think about whether a bad actor is a domestic terrorist who the best
thing they can do right now is get an ar-15 and shoot up a transformer station to take down the
power lines. ar-15 is a kind of tech that has a lot of [10770](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10770.85s)

capability. it's a kind of tech that has some capability. as you have more people getting a sense of being disenfranchised by the current
system and being motivated to utilize what is at their resources to do something about it,
and the barrier of entry of the more powerful tech is getting lowered, you can put those things
together, right? and – but whether you're talking about that or you're talking about international
terrorism from larger groups, or you're talking [10803](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10803.12s)

about full military applications. but let's just go ahead and say – and like,
can we make deep fakes that make the worst kinds of confusion,
conspiracy theory, in-group, out-group thinking, propaganda, of course, right? like, that is an emerging technology that's
eminent. can we use people's voices and what looks like their
video and text for ransom and fucked up stuff? can we – like, so you can think of all the bad actor
applications, and then you can pretty much apply it to – this is a piece of theory
i wanted to say. [10847](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10847.51s)

every technology has certain affordances. if you build it, it can do things, where without
that technology, you couldn't do those things, right? a tractor allows me to do things that i couldn't
do without a tractor, just the shovel, in terms of volume and types of work and various
things. every technology is also combinatorial with other tech because what a hammer can
do, if i don't have a saw to cut the timber first, is very different than what it can do if you
have that. and obviously, it requires the blacksmithing [10882](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10882.67s)

hammer, right? so, you have – you don't just have individual
tech, you have tech ecosystems. and the combinatorial potential of these pieces
of tech together have different affordances, right? so – but then what do we use it for is based
on the motivational landscape. i can use something like a hammer to be jimmy
carter and build houses for the homeless with habitat for humanity, or i can use it as a
weapon and kill people with it. and so, the tech has the affordances to do
both of those. so, the tech will be developed and utilized
based on motivational landscapes. sure. and just briefly, and going back to earlier,
it's not just dual because that would be double-edged, it's multipolar. omni. yes. okay. so, what we can say is the tech will end up
getting utilized by all – potentially getting utilized by all agents for all motives, that
that tech offers affordances relevant to their motives. right? and so, when we're building a piece of tech,
we don't want to think about what is our motive [10944](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10944.21s)

to use it. we want to think about are we making a new
capacity that didn't exist before, lowering the barrier of entry to a particular kind
of capacity where now what are all the motives that all agents have who have access to that
technology and what is the world that results from everybody utilizing it that way? that's factoring second, third, fourth order
thinking into the development of something – a new capacity that changes the landscape
of the world. i would say that every scientist who is working
on synthetic bio for curing cancer or ai for solving some awesome problem, every scientist
and engineer and et cetera has an entrepreneur, an ethical responsibility to think about the
new capability they're bringing into the world [10989](https://www.youtube.com/watch?v=g7WtcTATa2U&t=10989.59s)

that didn't exist, not just how they want
to use it, but the totality of use that will happen by
them having brought it into the world that wouldn't have they not. there is no current – when i say there's
an ethical responsibility, there is no legal responsibility. there is no fiduciary responsibility where
you are liable for the harms that get produced by a thing that you bring about that someone
else reverses and uses a different way. but there is financial incentive and nobel
prizes for developing the thing for your purpose and then, again, socializing the losses of
whatever anybody else does with it. so this is one of those cases where the personal
near-term narrow motive – this is us being [11030](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11030.25s)

fucking narrow ais in an ethical sense – is
to do the thing even if the net result of the thing ends up being catastrophically harmful. so the incentive deterrent, the motivational
landscape is messed up. so every tech – now i want to make a couple
more philosophy of tech arguments. tech is not values neutral, meaning the hammer
is not good or bad. it's just a hammer, and whether you use it
to build a house for the homeless or beat someone's head is up to you. the motivational landscape and the tech have
nothing to do with each other. this is not true. if a technology gives the capacity to do something
that provides advantage, relative advantage [11071](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11071.21s)

in a competitive environment, whether it's
one nation-state competing with another nation-state or one corporation or one tribe with another
one. if it provides significant competitive advantage,
if you use it a particular way, then anyone using it that way creates a [[multipolar trap]]
that obligates the others to use it that way or a related way. and so we end up getting a couple things. this is the classic example i've used a lot
is if we think about – and it's because there's been so much analysis on this example. if you think about the plow as a technology
that was one of the key technologies that moved us from sub-[[dunbar number]], hunter-gatherer,
maybe horticultural subsistence cultures to large agricultural civilizations. the plow is not a neutral technology where
you can choose to use it or not choose to use it. the populations that used it made it through
famines and grew their populations way faster than the ones who didn't, and they used their
much, much larger populations to win at wars. so the meanset that goes along with using
it ends up making it through evolution. the meanset that doesn't doesn't make it through
evolution. and correspondingly, there are – in order
to implement the tech, it has ethical consequences. i had to clearcut in order to do the kind
of row cropping that the plow really makes possible. i have to get an open piece of land that is
now being used for just human food production and not any other purpose, so i'm going to
clearcut the forest or a meadow or something to be able to do that. i'm going to – so i'm already starting the
anthropocene with that, right, changing natural environments from whatever value – whatever
habitat and home they were for all the life that was there to now serving the purpose
of optimizing human productivity. and i have to yoke an ox, and i probably have
to castrate it and do a bunch of other things to be able to make that work and probably
beat the ox all day to keep pulling the plow. in order to do that, i have to move from the
animism of i respect the great spirit of the buffalo and we kill this one with reverence
knowing that as it nourishes our bodies, our [11218](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11218.92s)

bodies will be put in the dirt and make grass
that its ancestors will eat and are part of the great circle of life and whatever kind
of idea like that to it's just a dumb animal. it's put here for human purposes, man's dominion
over – it doesn't have feelings like us, that kind of thing, which then spills over
to it's just – it's not like us, so we remove our empathy from it and we apply that to other
races, other classes, other species, other whatever, right? so, something like the plow is not values
neutral. to be able to utilize it, i have to rationalize
its use realizing it creates certain externalities and if i see those externalities, i have to
have a [[value system]] that goes along with that. wait, sorry. [11262](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11262.9s)

to be particular with the word choice, it's
not that the plow is not value neutral. it's the use of the plow. exactly, exactly. and that the plow doesn't use itself, right? and so, the use of the plow, not values neutral. now, a life where i am hunting and gathering
versus a life where i'm plowing are also totally different lives. so, it codes a completely different behavior
set. in doing that, it makes completely new mythos,
which is why the hunter-gatherer mythos and the agrarian mythos are completely different,
right? [11292](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11292.56s)

and they have different views towards men
and women and towards sky gods versus animism and towards all kinds of things. and so – but the other thing is that it
provides so much game-theoretic advantage of those who use it relative to those who
don't that when they hit competitive situations, that's why there are not many hunter-gatherers
left and why the whole society went agricultural. so, it's not just that the tech – so, the
tech codes – the tech requires people using it, which changes the patterns of human behavior. changing the patterns of human behavior automatically
changes the patterns of perception in human psyche, metaphors, cultures, etc. and the externalities that it creates and
the benefits that it causes become implicit [11333](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11333.05s)

to the [[value system]] because the [[value system]]
can't be totally incommensurate with the power system, right? and so the dominant narrative ends up becoming
support for – one could argue apologism for the dominant power system because we can't
feel totally bad about how we meet our needs. so we have to have a [[value system]] that deals
with the problems of how we do so. but then it's also that the tech that does
this becomes obligate because when anyone is using it, everyone else has to or they
kind of lose by default. so when you recognize that tech affects – that
technology when utilized affects patterns of human behavior, humans now do the thing
they do with the tech. so people do this thing and they didn't used
to do this thing, right? [11380](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11380.01s)

on the cell phone or whatever. to get the benefits of the tech, you have
a totally different pattern of human behavior. as a result, you have different nature of
mind. you have different [[value system|value systems]] that emerge
and it becomes obligate or some version of a compensatory tech becomes obligate because
the amish are not really shaping the world. they're no longer engaged in the great power
competition. i have a bone to pick there. i had watched a few months ago and i don't
know anything about the amish or didn't know anything about the amish and i'm just someone
who grew up in this city and so i dismissed them as luddites like we've used that term
several times and they're backward. [11413](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11413.899s)

they don't know what they're talking about. and then i watched a video. the amish aren't idiots. they're not asinine. there's a reason why they do what they do
and they either explicitly or intuitively understand that the technology changes the
social dynamics in the way that they view the world. totally. and has ethical considerations. but that influenced me. [11435](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11435.84s)

that influenced perhaps millions of people
because it's a video i think has a few million hits. even if they're local, just them saying, you
know what, i don't care. i'm going to continue to act right. it can still influence outward. anyway, and we're talking about it now. maybe this will influence people and hopefully
to something positive and hopefully to myself something positive. yeah. okay, so it's not that you come to this a
few times, which is even if you have a meme [11457](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11457.71s)

plex that is not, that doesn't become part
of the dominant system, can it infect or influence the meme plex in a way that steers it? yes. but one does not want to be naive about how
much influence that's going to have. they want to be thoughtful about exactly how
that will work and what kinds of influences. we mentioned not all of buddhism got picked
up everywhere, right? like the parts that had to do with why people
should take vows of poverty and live on very little, that didn't really get picked up. the parts on how to reduce stress got picked
up. the parts on what a healthy motive is didn't
get picked up as much as the parts on how [11495](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11495.56s)

to empower your motive through a more functional
mind. so it's important to get the memes might live
in a complex, in a context when they influence, parts of them are going to interact with another
meme plex and the techno plex and everything else. and so you are right to say that it's not
that they have no influence, but obviously the amish, not speaking to that they're dumb
and backwards, but that in their don't want to engage tech for these reasons argument,
they don't have a significant say in whether we engage a particular nuclear war or not. or they were not the ones that overfish the
ocean, cause [[species extinction]], but they also couldn't stop it. [11542](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11542.01s)

they are not the ones that are creating synthetic
biology that can make totally new species. so the – and this is why i say there is
a naive techno optimism that focuses on the upsides and doesn't focus on all the nth order
effects and downsides. and as we were just mentioning, the externalities
of tech are not just physical, right? you do this mining to get the thing you want,
but there's a lot of mining pollution or the herbicide does make farming easier in this
way, but it harms the environment and human health in this other way. that's physical externalities. but you also get these psychosocial externalities. you use facebook for this purpose and it ends
up eroding democracy and doubling down on [11574](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11574.92s)

bias and increasing addiction and body dysmorphia
and things like that, right? so the tech affects not – it doesn't – it
has effects that are not the ones you intended, some of which might be positive. you can have a positive externality, but it
might have a lot of negative externalities. and those negative externalities can cascade,
second, third, fourth order effects. so there's a naive techno optimism that doesn't
pay enough attention to that. there's a naive techno pessimism that says,
well, i'm aware of those negative externalities. i don't want those for our people. we think we can isolate our people from everybody
else and say we're going to not do that. but we're going to have decreased influence
over what everyone who is doing that has, [11612](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11612.45s)

right? which is what then some of the ai labs argue
is there's an [[arms race]]. we can't stop the [[arms race]] on it. and so only being at the front of the arms
race can we steer it. i would argue that that is a naive version
of that particular thing, but nonetheless. so if we want to – and one way of reading
one of the problematic lessons of the elves in tolkien is – and i'm just making this
as like a toy model – is in some ways, they figured out how to have a nicer life than
the humans and dwarves and whatever else. they were able to do radical life extension
and figure out great gdp per capita where the poorest people were doing well. [11662](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11662.76s)

and they were so kind of – but they became
insular because they were so disenchanted with the world of men and elves and whatever
that they're like, fuck it. we're just going to kind of isolate and do
our own thing our own way. but it ends up being that you're still all
sharing middle earth. and the problem somewhere else can cascade
into catastrophic problems that end up messing up your world too. so you can't be isolationist forever in an
interconnected world. so they actually had to – they were kind
of obligated if we rewrote the story to take whatever they had learned and try to help
everybody else have it or have enough of it that you didn't get work dominance and stuff
like that. [11696](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11696.74s)

so basically arguing that a isolationist – we
see a problem. we're going to avoid that for ourselves – doesn't
work with planetary problems. and so i'm not interested in the naive techno-negative
versions that say because we see a problem with tech, we're not going to do it. but we're also going to kind of lotus-eat
in the process and not engage with the fact that we actually care about what happens for
the world overall and we have to engage with how the world as a whole is doing that thing. makes sense what i mean by the naive techno-pessimism. and it is that you do not get to do effective
isolationism on an interconnected planet that is hitting [[planetary boundary|planetary boundaries]] with exponential
tech. yeah, i guess what i'm trying to express is
that even the amish with what they're doing, it's not as simple as the meme-plex that's
exported by the amish is the amish meme-plex. there's something else that even influenced
them. even yourself, you may be in a position that
saves earth at least for now from a hugely catastrophic event. same with yodkowsky and same with some others. but what influenced you? there's some good in you, hopefully, that
was influenced by something else, by something else that's good, which also influenced the
amish. each person is corrupt in some way. [11774](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11774.189s)

so i'm saying that there's something that's
like the unity of virtues that influences us. and that as long as we go back and we think
or constantly we're assessing ourselves and saying, is what i'm doing good? then these other meme-plexes that are being
thrown to us, yes, it's in a different context. somehow it can orient and pick up the good. we're completely on the same page, which is
that that happens, does not always happen, and that that is an important thing to have
happen. but if that happened adequately or at the
– yeah, i will say adequately, then we wouldn't have extinct all the species that we have. [11815](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11815.189s)

we would not have turned as many old growth
forests into deserts. we would not have had as many genocides and
unnecessary wars and et cetera. so seeing the failure in where either someone's
definition of good is too narrow, get our god to win and fuck everybody else or grow
gdp and that'll take care of everything. we can well-intendedly pursue a definition
of good that's too narrow and externalize harm unintentionally. we can pursue a definition of good that we
really believe in that other people don't believe in and our answer is to win over them,
but it creates an [[arms race]] where now we're in competition over the thing. or where there are people who are really not
pursuing the good of all even – they're [11854](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11854.12s)

not even trying to. whether it's sociopathy from a head injury
or genes or trauma or whatever it is, they are pursuing a different thing. but they're good at acquiring power, and this
is actually a very important thing is that the psychologies that are – that want power
and are good at getting it and the psychologies that would be the best stewards of power for
the well-being of all are not the same set of psychological attributes. they're pretty close to inversely correlated. so those types of things have to be calculated
in this. so what you're saying right now, which is
great, you're saying it is that there is some [11901](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11901.75s)

odd impulse that is not only an impulse, right? that you're calling a universal virtue or
good or something. and you're saying that some people feel very
called by that and that that's important. i agree completely. now, why is that historically and currently
not a strong enough binding is the important question. why has that not been a strong enough binding
for all the species that are extinct and all the animals in [[factory farm|factory farms]] and all the disruption,
et cetera? and then what would it take for it to become
a strong enough binding or the nature of the question here? [11947](https://www.youtube.com/watch?v=g7WtcTATa2U&t=11947.54s)

and that's actually at the heart of the [[metacrisis]]
question is to have – like what is a system of ought that is actually commensurable with
the system of is and what is a way of having that actually sufficiently influence behavior
such that the catastrophic behaviors don't occur? and that the nature of the influence is not
so top-down that it becomes dystopic and that's something like is there either a – so one
way of thinking about this is i've mentioned the term a couple of times [[superstructure]],
[[social structure]], infrastructure. that comes from marvin harris' work on cultural
materialism basically saying every civilization you can think of in those ways, what is its
kind of mean plex, what is its social coordination strategies, and what is the physical tooling
set upon which it depends. [12001](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12001.21s)

and different social theorists will say which
of these they think is most fundamental. the [[value system|value systems]] are ultimately what steer
behavior and determine the types of tech we build and the types of societies. religious thinkers think there, right? enlightenment thinkers think there. the [[social system]] actually, whatever you incentivize
financially is what's going to win because whether it's good or not, the people who do
that get the money, can incentivize more people, create the law, et cetera. so ultimately the most powerful thing is the
social coordination systems. and then the other schools of thought say
no, actually the thing that changes in time [12036](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12036.93s)

the most is the tech. and the tech influences the patterns of human
behavior, values, everything else. and so – and that's actually what marvin
harris roughly was saying was that the change in the tech plex ends up being the most influential
thing to the change because it does affect world views and it does affect [[social system|social systems]]. in the way we already mentioned, the change
of the tech plex of the [[printing press]] affected both world views and [[social system|social systems]], so did
the plow, so did the internet, so is about to be ai. i would argue that these three are interacting
with each other in complex ways. they all inter-inform each other and what
we have to think about is what changes in [12077](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12077.8s)

all three of them simultaneously, factoring
all the feedback loops, produce a virtuous cycle that orients in a direction that isn't
catastrophes or dystopias. it's the right way of thinking about it. and ultimately, the direction actually has
to be the [[superstructure]] informing the [[social structure]], informing or guide, bind, direct
the infrastructure. sorry. can you repeat that once more? yeah. right now, especially post-[[industrial revolution]],
physical technology, infrastructure had way faster feedback loops on it than the others
did, right? [12121](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12121.8s)

and because of that, it started breaking the
previous industrial-era tech at massive scales with those externalities, whatever can't be
managed by agrarian era or hunter-gatherer era world views and political systems, right? so we ended up getting a whole new set of
political systems, both nation democratic, [[liberal democracy]] and capitalism and social
communism emerging as writing the [[industrial revolution]] and what should be the political
economy that governs that thing, right? but the feedback loops from tech and specifically
whether it's a nation-state caught in [[multipolar trap|multipolar traps]] that's building the tech in a central
government communist type place or a market building it but that has [[perverse incentive|perverse incentives]]
built into what its incentive structure is. that has influence on our [[social structure|social structures]]
and our cultures, [[superstructure|superstructures]]. that – we could say the dominance of that
direction is one way of thinking about the driver of the meta-crisis. now, the other direction, if we are to say,
no, these examples of the tech won't be built or we're not going to use the tech in these
ways, right? we're not going – yes, you can use a tech
that extracts some parts of rocks from other parts of rocks. it gives us metal we want but also gives a
lot of waste. no, you can't put all that waste in the waterway,
right? or you can't put that pollution there or you
can't cut all the trees down in that area because we're calling it a national park,
right? [12220](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12220.34s)

that law or regulation that is not just a
tech thing. that's the social layer. so that layer has to bind the tech and guide
and direct it, say, these applications and not these ones. yeah. right? yeah. so if the [[social system]] is not an instantiation,
if the [[social structure]] is not an instantiation of the [[superstructure]], i.e. it's not an instantiation
of the will of the people, then it will be oppressive, right? [12246](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12246.0s)

that's why the idea of democracy emerged out
of the idea of the enlightenment. which was, this was a kind of governance that
only worked for a comprehensively educated – and you read all the founding documents
– the comprehensive education had to be is and ought, right? it said you must have a moral education as
well as a scientific education and you must be schooled in the science of governance. and only a people like that – going back
to what we said earlier – could check the government, right? could both know the jurisprudence to instantiate
what is good law, could engage in dialectics to listen to other people's point of view
to come up with democratic answers. [12285](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12285.67s)

so it was the idea that there was a kind of
[[superstructure]] possibility which was some kind of enlightenment or era values that could
make a type of [[social structure]] that could both utilize the tech and guide it but also
bind its destructive applications. so when you're saying isn't there some [[superstructure]],
isn't there some sense of good that will make us bind our capacities, i would argue that
if the sense of good doesn't emerge from the collective understanding and will of the people
but is instantiated in government because we the technocrats know the right answer or
we the enlightened know the right answer, that will be oppressive. and people are right to be concerned by it. but if the collective understanding is i want
what i want and i don't care what the effects [12333](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12333.64s)

are or fuck those guys over there or i'm not
paying attention to externalities or whatever, then the collective will of the people is
too dumb to govern and misguided to govern [[exponential tech]] and will self-terminate. so you cannot have an uneducated, unevolved
set of values in a libertarian way guide [[exponential tech]] well. it has to be more considerate. it has to think through end order effects. but you also can't have a government that
says we are the enlightened ones and we figured it out and we're going to impose it on everyone
else without it being oppressive and tyrannical, which means nothing less than a kind of cultural
enlightenment is required long term. [12375](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12375.51s)

so the collective will of the people – now
this gets back to the alignment topic – is the will of the people aligned with itself
actually, right? is what i want factoring the effects of what
i want, the end order effects, which means how other people will respond to that and
all that comes from it. is what i want actually even aligned with
a viable future, right? and so that is when we're talking about getting
alignment right, alignment with my intention where my intention is that my country wins
at all costs where then china's like, well, fuck, we're going to do the same thing or
russia, et cetera, so you get [[arms race|arms races]]. that intent or my intent is i want more stuff
and keep up with the joneses and i'm not paying attention to [[planetary boundary|planetary boundaries]]. those intents are not aligned with their own
fulfillment because the world self-terminates for too long in that process. and so with the power of [[exponential tech]]
and the cumulative effects of industrial tech, we do have to actually get ought that combine
the power of that is and it can't be imposed. it does have to be emergent, which does mean
something like that sense that you're saying has to become very universal and nurtured,
right? and then has to also instantiate itself in
reformation of systems of governance. i love what you said. let's see if i can recapitulate it. there's tech at the bottom. [12468](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12468.53s)

there's a [[social structure]] here and then there's
culture. okay, so these are people up here. there's people in all three. there's people's values up here. so values are up here and then there's the
[[social structure]] over here and then there's tech over here. okay, so currently the tech influences the
way our society is structured, which also influences our values and a part of the meta
crisis is saying that that's upside down, but it shouldn't just be whatever values that
just get imposed onto the [[social structure]] onto the values have to somehow come from
someplace else. [12495](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12495.029s)

and then the mystics have their other they
have to be coherent with reality. sure. so the spiritual people may call this something
akin to god and the enlightenment people may say, i don't know, maybe there's some evolutionary
will that comes out. and if we just close our eyes and hope for
the best somehow that emerges, whatever it's called, it's not entirely us. it's not entirely our conscious selves. the conscious self would be the more humanistic,
the enlightenment way of thinking about it is that we can impose our own values. nietzsche had something similar to this. [12526](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12526.24s)

so i like this. i'm in agreement with it. i think we've just been using different terminology. you and i both know that when you, in many
ways, how to say this, an evolution of cultural worldview and values adequate to steward the
power of exponential technology in non-catastrophic or non-dystopic ways is happening in some
areas, but a regress is also happening in some areas. right? there is increasing left-right polarization. i thought you were going to say there's a
regress happening like in demand. [12569](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12569.75s)

so for instance, we generally think like it
has to be malthusian and the more that we use, the more the demand for that increases. and that's obviously removing some of the
more scarce objects like art and gold, which their value comes from scarcity. but there is like the largest health trend
right now is fasting. it's like we've gotten so much food that we're
like, let's just not have any food. and then there's also recycling. like, just imagine that we think about recycling
at all. so there is some recognition that, hey, look,
we're consuming too much. let's cut back. [12601](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12601.771s)

and so it's not purely just an exponential
function. it is we take into account the rate of production. well, so what we can see is that the percentage
of the total, let's go ahead and say us, but we could look at uae or nigeria or whatever,
various places, the percent of the us population that is regularly doing fasting is still a
relatively small percentage. and in the same way that – like it is true
that when there's a market [[race to the bottom]] that we saw in food, which hostess and mcdonald's,
et cetera, kind of won, which is how do we make the food more and more combinations of
salt, fat, and sugar and texture and palatability that maximize kind of stickiness and addiction,
which of course, if i have a fiduciary responsibility to shareholder maximization and the key to
that is to optimize the lifetime value of [12655](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12655.01s)

a customer, addiction is awesome. it's actually not awesome. it's legally obligate because of maximized
shareholder returns. so that created a [[race to the bottom]] where
rather than starvation being the leading cause of death, obesity was the leading cause of
health-related death in the west. okay. well, that bottom of the [[race to the bottom]]
also creates a race to the top for a different niche. so then whole foods becomes the fastest-growing
supermarket and biohacking and et cetera. so that's true. [12690](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12690.56s)

did that affect the overall population demographics
regarding obesity very significantly? not really. did it stop the [[race to the bottom]]? no. it just added another little niche race, which
also then separated – which created more class system separation. so it's not that those effects don't happen. are they happening at the scale and speed
necessary when we look at [[catastrophic risk|catastrophic risks]]? so of course i can also pay more for a post-consumer
recycled thing and there is more recycling happening. but there's also more net extraction of raw
resources and more net waste and pollution per year than there was the previous year
because the whole system is growing exponentially. so even if the recycling is growing, it's
actually not growing enough to even keep up with demand, right? so what i'm saying is now let's come – bring
that back to the memetic space, which is where i was. there are both evolution of values where people
are wanting to think through [[catastrophic risk]], [[existential risk]], planetary well-being
of everybody long term. so that's good. but there is also cultural kind of regress
where people are getting narrower [[value system|value systems]] with more antipathy towards other people they
share the planet with that have narrow [[value system|value systems]] on the other side. and left-right polarization in the us is one
classic example. and so the point is [[cultural enlightenment]]
is not impossible, but it's also not a given, right? the kind of memetic shift, and this is obviously
i think a big part of why you do the [[public education]] memetic work that you do is because
of having a sensibility about is it possible to support the development and evolution of
worldviews and people in ways that can propagate and create good. well, you're saying it much more benevolently. [12810](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12810.64s)

honestly, it's just selfish that i'm just
super, super, super curious about all of these topics. and by luck, some other people care to listen
and follow along. and i just get to elucidate myself. it's so fun. it bangs on every cylinder. and some other people seem to like it. i hope that what i'm doing is something positive. i hope that it's not producing more harm. i'm not even considering this is sent over
the internet and it's using up energy and [12834](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12834.68s)

– okay, what you just said takes somewhere that
i wanted to go that's very interesting. so we're talking about alignment and is a
particular alignment – is a particular, say, human intention aligned with the collective
well-being of everybody or even their own long-term future? at the base of the alignment problem is that
we are not aligned with the other parts of our own self, right? so from a kind of jungian parts conflict point
of view, motivation is complex because there's different parts of us that have different
motivations. and one way of thinking about psychological
health, the parts integration view, is the [12874](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12874.08s)

degree to which those different parts are
in good communication with each other and see synergistic strategies to meet their needs
that don't require that part of self's motivation harming another part of self. but they're actually doing synergetic stuff. so the whole of self pulls in the same direction. if you think of like the parts of self as
sled dogs, they can be pulling in opposite directions. you get nowhere. they're all choking themselves. so we can see psychological health and ill
health as how conflicted are the parts of [12902](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12902.28s)

our self versus how synergistic are they. synergistic does not mean homogenous. it doesn't mean we just have one motive. it means that the various motives find synergistic
alignment rather than – yeah, like our bodies are synergistic. our heart is not the same as the liver. exactly. now, in your heart, it's not going to optimize
itself. it delivers long-term harm. even though on its own, you could say it has
a different incentive, it is part of an interconnected [12926](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12926.63s)

system where that actually doesn't really
make sense. but a cancer cell will optimize itself or
its both itself, how much sugar it consumes and its reproduction cycles at the expense
of things around it. and in doing so, it actually is on a [[self-terminating]]
curve because it ends up killing the host and then killing itself. and so the cancer that does not want to bind
its consumption and regulation aligned with the pattern of the whole ends up actually
doing better in the short term, meaning consuming more and producing more. and then there's a maximum number of cancer
cells right before the body dies and they're all dead. [12965](https://www.youtube.com/watch?v=g7WtcTATa2U&t=12965.852s)

so there is a – if something is inextricably
interconnected with the rest of reality like the heart and the liver or the various cells,
but it forgets that or doesn't understand that and optimizes itself at the expense of
the other things, it can be on what looks like a short-term winning path but it self-terminates. it would be an evolutionary cul-de-sac. and i would argue that the [[collective action]]
failures of humanity as a whole are pursuing an evolutionary cul-de-sac. and so one way of thinking about this, when
we say we aren't even that aligned with ourself, we think of motive. we like to think of leaders. [13002](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13002.58s)

what is putin doing or what is biden or xi
doing in a particular thing? what is their motive? or what is the founder of an ai lab motive? motive will always be that each of the parts
of the self has a different motive. so typically like some unconscious part of
me still wants to get the amount of parental approval that i didn't get and then projecting
that on the world through some idea of success or to prove that it's enough or whatever. and some part of me is just directly motivated
by money. some evolutionary part is motivated by maximizing
mate selection opportunities. some part of me genuinely wants to do good. [13043](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13043.09s)

some part of me wants intellectual congruence. and so there can absolutely be a burn it all
down part. and this is why shadow work is important,
which is look at and talk to all of these parts and see how to get them to move forward
together. this is basically governance at the level
of the self. so i don't know if you ever watched and this
might be because we're long even though there's so much left to discuss a decent place for
us to wrap up on alignment. i would say one of the better a number of
the theorists who you have referenced on the show would be good references for what i would
consider the deepest drivers of the [[metacrisis]] and also what the alignment considerations. [13094](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13094.33s)

if you think of like in iain mcgilchrist's
work with the master and the emissary, the right hemisphere is the master and the left hemisphere
is the master's emissary. in his 2009 opus, the master and his emissary,
iain mcgilchrist discusses the distinct functions of the brain's left and right hemispheres. generally, there's plenty of "pop science
woo" around this concept, but then you can dispel by going even further to find the correctness
about it. the left hemisphere focus on processes such
as formal logic, symbol manipulation, and linear analysis, while the right hemisphere
is concerned with context awareness, the appreciation [13131](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13131.319s)

of unique instances, and topological understanding. hey, maybe there's some stone duality between
them, but i haven't thought much about this. john vervaeke's work, by the way, explores
the primacy of cognitive processes like relevance realization, aiming
to bridge the gap between analytic and intuitive thinking. both mcgilchrist and vervaeke emphasize
the importance of integrating the strengths of each hemisphere or modes of cognition when
attempting to tackle intricate problems such as the risks of ever more powerful ai's. the argument is that ai systems primarily
operate using left hemisphere capabilities like pattern recognition, logical reasoning,
and general optimization problems. [13178](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13178.93s)

however, they fail to adequately consider
the subtleties of human values and ethical implications, which thus leads to unintended
consequences. to mitigate ai risks and prevent an [[arms race]],
incorporating insights from both hemispheres and embracing context awareness is crucial. this requires interdisciplinary collaboration
between mathematicians, computer scientists, physicists, philosophers, neuroscientists,
and by the way, it's something that we're attempting in our humble manner on the theories
of everything channel. by exploring concepts in [[complex system|complex systems]] theory
and how it applies to our current unprecedented situation, we at least hope to understand
the interconnectedness of the factors at play in ai development. [13222](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13222.92s)

for instance, addressing ai risks can involve
analyzing multi-agent systems, considering network effects, and potential feedback loops,
which iain mcgilchrist would argue greatly benefits from your right hemisphere's contextual
understanding. we do not think ourselves into a new way of
living. we live ourselves into a new way of thinking. you could say, and i talked to ian about this,
and i said, so would you say that the meta-crisis as i formulate it is the result of getting
the master and the emissary wrong, which is kind of getting the principal and agent between
those two different aspects of human wrong? and he goes, yes, exactly. that's kind of the whole key, that there is
a function that he's calling the master that [13266](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13266.66s)

perceives the kind of unmediated field of
interconnected wholeness, multimodally perceives and experiences it. and then there is this other set of networks,
capacities, or dispositions that perceive parts relative to parts, name them, do symbol
grounding, orient more in the domain of symbol, and can do relevance realization. what part is relevant to a particular goal
i have, and salience realization, what things should be relevant to some goal and i should
be tracking, and information compression, which are largely things that we think of
as human intelligence, which of course ai is taking that emissary part and turning it
into an external tool. rather than that's the thing that makes tools
in us, now take that thing and make that as [13315](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13315.83s)

a tool, but also unbound by the master function,
the way he would call that. that's a very interesting way of thinking
about ai alignment and whatever, and that the master function that is perceiving the
unmediated, ground directly, not mediated by symbol field of interconnected wholeness,
that the other function that can do relevance realization, parts realization, salience realization,
info compression, basically utility function stuff. that that has to be in service of the field
of interconnected wholeness, if not, we'll upregulate some parts at the expense of other
ones, and the cumulative effect of that on an [[exponential curve]] will eventually bring
us to the [[metacrisis]] in self-terminate. i would say what mcgilchrist was saying was
expanding on what bohm said about the implicate [13360](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13360.729s)

order and wholeness. bohm's theory of wholeness and the implicate
order states that there is something like life and mind unfolded in everything. a tremendous number of ways in which one can
see unfoldment in the mind. one can see the thoughts unfold, feelings
unfold, thoughts, because given by a feeling will give rise to a thought, thoughts unfold
feelings, the thought that the snake is dangerous will unfold the feeling of danger, which will
then unfold when you see a snake, right? bohm was looking at the orientation of a mind
that mostly thinks in words, of western mind, you know, in particular, to break reality
into parts and make sure that our word, the symbol that would correspond with the ground
there, corresponded with the things that it [13409](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13409.67s)

was supposed to and not the other things,
so try to draw rigorous boundaries to, you know, divide everything up, led to us fundamentally
relating to everything as parts first. so how then, now we have this human mind that's,
you know, paleolithic and it's now put in a world where we have a different technology
that is relying on [[reward circuit|reward circuits]] that maybe are not as virtuous as we would like. is that where we are now in this conversation? and when bohm and krishnamurti did their dialogues,
which i don't know if you've watched those or some of my favorite dialogues in history,
bohm was basically answering what is the cause of all the problems, what's the cause of the
[[metacrisis]], he didn't call it that at the time. and he basically said a kind of fragmented
or fractured consciousness that sees everything as parts where you can upregulate some parts
relative to other ones without thinking about the effect of that on the whole, right? and that obviously comes from einstein being
one of his teachers, where einstein said it's an optical delusion of consciousness to believe
there are separate things, there is in reality one thing we call universe. regarding the theme of consciousness, it's
prudent to give an explication here, as often at least i found that mysteries arise because
we're calling different phenomenon by the same term. and this applies to consciousness, which doesn't
refer to just one aspect, but rather several [13488](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13488.42s)

that can be delineated. to further differentiate, i spoke to professor
greg henricks on this very topic. i'm attempting to delineate a few concepts,
that is adjectival consciousness, adverbial consciousness and phenomenal consciousness,
which i believe is the same as p-consciousness. but if that's not the same, then that's four
different concepts. so what are they? can you give the audience and myself an explanation
as to when are some satisfied but not others so that we can delineate? totally. yep. [13519](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13519.439s)

yeah. and actually, adjectival, adverbial are going
to, when we use p, when john and i certainly use p-consciousness, phenomenological consciousness
is reflecting on adjectival adverbial consciousness. and john refers to john vervaeke. john vervaeke, yeah. because we then did a whole series, untangling
the world knot, to make sure that our systems were in line, both in terms of our definitional
systems and our causal explanatory framework. so we did a 10-part series on, just the two
of us, on untangling the world knot, of consciousness. and then we did one on the self. then we did one on well-being. [13555](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13555.39s)

and we also did one on development and transformation
with zach stein. so all of we, our systems, i think, are now
completely synced up, at least in relation to the language structures that we have. and so i can tell you what we would mean by
adjectival adverbial consciousness, which then sort of is what most people mean by phenomenological
consciousness. okay. so if i understand correctly, one has to do
with degrees? mm-hmm. and then another has to do with a here-ness
and a now-ness? yeah, exactly. [13584](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13584.779s)

so actually, there's, i like to, so i would
encourage us to say, let's define conscious, there are three different kinds of definitions
of consciousness, okay, that i think of. the first definition of consciousness is functional
awareness and responsivity. okay. this is something that shows awareness and
can respond with control. and at the broadest definition, then even
things like bacteria can show a kind of functional awareness and responsivity. okay. that's the behavioral responsiveness. and when you say, hey, is that guy conscious? [13614](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13614.62s)

what you mean is he's not responding at all. okay. he's not showing any functional awareness and responsivity. he's either knocked out or blacked out or
asleep. okay. so when you say consciousness in that way, that's functional awareness and responsivity, which you can see from the outside and you
see in the way in which the agent's operating on the arena, [13633](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13633.29s)

because they're showing functional awareness and responsivity. okay. the second meaning of consciousness is subjective conscious experience of being in the world. the first person experience of being, and this is where the hard problem of consciousness comes online. and that's what most people mean by p-consciousness or phenomenological consciousness. [13655](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13655.159s)

it's a [[subjective experience]] of being, which is only available from the inside out. so that's, and then the final one is a self-conscious
access. okay. so that now i can be, know that i have had
an experience retrieve it, and then in its highest form
report on it. so self-consciousness, then is the capacity to recursively access one's phenomenological
thing and an explicit self-consciousness, which is what humans have and other animals
generally don't, [13686](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13686.279s)

is this capacity to say, curt, i am thinking about your question. i'm experiencing your face, and this is my narrative in relation. so that's explicit self-conscious awareness. uh-huh. just a moment. you said access, is that the same as access
consciousness or is that's different? no, that's ned block's access consciousness, [13704](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13704.84s)

which basically there's the, do you have the
experience? and then is there a memory access loop that
stores it and then can use it? so if i can gain access to it, that's sort of access consciousness as relates
to that. i want to make sure that i understand this. there's a door behind me. if i go and i access, is what i'm accessing
qualia? and is it the action of accessing that's called access consciousness? [13728](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13728.15s)

like the manipulation of data or is- right, it's the, well, it's basically, so you have awareness and then you have memory of the awareness that you know that some aspects of it knows that you were aware. so it's like, so you can imagine awareness without really, like one way of differentiating
it would be sort of, we have what a sensory perceptual
awareness that lasts say three tenths of a second to
three seconds. [13748](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13748.94s)

it's like a flash, okay? then you have working memory, which extends it across time and puts it on
a loop. that loop is what allows you to then gain
access to it and manipulate it. so working memory can be thought of then as
a part, as the whiteboard that allows continuous access to the flash. so there's a flash and then there's the extension and manipulation
of the flash [13773](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13773.75s)

which you then need access to, okay? the basic layers of this, what john and i
argue is that out of the body comes what we call valence qualia. valence qualia basically orients and gives
value to and can be thought of as in like pleasure
and pain in the body, okay? and it yokes a sensory state with an affective
state and points you in a direction. or yoke means? [13799](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13799.3s)

tie together, like to yoke stuff together. so this is the sort of the earliest form of
consciousness is probably of kind of valence consciousness,
okay? that basically pulls you, you know, oh, it feels good, feels bad kind of deal. gets me active, gets me passive. but it's this sort of like, hmm, this kind of felt sense of the body, okay? that's the argument from john and i's position is that that probably is the base [13824](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13824.88s)

of your subjective conscious experience or the base of your phenomenological experience,
okay? then what happens, and that would be maybe
present in, you know, in say reptiles, fish, maybe down into insects at some level, okay? then the argument would be in birds and mammals and maybe lower, we don't really know, but there's good reason to believe in birds
and mammals. you get a global workspace. the global workspace is when it extends [13853](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13853.41s)

from these sensory flashes into a workspace where you have access and recursive looping
on it. and it's the framing of that is the adverbial
consciousness is the framing and extension of that. the here-ness, now-ness, and togetherness that indexes the thing, pulls it together. that's what john calls adverbial consciousness,
okay? okay. and then it's what's on the screen of that adverbial consciousness [13881](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13881.51s)

is what john calls adjectival consciousness. so it's like, it's the screen of attention that orients and indexes, that's adverbial,
okay? and then what is actually the properties that you experience, that's adjectival. first, i came in with three questions and i have so many more. okay, this valence, is it purely pain and
pleasure or is there something else? are the third, fourth elements? [13905](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13905.26s)

there's certainly pleasure, pain, active,
passive to orient and like and want, basically. but basically you have what's called the circumplex model of affect, which basically is the core energizing structure of your motivational, emotional, and its broadest frame is two poles. one is active, passive, okay? it's like sort of spend energy or conserve
energy. and the other is pleasure. [13934](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13934.05s)

that is either something that you want or something you like, or pain. that's something that you don't want or don't like at its basic. so that's the, so the valence is what we sort
of focused on in relationship to just the ground of it. but there are definitely at least these two
poles of active, passive and pleasure pain at a
minimum. can you make an analogy with this computer
screen right now? so the computer screen is somehow the workspace [13958](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13958.279s)

and then the pixels and the fact that they're
bound together is adjectival or the intensity is adverbial or the other way around. like, can you just spell out an analogy? absolutely, right. so the screening, the framing of the screen, which bren basically says, okay, this is the frame and the relevance and the here-ness now-ness of what is gonna
be brought. that is adverbial. [13980](https://www.youtube.com/watch?v=g7WtcTATa2U&t=13980.189s)

that's what john called adverbial consciousness. and he has a whole argument as to why, especially through what's called the pure
consciousness event that's achieved in meditation and several other things, there's a differentiation but what he calls the indexing function of
consciousness, which basically is the framing. you bring, you index, you say that thing without specifying what the thing is. okay, it's the that thing that brings it. [14003](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14003.89s)

and then you then discriminate on the properties. that's the different pixel shapes that give rise to a form, that give rise to an experience quality, and that's the adjectival
quality. and these are, both of these are john's terms but i've incorporated them in my work when
i use them. okay, another analogy now to abandon the screen. it'd be like if looking is one aspect and then what you're looking at is another, what you're looking at is akin to the qualia [14032](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14032.37s)

in a pure consciousness event. the at may not be there, but you're looking. exactly. it's the framing. exactly, index framing. that's why john, when he takes off his glasses,
okay, the glasses are much more like the adverbial
framing. they pull and organize, okay, and it's the
looking, okay, the pointing, the indexing. in fact, he actually, he uses work in cognitive
science, [14055](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14055.021s)

okay, where you can track, like if i give
you like four different things to track on a screen,
okay, and they're changing colors and changing shapes, four different things you can track. five, six, seven, you stop losing ability
to track. however, what you lose first is the ability to track the specifics. you can tell where something is, but you can't tell what it is actually. so in other words, it's sort of like you're
trying [14083](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14083.06s)

to track everything, but it changes like from
red to blue to green, you're much better. like i think it's over there. it indexes, but i can't tell you whether it's
an a, a b, a red or a green, i can't tell you the specificity. so in other words, i'm tracking the entity,
okay, that's the index, and that's different than specifying the nature of the form. and indeed we have lots of different systems that track the, like what is the thing [14106](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14106.06s)

versus how is it moving? the how is it moving is more of an index structure. but if we think of this kind of bohmian wholeness, we could say that the [[metacrisis]] is a function of missing bohmian wholeness and doing optimization on parts. and so i can optimize self at the expense
of other, but of course that then leads to others figuring
out how to do that and needing to for protection. and now [[arms race|arms races]] of everybody doing that. the whole externality said, i can optimize self at expense of other, i can optimize in group at the expense of
out group. i can optimize one metric at the expense of
other metrics. i can optimize one species at the expense
of other species. i can optimize my current at the expense of
our future, all the way down to one part of self relative to the other parts of self. so the wholeness of all the parts of self
in synergy and all of the people, species, et cetera, [14166](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14166.27s)

and how to consider the whole, how to consider the effects on the whole, maybe that was something that other animals did not have to do. maybe it was something that even earlier humans didn't have to do because they couldn't affect
the whole all that much. when we have the ability to affect the whole
this much, this quickly because of tech, right? because, and particularly because of exponentially [14188](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14188.47s)

powerful tech, whatever ways we are either consciously saying this is a part of the whole, i don't care about, or i'm happy to destroy
[[conflict theory]], or this is a part of the whole, i'm just not
even factoring. maybe i don't even know the factor that's in the unknown, unknown set, but i'm still gonna affect it by the thing
i do. so what is outside of my care or my consideration,
right? [[conflict theory]] and [[mistake theory]], with [[exponential tech]] gets harmed, produces its own counter responses and cascade
effects. the net effect of that is termination. with this much power, what does it take to steward the power adequately is to think about the total cascading effects
of the choices and all agents doing that and say, how do we coordinate all agents doing
that in a way that has the integrity of the whole
upregulated rather than downregulated. and so i would say, [14248](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14248.27s)

bohmian wholeness is a good framework for
alignment, not alignment of an ai with human intent, but aligned with the interconnected complexity
of reality. may i inquire, how did you attain such a vast array of knowledge? what's your educational background? what does your routine look like for studying? is it a daily one where you read a certain
type of book and you vary the field week by week? what is the regimen? [14276](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14276.35s)

how did you get the way that you are? i think my learning process, probably in some ways similar to yours, you said very fascinated and curious. and i mean, you did it. you did something better than i did, which is you pick the topics you're most interested in and found the top experts in the world and got them to basically tutor you for free. in terms of like an aristocratic tutoring
system, [14301](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14301.63s)

you did a pretty awesome thing there. there were a few cases where i was fortunate
enough to be able to do that. other times i just had to work with the output
of their work. but i think for me, it was a combo of just innate curiosity, independent of any use application. and just, i think it's natural when you love something to want to understand
it more. and so for me, the impulse to understand the
world [14329](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14329.35s)

is kind of a sacred impulse. but then also the desire to serve the world requires understanding it well enough to know how the fuck to maybe do that. so there is both a very practical and very not practical impulse on learning that happened to fortunately converge. and how is it that you're able to articulate
the views that you have? how do you develop them? [14352](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14352.17s)

do you start writing? do you do it in conversation with people? do you say some sentiment you realized, you know what, that was actually pretty great. i didn't even realize i thought that until
i had said it. now let me write it down so i can remember
it. you know, i have hypotheses about how people
develop the ability to communicate well, but my hypotheses about that in my own process are probably different. [14374](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14374.67s)

i think my own process is i was homeschooled and i was homeschooled in a way that's maybe
a little bit like what people call unschooling now, but i had no curriculum at all. but my parents just had the kind of, they had never studied educational theory. they hadn't studied constructivism and thought that montessori and dewey's thoughts on constructivism were right. they just kind of had a sense that if kids
enabled [14399](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14399.779s)

and if kids innate interest is facilitated, there's a kind of inborn interest function that will guide them to be who they're supposed
to be. so there were some downsides to that, which is because i had no curriculum, i didn't have like writing a letter a bunch
of times to get fine motor skills down. so i have illegible handwriting. i know what the shapes look like, but i have illegible handwriting. [14430](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14430.25s)

i spelled phonetically till i became an adult and spell checker taught me how to spell. interesting. and so like i missed some significant things, but i also got a lot earlier, deeper exposure to the things i was really interested in, which were philosophies, spiritual studies across lots of areas, activism across all
the areas and sciences and poetry. but my education was largely talking with
my parents [14462](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14462.42s)

and some of their friends and it was largely
talking. right, i actually didn't, it wasn't till later that i did a lot of reading and writing. so i think it just was very conversation, it was very native more than in a lot of people's developmental environment. i think that's the answer for me. i could say that for other people i have seen when they start writing and trying to say what is the most concise and precise way of
writing this, [14491](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14491.69s)

that really helps. also, when they start communicating with people and getting feedback on their verbal communication, when they watch other communicators that they're really inspired by and watch
the patterns. but i think it was just, that was pretty native
for me. all right, that was quite a slew of information and it's advantageous to go through and let's go over a summary as to what's been
discussed so far. you'll get a final word from daniel in a few
minutes. for now, you've watched three hours plus of
this. let's get our bearings. we've talked about how the emergence of ai poses a unique risk that can't be regulated by a national agency like the fda for ai, but instead they require some global regulation. again, this is all argued by daniel. these aren't my positions, i'm just summarizing what's occurred so far. [14539](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14539.83s)

the risks associated with ai are not those that are comparable to a single chemical, as ais are dynamic agents. they respond differently and unpredictably
to stimuli. we've also talked about the [[multipolar trap]], which is regarding self-policing and a collective theory of justice, such as singapore's drug policy that was outlined, and how this line of thinking can be applied to prevent global catastrophic events [14561](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14561.36s)

caused by [[coordination failure|coordination failures]] of self-interested
agents. you can go back to that bit on nash equilibrium to understand a bit about that, as well as the [[multipolar trap]] section timestamps in the description. we also referenced a false flag alien invasion and can that unify humanity? a theme throughout has also been how ai has the potential to revolutionize all fields, but it also poses risks such as empowering
bad actors [14582](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14582.79s)

and the development of unaligned general [[artificial intelligence]]. okay, so this happened about one week ago
or so. i debated whether or not i should just record an extra piece now, or if i should wait till some next video, but given the pace of
this and how much content has already been in this single video, i thought, hey, i'll
just record it and give you all some more content. maybe some people aren't aware of this [14606](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14606.09s)

and i think they should be. the godfather of ai leaves google. this is geoffrey hinton. ai could manipulate or possibly figure out a way to kill humans? how could it kill humans? if it gets to be much smarter than us, it'll be very good at manipulation because it will have learned that from us. and very few examples of a more intelligent
thing [14625](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14625.43s)

being controlled by a less intelligent thing. and it knows how to program, so it'll figure
out ways of getting around restrictions we put on it. it'll figure out ways of manipulating people to do what it wants. it's not clear to me that we can solve this
problem. geoffrey hinton is someone who resigned from
google approximately one week ago because he believed that ai bots were quite scary. right now, they're not more intelligent than
us, [14648](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14648.6s)

as far as he can tell, but he thinks they
soon may be. he also said here in some of these quotes
that i have that it's hard to see how you can prevent
bad actors from using large language models or the upcoming [[artificial intelligence]] models for bad things, dr. hinton said. after the san francisco startup openai released a new version of chatgpt in march, as companies improve their [[artificial intelligence]]
systems, hinton believes that they become increasingly
dangerous. [14672](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14672.99s)

look how it was five years ago and how it
is now, he said of ai technology. take the difference and propagate it forward. that's scary. his immediate concern is that the internet
will be flooded with false videos and text and the average
person will not be able to know what's true any longer. now he says, and i quote, the idea that this stuff could actually get
smarter than people, a few people believe that, he
said, [14697](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14697.29s)

but most people thought it was way off and i thought it was way off. in fact, i thought it was 30 to 50 years or even longer away. obviously, i no longer think that. also, there's this ted talk that's recently
been published as well, just a few days ago. it seems like less than two weeks ago. yejin choi, who's a computer scientist, said
this. extreme scale ai models are so expensive to
train [14718](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14718.94s)

and only few tech companies can afford to
do so. so we already see the concentration of power, but what's worse for ai safety, we're now at the mercy of those few tech companies because researchers in the larger community do not have the means to truly inspect and dissect these models. then chris anderson comes on and asks about, hey, look, if what we need is some huge change, why are you advocating for it? [14753](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14753.39s)

because there's a huge change, a large change, it's not like a foot at a time, every time these ais are released. this is what her response is. acceleration, are you sure that given the
pace at which those things are going? chris even says that it feels like wisdom
and knowledge. there's a quality of learning that is still
not quite there. we don't yet know whether we can fully get
there or not just by scaling things up. [14776](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14776.83s)

and then even if we could, do we like this
idea of having very, very extreme scale ai models that only a few can create and own? and lastly, there's this video by sabine hossenfelder that was released just a few days ago. many people are concerned about the sudden
rise of ais and it's not just fear-mongering. no one knows just how close we are to human-like [[artificial intelligence]]. current concerns have focused on privacy and
biases [14803](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14803.39s)

and that's fair enough. but what i'm more worried about is the impact
on society, mental wellbeing, politics and economics. a just released report from goldman sachs says that the currently existing ai systems can replace 300 million jobs worldwide and about one in four work tasks in the us
and europe. according to goldman sachs, the biggest impacts will be felt in developed
economies. our currently unaligned general intelligence
is an issue. [14833](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14833.92s)

adding artificial in there is like another
can of worms, man. the alignment problem isn't just about aligning human intention with a collective
wellbeing, but also about aligning the different paths
of ourselves to work synergistically toward a common goal. this requires a cultural alignment enlightenment. i think he used the word though, i'm not entirely
sure. we also talked about meme complexes that survive past their hosts and how this is intimately tied up with the
notion of the good. [14857](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14857.989s)

and just so you know, my feelings are that memes are an emphatically mechanical way of looking at a complex phenomenon, such as a society and an extremely complex
phenomenon, such as a religion of a society across time and across other societies interacting. i don't believe my point was adequately conveyed and if you're interested in hearing more, then let me know in the comments and i'll consider expanding on my thoughts [14877](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14877.97s)

in a future podcast. we also talked about naive techno-optimism and how it often overlooks externalized costs
of progress. a responsible techno-optimism requires thinking
about how to get more upsides with less downsides, which can't be achieved. goodhart's law then applies to any metric that's incentivized. it leads to perverse forms of fulfilling said
metric. all right, as you can see, [14900](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14900.109s)

so much energy went into this episode, so much thought, so much editing, so much script revision, so much interaction with the interviewee and double checking if this was accurately
representing what was said. we plan on continuing that for season three. more work went into this episode than any, any of the other episodes of the whole history of theories of everything. [14920](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14920.34s)

if you'd like to support this podcast and continue to see more, then go to patreon.com slash kurtjaimungal. the link is on the screen right now as well as in the description. there's also theoriesofeverything.org if you're uncomfortable giving to patreon. there's also a direct paypal link if that's what you're interested in. you should also know that as of right now, [14937](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14937.699s)

there's launched merch. we've just launched the next merch. this is the second time that merch has ever been on the toe channel. the first one is completely gone. you can't find any of those any longer, but now you can see it on screen. these are references to different toe episodes like just get wet and dystocia. thumbs up if you recognize that [14957](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14957.4s)

and you have toe and it's babbling all the
way down. that's from carl friston, by the way. don't thrust your toe, trust me. don't trust your toe. hey, don't talk to me or i'll bring up hegel. many of these are references, like i mentioned. i agree. i agree with how you're agreeing with me. this is what vervaeke said to iain mcgilchrist. you have to be a significant fan to understand
this reference. [14974](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14974.63s)

and then also there's vervaeke, who's known
for saying there's the being mode and then there's the having mode. got abhijgnosis. i say "phase space" inorganically in everyday
conversation. i have a toe fetish. i'm just a gym rat for toes. that's me. that's what i say frequently. there's also a purse and a toe hat. some toe socks. [14993](https://www.youtube.com/watch?v=g7WtcTATa2U&t=14993.029s)

i think that was one of the most popular of
the first round. so the toe socks are making a comeback. if you want to support the channel and flaunt
whatever it is that you feel like you're flaunting, then feel free and visit the merch link in
the description or you can visit tinyurl.com slash toe merch, t-o-e, merch, m-e-r-c-h. just so you know, everything, every single
thing that you're seeing, this editing, these effects, speaking with the interviewee, all
of this is done out of pocket. i pay for the subscription fees. i pay for zoom. i pay for adobe. [15025](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15025.159s)

i pay for the editor. i pay personally for travel costs. if there are any, i pay for so much. there's so much that goes into this. sponsors help, but also your support helps
a tremendous, tremendous amount. i wouldn't be able to do this without you. so thank you so much. thank you for watching for this long. holy moly. again, if you want to support, then you can
get some merch if you like. [15043](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15043.87s)

and if you want to give directly on a monthly
basis to see episodes like this with such hopefully quality, hopefully something that's
educating, that's elucidating to you, that's illuminating to you, then visit patreon.com
slash kurtjaimungal. or like i mentioned, there's a paypal link
in the description. there's also a crypto link in the description. now i'm also interested in hearing what the
other side, the other side, the people who are pro ai, unfettered ai, who say, hey, there's
nothing to see here. you are all being hyperbolically hysterical. i'd like to see someone respond to what daniel
has said about ai, but also civilizational risks in general and how ai exacerbates those. [15080](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15080.14s)

so if you think of any guests who would serve
as great counterpoints, especially those who are researchers in machine learning, then
please suggest them in the comment section. if you're a professor and you're watching
and you'd like to have a friendly theolocution, that means a harmonious incongruity, a good
natured debate where the goal isn't to debate, but to understand one another's point of view. if you're watching this and you think, hey,
i would like to come on to the theories of everything channel as a professor, along with
my other professor friend who believes something that's antithetical to what i believe about
ai risk, then please message me. you can find my email address, i'm sure you
can also leave a comment. yeah. and who knows when the next episode of toe
is coming out, by the way, the next one is going to be john greenwald should be in about
a week or a week and a half. all right. let's get back to this with daniel schmadenberger. well, this is a great place to end. daniel, you're now speaking directly to the
people. well, you have been this whole time, but even
more so now to the people who have been watching and listening. what's something you want to leave them with? what should they do? [15139](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15139.7s)

they're here. they've heard all these issues. they hear bohmian. they're like, okay, that sounds cool. that's motivating. it's a bit abstract, but it is motivating. okay. what should i do, daniel? i want the earth to be here in decades from
now, centuries from now. what should i do? [15155](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15155.27s)

so i'm going to answer this in a way that
i think factors who your audience probably is. i don't know. we even shared demographics with me, but based
on the attractor, i can guess. if i was answering just to a series of technologists
or investors or bureaucrats, i might say something different and realizing that amongst that
audience, there are people who are going to have radically different skills and capacities
and parts of it that they feel the most motivated and oriented to. so i'm obviously not going to say one thing
everybody should do. okay. what i'll say is whether it's hearing a conversation like this
where the [[planetary boundary|planetary boundaries]] and really thinking about how that there's more
biomass of animals and [[factory farm|factory farms]] than there are in the wild left of the total amount
of [[species extinction]] or what the risks associated with rapid development of decentralizing synthetic
biology and ai are. you hear these things, you're like, fuck,
and it connects you to what is most important beyond your own narrow life or even the politics
that is coming into your stream. or whether it's when you have a deep meditation
or a medicine journey or whatever it is and connect to what is most meaningful. design your life in a way where that experience
happens regularly. [15253](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15253.81s)

so what you are paying attention to and optimizing
for on a daily basis is connected to the deepest values you have. because on a daily basis, the people around
you and your job and your newsfeed are probably sharing other things. so try to configure it that the deepest true
good and beautiful that you're aware of is continuously in your awareness. so your daily choices of how you spend your
time and money is continuously at least informed by that. that's the first thing i would say. i would say a couple of other things. [15291](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15291.71s)

aligned with that is look at things that are
happening in the world online to have a sense of things that you can't see in front of you. but then also get offline and connect with
both the trees in front of you and how without any modeling or [[value system]] just how innately
beautiful they are. and also the mirror on experience when you're
with a homeless person. so both have a sense of what's happening at
scale but then also ground and embodied sense your own care for the real world that is not
just on a computer. there's a real world here. and then realize like deepen shit actually
matters. like independent of whether i can formalize
a particular meaning or purpose of the universe [15341](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15341.86s)

argument or formalize a response to solipsistic
arguments or nihilistic arguments like prima facia reality is meaningful and i actually
do care. i wouldn't get sad or upset or inspired or
moved if i didn't care about anything. i actually do care. and so life matters and i make choices and
i can make choices that affect the world. so my own choices matter. so what choices am i making every moment and
what is the basis that i want to guide them by right to just deepen the sense of the meaningfulness
of life in your own choice and the seriousness with which you take how you design your life. particularly factoring the timeliness and
eminence of the issues that we face currently. [15392](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15392.92s)

and then the last thing i would say is as
you could like really work to get more educated about the issues that you care about and are
concerned about, really work to get more educated about them, get more connected to the people
working on them and really study the views that are counter to the views that naturally
appeal to you. so you bias correct so that your own well-motivated
biases don't mess up your action. and in doing that, don't let yourself become
unagentic. don't let yourself become so overwhelmed. don't let yourself fall into easy certainties. but also don't let yourself be overwhelmed
by the total uncertainty that you can't act realizing that if you don't act, there are
ethical consequences to that too because we're [15450](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15450.13s)

on a moving train. thank you, daniel. i appreciate you spending almost four hours
now with me. likewise. we covered a bunch of areas that i did not
expect, but they're all good areas. i'm curious how the thing ends up getting
edited and what makes it through. and i'm also curious with your particularly
philosophically interested and insightful audience what questions and thoughts emerge
in this and maybe we'll get to address some of them someday. yeah, there's definitely going to be a part
two. [15487](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15487.71s)

cool. a much more philosophical part two if this
one wasn't already. the podcast is now concluded. thank you for watching. if you haven't subscribed or clicked on that
like button, now would be a great time to do so as each subscribe and like helps youtube
push this content to more people. you should also know that there's a remarkably
active discord and subreddit for theories of everything where people explicate toes,
disagree respectfully about theories and build as a community our own toes. links to both are in the description. [15516](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15516.88s)

also, i recently found out that external links
count plenty toward the algorithm, which means that when you share on twitter, on facebook,
on reddit, et cetera, it shows youtube that people are talking about this outside of youtube,
which in turn greatly aids the distribution on youtube as well. if you'd like to support more conversations
like this, then do consider visiting theories of everything.org. again, it's support from the sponsors and
you that allow me to work on toe full time. you get early access to ad-free audio episodes
there as well. every dollar helps far more than you may think. either way, your viewership is generosity
enough. [15551](https://www.youtube.com/watch?v=g7WtcTATa2U&t=15551.59s)

thank you.