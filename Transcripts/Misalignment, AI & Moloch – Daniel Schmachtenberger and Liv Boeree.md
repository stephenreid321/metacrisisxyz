---
tags: transcript
aliases:
youtube_id: KCSsKV5F4xc
published_at: '2023-03-30'
---

<div class="yt-container"><iframe src="https://www.youtube.com/embed/KCSsKV5F4xc"></iframe></div>

hey guys so this is a bit of an unusual one compared to my usual format because this is a long form conversation that was actually meant to be part of my upcoming win-win podcast that's going to be launching in a few weeks but the topic of it is so urgent that daniel who i'm talking to and i decided to release this early you will have noticed how mad the rate of ai progress is getting and i say mad in like every sense of the word [26](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=26.22s)

it's incredibly exciting and there's so much cool stuff coming out like just i can't even keep up with it but at the same time it's it's also a little like overwhelming to the extent that you know we are creating technologies that we don't even understand and unleashing them and connecting them to the internet and it feels like the potential risks [46](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=46.16s)

and harms of that are not being properly internalized into the sort of general calculus of all the different people doing this so yeah in this conversation i'm talking to daniel schmaxenberger who is frankly just one of the smartest people i've ever met i if you've been following my more recent content around this topic of mollock and [[game theory]] and when is competition healthy and [69](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=69.78s)

unhealthy his thinking has inspired a lot of this content so it's a real pleasure to be finally talking to him and specifically in this conversation we get into i think a sort of blend of topics that haven't really been discussed in this way before we talk about the nature of [[game theory]] and mollic and how it interplays with our wider sort of capitalistic [[economic system]] and how that also then interplays in the development of ai so if any of those topics interest you and they should because if you live on this planet this will affect you for good or for worse then you should take the time to watch this conversation in full um it's one of the most fascinating chats i've ever had it's also one of the most terrifying so let me know what you [112](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=112.86s)

think yeah so i am really happy for us to be having this conversation today you and i have talked about moloch and the relationship of the uh kind of moloch metaphor to the overall state of the world and what i sometimes call the [[metacrisis]] for a couple years now and you've put out these [135](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=135.599s)

exceptionally good educational videos on moloch in uh expressing itself in different environments in [[social media]] and general media uh i hope everybody has watched those um we're in the moment right now where there is this uh rapid race on the development of [[artificial intelligence]] technologies development and public [158](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=158.459s)

deployment of them uh this we're recording this shortly after gpt4 has been publicly released and uh then after so many of the other companies that have ai capability have also had to release their large language models in response and so what i'm actually really wanting to talk about is ai risk and a way of thinking about the totality of the ai risk landscape that is [183](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=183.12s)

um for me uh clarifying and a little different than the way iris is usually talked about and maybe unifying across different categories of risk and they give us some insight into how to think about what protecting against it might require and the moloch frame i find [204](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=204.84s)

gives incredibly valuable insight in thinking about the ai risk frame and the agi uh misalignment issue is very helpful in thinking about the moloch issue i think the the two metaphors clarify each other and then i think the actual moloch type dynamics give rise to the ai risk scenarios that i am most concerned about and so that's what i'm wanting to talk [229](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=229.459s)

about which is why i was particularly interested in you and i having this conversation since you're holding the mantle of helping the world understand the moloch dynamics and uh so with that i would love if you would share what the moloch thing is about for people who don't already know i know most of your listeners will already know but uh why are we referring to it that [251](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=251.48s)

way what is the phenomena why is it interesting so so probably the most concise definition i can give is that it's the god of negative some games like unhealthy competitive situations so by that i mean like a system of bad incentives that incentivize agents within that system you know players within that game to sacrifice more and more of their [277](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=277.8s)

other values in order to win the narrow you know win within that narrow domain in other words win the game and by doing this sort of sacrifice of of all these other values they're essentially taking selfish actions that externalize harms to everybody else um both within that game and also you know even people outside of it you know [301](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=301.38s)

to the wider system as a whole and hence making the game a negative something okay so sometimes when uh trying to describe the generality of the instances aren't already clear it's it's hard for people so could you give a couple examples of what that looks like and why thinking about it as uh a god is at all an interesting frame [327](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=327.24s)

um like why is that even the frame that's rising yeah so an example i gave in my first molok video uh the beauty wars is about these beauty filters that have now become completely commonplace on instagram and in fact on most [[social media]] platforms and the reason why these things are sort of so particularly monarchy is that [353](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=353.1s)

every everyone who's trying to sort of play the the beauty influencer game or any kind of influencer game frankly on these platforms you get directly rewarded with more likes and follows if your pictures look better if your face looks better you know your complexion's clearer and these filters started appearing where not only would they make your facebook smoother but they were [372](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=372.78s)

just like tweak your features in really really subtle ways um you know just like make your eyes a little bit bigger and and and um make your face basically converge upon this whatever these like what seem like normalized beauty standards but do successfully seem to hijack people's brains people do like these and like just you know to show you how like how [391](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=391.86s)

full these things are like i would upload a picture of myself that i loved and then i would apply the filter to it and then i would compare the two side by side and i would now no longer like the original picture and therefore it means you know it makes you hate your natural face and yet despite knowing this because you know you know you'd get the [411](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=411.479s)

direct reward uh of of getting more likes and follows by using these things and then on top of that you know that everyone else is using them as well and so if you don't use them then you're essentially going to get like left behind the curve you're no longer going to be competitive in in the influencing game it's a really classic example of monarchy bad incentives and driving a [432](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=432.0s)

kind of like [[race to the bottom]] where everyone ends up miserable no one wants to be using these things but feels like they have no choice if they want to stay competitive another example would be [[climate change]] pollution you know pollution from countries that are trying to grow their gdp you know essentially not get left behind their competitors get left behind other countries and [453](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=453.72s)

externalizing the the costs of their gdp growth to the atmosphere by polluting with co2 so essentially a tragedy of the commons type situation and then a third example is the the classic [[arms race]] a country notices the competitor is developing some new type of hypersonic missile um or autonomous weapons and so on and even if they don't want to ex spend a [483](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=483.9s)

bunch of their gdp on these very expensive new types of weapons they feel like they have no choice because if they don't then they're going to be vulnerable to their enemies great so first it's worth noting that each of these are already abstractions across a lot of cases right when we describe an [[arms race]] whether it's these two countries or [503](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=503.099s)

these two countries and whether it's on hypersonic missiles or ai weapons or [[bioweapon|bioweapons]] those are each different instances so the generalization across the class of if anyone is developing better weapons technologies everybody else has to develop correspondingly the counters to those weapons and the same type of weapons or they kind of lose by default because [524](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=524.399s)

it's a situation where anyone does something that increases their own security in a certain way that also inexorably decreases the security of others unless they do some counter response um so there's lots of different examples of an [[arms race]] but [[arms race|arms races]] as a whole is already a big generalization tragedy of the commons is the same right [542](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=542.88s)

because we can be looking at the situation where we're talking about overfishing or wailing or deforestation or desertification or co2 and these are all cases where uh the overall commons is being degraded by every actor pursuing their own near-term incentives right the actual incentives laid out in the economic landscape but to recognize that when we [566](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=566.64s)

look at every environmental issue facing the world no one is trying to extinct all the species nobody is trying to desertify the planet nobody wants [[climate change]] or the venusification of the planet and yet the entire world is making it happen right and that's so when we look at features of the world [587](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=587.24s)

that nobody wants and that are bad for everyone why can't we change them this is where the moloch frame comes in right and we can see that in both the [[arms race]] everyone's like look i don't want to necessarily live in the world with the autonomous weapons or the [[bioweapon|bioweapons]] but we have to because they're going to and if we all make the agreement that we're not going to how do [608](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=608.399s)

we know they're keeping the agreement and they're not lying and defecting in some underground military base so we have to assume under partial information that they are doing the thing because the risk deaths would be too high if we assume the other way so under partial information we have to assume that worst case you know do the same thing they're assuming the same thing so because of [625](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=625.14s)

the inability for trust and coordination we get this kind of [[race to the bottom]] and uh the same is true in all these various scenarios so we see a lot of features of the world that it seems like are comprehensively bad for everyone trending in a much worse direction nobody can really do anything about and nobody wants and so these properties are kind of the [646](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=646.98s)

[[emergent property|emergent properties]] of bad coordination and so you have in other places describe moloch as the god of [[coordination failure|coordination failures]] or basically the principle of [[coordination failure|coordination failures]] the reason to talk about it as a you know a god or something is to say like okay well since no agent is trying to make it this way what is making it this way is there some [670](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=670.32s)

kind of emergent agency or some underlying system dynamics we can think of it as underlying system dynamics and uh you know i think you and i and many people in ours here both came across this frame from scott alexander's meditations on moloch paper that references uh both this great poem on moloch and uh you know a number of pieces in popular [694](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=694.92s)

culture um and if people haven't read it everybody should read scott alexander's paper on meditations on moloch um because of what it's trying to get to is if every environmental issue from [[dead zone|dead zones]] in the ocean to uh plastics and wasting plastic into all of these issues nobody wants but also [717](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=717.839s)

nobody can stop because the cost of someone stopping it disadvantages them relative to everyone else if everyone else is going to continue to externalize that cost to the commons rather than internalize it and decrease their profit margins and so how do we deal with that thing and if all of the things that are moving us towards increased likelihood for global [739](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=739.68s)

[[catastrophic risk]] or at least many of them have this in common this is an underlying feature that we have to really understand right and um so you could call it the god of [[coordination failure|coordination failures]] of the unhealthy kind of game dynamics not the ones that upregulate every because yes an [[arms race]] upregulates everyone's capacity in a certain way but it is also of regulating [758](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=758.579s)

a capacity that everyone wishes we didn't have that is only relevant because everybody else has it right if we could all just agree to decrease military spending by a factor of 10 and reinvest all of that in healthcare and um infrastructure and everything the world would be better by everyone's standards so we're not saying that there are no types of competition that lead to [778](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=778.62s)

positive some dynamics but there are these other ones so um so it's very interesting like moloch can be seen as uh a kind of way of looking at generative dynamics that lead to the overall state of global [[catastrophic risk]] right that there are other places where i've talked about the [801](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=801.42s)

[[metacrisis]] and tried to give a formalization of it we can link that here so i won't do it at length but i'll just very briefly say the [[metacrisis]] thesis is that we are at a unique time in history where there are are an increasing number of global [[catastrophic risk|catastrophic risks]] with increasing probabilities and that has never been the case like [823](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=823.2s)

this before where the attractor state of increasing catastrophe is the most likely attractor state of the future across many different dimensions of how that could play out and the other attractor state and maybe i'll i'll come back to that i'll explain this one a little bit first it is um a [[catastrophic risk]] is not new civilizations have faced war and have faced famine and have faced uh plagues and have faced self-induced environmental ruin easter island in many cases previously they were just local they weren't global and that was because the overall civilizations were local we didn't have fully globalized [[supply chain|supply chains]] where everything depended upon six continental [865](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=865.8s)

you know radical interdependent type things and when we could destroy a local environment we couldn't destroy the biosphere writ large or oceans or something so obviously it's our level of technological capacity that allows us to have a global civilization that allows what happened to all previous civilizations which was civilizations did go through growth [887](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=887.16s)

curves where they had peaks and then they failed and they kind of all failed right at least that's a overarching architecture we see in collapse of complex societies by taintor and other books like that kind of describe some of the dynamics but we are for the first time facing that in a global way so it is not unprecedented the thing [908](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=908.699s)

about [[civilizational collapse]] it's just unprecedented to think about it globally but obviously the egyptian the mayan the roman the all the previous empires failed for various reasons uh we didn't actually have world ending tech we didn't have the capacity to ruin everything rapidly until world war ii in the bomb the bomb allowed something where a rapid escalation could destroy [931](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=931.019s)

kind of everything that was novel there were hundreds of 200 000 years of homo sapien history before that we couldn't destroy everything quickly and then we could so that was a bright line in this hand and that was very recent and we couldn't ruin the entire planetary we couldn't reach [[planetary boundary|planetary boundaries]] and mess up the biosphere until industrial tech but the industrial tech doesn't get there rapidly like the nukes it takes a few hundred years of its proliferation for cumulative effects right but we went from half a billion people be for the [[industrial revolution]] to 8 billion people we increase the resource consumption per capita moving into the [969](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=969.839s)

industrial world by 100x plus and that's utilizing resources from the earth faster than they can be replaced turning them into trash from pollution faster than they can be processed running the environment on both sides with an exponential economic growth curve that just to keep up with compounding interest has to become exponential and to not over inflate that currency has to [989](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=989.399s)

equate to more goods and services on a linear [[materials economy]] you don't get to do that thing forever so industrial tech bound a linear [[materials economy]] turning the earth into trash and pollution through a commodity cycle faster than it can be replenished attached to an [[exponential curve]] of finance um utilizing growing industrial tech and [1012](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1012.079s)

globalization is what creates all the [[planetary boundary|planetary boundaries]] we're facing of which [[climate change]] is one but [[species extinction]] and [[biodiversity loss]] and um on and on and on the entire [[planetary boundary|planetary boundaries]] framework as a result of that that is the result of tech without the industrial tech we couldn't have done that right so cavemen can't mess up the entire planet right stone age tools even [1034](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1034.579s)

bronze age tools can't do that nor can they have a war that kills everything so then nor can they spread memes they're not informationally connected either um although their means spread much much slower and more locally right um and so that is also the result of the tech right you and i are talking via satellites right now right via [1059](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1059.0s)

literal outer space type communication for this thing to be able to happen on computers that were generated in six continent [[supply chain|supply chains]] and um that are more advanced in the things that ran the manhattan project um that are available to all of us and but so so the cumulative effects of industrial tech bring us to [[planetary boundary|planetary boundaries]] and kind of increasing fragility where there used to be a lot of people who lived on local subsistence not dependent on the total grid and there were a lot less total people now there's a lot more total people and they're almost all dependent on the grid not local subsistence or the fragility of those things is radically higher so you both get fragility of the planet and fragility of the human life support [1099](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1099.559s)

systems multiplied by a lot more people which can of course also escalate to violence when things start to break down things like that so uh then you get the bomb is the example the first fully existential tech and for the first time in history we actually had to make an entire [[world system]] to not use our new tech whereas before that every time we had new tech there was always a race to [1118](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1118.94s)

deploy it as fast as we could for a strategic advantage this is a situation where nobody can win everybody loses so [[mutually assured destruction]] and the entire post-world war ii world of the redwoods [[financial system]] the un etc was all how do we make a [[world system]] that doesn't do that thing um right and it happens to be that that [1137](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1137.539s)

was successful which is why we haven't had a kinetic world war iii since then we're closer than ever to that right now um with a proxy war between nato and russia being as close to not proxied as it is and other things in the horizon but um nutrition destruction doesn't work when you have many many players that have [[catastrophe weapon|catastrophe weapons]] and many types of [[catastrophe weapon|catastrophe weapons]] which is the scenario we have now it's like a lot it's like a local it's a local minimum but there's like ton it's a very tiny uh minimum wage just a little nudge and it could fall off down the hill there's many routes for it to topple off down the hill yeah yeah and the the other thing is that a major part of the post-world war ii solution was one of the major reasons for war as you [1186](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1186.02s)

mentioned before was competition over resources and if major nations want to be able to grow their economic quality of life for everybody they want more stuff to not have to invade each other to take their stuff how can everybody get more stuff simultaneously well we can create an exponential monetary system and globalization and free trade and much more industrialization and just take stuff from nature faster so that everybody can have more stuff exponentially right super positive some dynamic except you can't take stuff from nature forever and be able to keep doing that so you start hitting [[planetary boundary|planetary boundaries]] and we're right at that point and then when our own inability to keep growing without taking other people's stuff comes now the conflict type dynamic so apart to the the [[planetary boundary|planetary boundaries]] we're facing are actually partially the nature the result of the solution to not world war iii which is why also the degrothers have to factor that degrowth ends up driving world war iii if you don't have other ways of [1243](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1243.26s)

tending to the fact that many people would not voluntarily choose austerity in the presence of less stuff they would choose war in the other if they thought they could win but then the sort of so then the flip side argument is like okay so yes to an extent technology has gotten us into this mess but it sounds like we should pile on more technology to expand those essentially those [1263](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1263.96s)

[[planetary boundary|planetary boundaries]] um to be able to more efficiently extract resources and thus sort of keep the house of cards going no we can say for sure that luddite solutions don't work even if they would be better because unless because of [[multipolar trap|multipolar traps]] because of moloch because the tech [1284](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1284.0s)

equals power and if somebody says hey this tech is causing harm so we're not going to do it they also just lose in the short term to whoever does right and so okay we think ai weapons are bad so we're not going to build them we think okay great then you're going to be destroyed by whoever does so um so we unless you can get universal agreement you can't just lose an [[arms race]] and this has been one of the challenges of whether it was china engaging with tibet whether it was colonialists engaging with the native americans whether it was genghis khan or any of his guys engaging with more peaceful tribes that were smaller uh the peaceful tribes lose it were and they also lose at population games [1329](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1329.0s)

right like the ones that are going to unrenewably use the planet to grow their population faster so we are in this unique situation where the result of that is an exponentiation right it's the who has made it through or the people that both win at war and when at economic growth and so we have radically more warfare like total [1351](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1351.32s)

warfare potential and that is radically more distributed and radically more externalities on the environment and all the fragilities associated and that situation ends up leading to catastrophic breakdown of everything so basically so far the answer has been when at the race or lose the race itself is [[self-terminating]] this is kind of the [[metacrisis]] hypothesis so we take the next step and not only did the post-world war ii model in in doing the good thing of we didn't have nuclear war yet right we didn't have a kinetic war between superpowers it all it also increased total global fragility increase the movement towards all the [[planetary boundary|planetary boundaries]] and we proliferated a heap of other technologies that are [1394](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1394.22s)

truly catastrophic now that unlike nukes are not easy to control nukes are extremely hard to make uraniums not in many places it's hard to enrich you can see where it is with satellites because it's radioactive and so you can limit it and only have a g9 or whatever that actually has nuclear capabilities and limiting iran and many countries from getting it has been a major part of [1415](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1415.58s)

the world order right um but when we're talking about cyber weapons or grown weapons or [[bioweapon|bioweapons]] or the types of attacks that ai makes possible or other types of [[exponential tech]] these do not require uh something that has to be mined in one particular area in the same way these do [1435](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1435.62s)

not require nation like top level [[nation state]] level capabilities once they're developed for any purpose they're pretty much more easily accessible and what that means is and we always talk about [[exponential tech]] democratizing power right decentralizing and democratizing and decentralizing democratized sounds nice in some ways when you don't like concentrations of [1456](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1456.32s)

power and the abuses they're in but the democratization of [[catastrophe weapon|catastrophe weapons]] is has a downside and one is when we're talking about not just a few [[nation state|nation states]] but lots of [[nation state|nation states]] and non-state actors and people who you can't even tell whom having those capabilities you can't put nutrition destruction or force nash equilibriums in the same way [1477](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1477.799s)

um what it portends for kind of just disgruntled misanthropes of which there are more as the other issues are advancing in technological unemployment increases and people migrating because of [[climate change]] increases and all those types of things um and then some of the exponential texts just even cause the ability to cause [1498](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1498.02s)

pretty catastrophic stuff by accident right whether kovid was from a lab leak or not the idea that if you're doing gain of function research and synthetic synthetic bional lab that it can leak and as you're doing lots more of it that the probability of that increases like that's not even intentional that's and as easy as it is for a lab leak you know of that type it's way easier for ai [1519](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1519.799s)

leak because it's connected to the internet it's actually almost very hard not to have those types of things happen so what we're saying is that we're at a novel point in history that world war ii is a novel point first truly [[catastrophe weapon]] now we're at the point where we have multiple types of cadastory weapons many actors that have them no good force nash equilibrium [[planetary boundary|planetary boundaries]] fragility and that we're not saying that lots of things aren't getting better of course all of the pinker and friends arguments about the things that are getting better are the point is that they're getting better at the cost of other things that are being made worse where externalities are being driven the things that are being made worse are getting very near criticality [1563](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1563.24s)

points and tipping points that change the game fundamentally so you have a world of increased [[catastrophic risk]] and of course you have cascades between these because you can have well before [[climate change]] and whether it's just from co2 or whether it's from the localized effects of deforestation or whatever we do have increasing extreme weather events so then you get human [1584](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1584.6s)

migration and do we see likely possibilities for much larger amounts of human migration in the near future yes and can that lead to resource wars which can lead to escalating wars if they hit already tense geopolitical environments whether it's india pakistan or whether it's you know so many issues like that um so we can see that whether we're talking about large-scale military dynamics or breakdown of [[supply chain|supply chains]] in human systems or what [[exponential tech]] can add they all actually kind of cascade into each other they have the capacity so there is some need to tend to and that what you do to make one of them better can often make another one worse right so people will propose hey we need to tax carbon heavily and properly price [1630](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1630.679s)

carbon where the the price of the tax allows us to sequester the co2 but if everyone doesn't internationally to do it and say the us does or europe does and china doesn't and that equals a radical change to gdp which gets reinvested in military plus overall geopolitical diplomacy then you're also changing the balance of power in the world as a resultant so this is one of [1652](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1652.94s)

these classic cases where the way you make one thing better can make other things worse so how do you kind of factor all that together so this is i i took longer than i wanted but that's roughly the [[metacrisis]] thesis right and so moloch is one way of looking at one of the generative dynamics it gives rise to this a comment that often arises on a lot of my videos i've noticed is that [1674](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1674.779s)

people really you know they're sensing that there is some malevolent force that is you know essentially making the world you know making it hard for people to coordinate making it so that we seem to be trending more towards like greater militarization and a greater war you know risk of war and so on but they can't they they end up ascribing it to like you know like a q anon type theory or something like that you know it's like oh it's a shadowy cabal of elites it's the elites who are driving this and so on it's like there's some truth to that and that like elites have more power and therefore have a little bit more responsibility in driving a molecule process but there's no i wish there was [1715](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1715.1s)

a centralized cabal who were like drawing because then at least then we'd have some some easier ways like okay we've got we know who the enemy is and the enemy is physical and real and like therefore you could take it out but it's it's more distributed than that because it's it's this like nebulous collection of bad incentives that we happen to call marlo because we need to give it a name [1734](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1734.24s)

we need to give it a face so that we can understand it um but yeah it's it's i i think it's an kind of an important point to to to to sort of hammer home to people because they they're looking for an enemy and they're looking for a scapegoat but all the while that they keep blaming it on like constantly just purely blaming it on the elites they're [1753](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1753.559s)

missing that's not going to solve the problem you can kill all the elites and molecules will be there yes so this is why looking at the moloch type dynamics the [[coordination failure|coordination failures]] are very useful for understanding lots of features of the world is that um the in the various environmental issues the various market type races that end [1773](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1773.36s)

up being racist to the bottom or that bring way more risk i'll give you another great example with the race to ai right now or the ones with [[social media]] that happened the there's a [[perverse incentive]] to focus more on the opportunity and less on the risk of any new technology even though any new technology will do both because [1794](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1794.12s)

if i say whoa there might be real risk in this this is very powerful people could use this for various purposes we want to do a real thorough deep risk analysis before releasing this thing and not release it wrongly we want to do some real safe to fail testing and someone else is like we do some box checking risk analysis and then talk about all the awesome upsides in rush [1816](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1816.14s)

ahead they get first mover advantage they get more um investment they get metcalf law and winning the [[network dynamics]] and so there is a [[perverse incentive]] against thoughtful consideration and precautionary principle and so we see that lead got put in gasoline for some really simple thing of engine knocking that knocked a billion [1840](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1840.02s)

points of iq off the planet and 4x the aggressiveness of everybody by literally atomizing lead that we had to pull out of deep oars and brain toxic to find the whole planet and took like 80 years before we finally outlawed the thing and the effects that that had on the entire population of choice they made are irreversible and the same with ddt and parathione and [1860](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1860.059s)

malathion and on and on where we or cigarettes where we don't regulate the thing until way after the harms have been so clear but as we're getting tech that has and and elon and many people have talked about this for a long time as we're when we're dealing with ai when we're dealing with synthetic bio when we're dealing with technology that has rapidly much [1880](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1880.039s)

more rapid and much more scaled and consequential and complex types of effects if you wait until it hits a certain point to try to regulate it's too late now you have radical irreversibility and um so the we saw on we saw chad gpt get to 100 million users in a fraction of the time [1901](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1901.34s)

that it took tick tock or facebook or anyone previously and obviously it has a lot more total power and things that it can do and so uh we this isn't a situation where we want to have an anti-incentive against precautionary principle maximum incentive on race where the regulators are inherently slower and more stumbly than the [1921](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1921.74s)

um ones incentivizing it right like there's that that is also part of the moloch dynamic which starts to bring us to the ai conversation did you want to talk about the relation between molec and capitalism before we get to that yeah um i'll start by saying [1941](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1941.48s)

the competition the you know cold war between the ussr and the usa was not which was being framed as two different [[political economy|political economies]] right communism and capitalism and competition was not capitalism right and the up things that were happening inside of the ussr were not capitalism so we're not talking like the critique that we're [1967](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1967.1s)

about to offer of capitalism is not saying some previous [[economic system]] or [[political economy]] was better and actually morlock instantiated itself through those systems as well capitalism was more effective and it did get selected because it was more effective at both good things and up things right which is kind of what you mentioned it wins a war but it has [1986](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=1986.36s)

to sacrifice important stuff to do so so the thing that that can be reductionist and when certain critical metrics but harm other stuff in the process where eventually the cumulative effects of those harms or either catastrophe or [[dystopia]] world that nobody really wants and those are the two primary attractors right now you have catastrophes and to [2006](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2006.279s)

prevent all the catastrophes to make sure that people can't build [[catastrophe weapon|catastrophe weapons]] in their basement what type of surveillance is needed to make sure that you have enough controls on all the things if you really have the ability to control the entire landscape of things that could lead to [[catastrophic risk|catastrophic risks]] that are radically decentralized most of those solutions look pretty dystopic and so we want a future a [[third attractor]] future that is neither catastrophes nor dystopias and without saying what it is first let's just say that it's not those things and we can almost all universally agree that we would prefer not to those things which means that we need something that has the power to be able to prevent [2045](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2045.1s)

[[catastrophic risk]] but also needs checks and balances on its own power right doesn't have unchecked power and and capturability or corruption or those types of dynamics that are then uncheckable it's beyond the scope of this video to talk about what that [[third attractor]] solution is um but uh so we have you know when we talk about capitalism as a kind of dominant global [[economic system]] and of course we don't have pure laissez-faire capitalism we have this kind of hybrid um [[political economy]] but roughly this thing it does not subsume all of moloch like we said moloch was operating under feudalism and under communism under other systems and in the [2083](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2083.56s)

competitions between them it is more fair to think of it as kind of um the god of [[game theory]] uh but as capitalism being such a powerful part of that stack we can think of it as a metaphor for a moment and say capitalism is well let's just start by the a couple key aspects of the incentive [2107](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2107.02s)

dynamics vast majority of human history and tribal type dynamics pre-agriculture all of our kind of genetic fitness in that environment we didn't have the ability to store a lot of surplus right that happened post to plow and grain and storage technologies and whatever uh in which case there were all these [2129](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2129.0s)

kind of sayings in various tribes the best place to store extra food is in your neighbor's belly because rots otherwise and you know etc and tribe and are invested in so but as soon as we start getting to uh [[private property]] ownership and the ability for a lot of surplus where i could differentially make it through a famine better than somebody else i could [2150](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2150.099s)

have a higher quality of life than somebody else i could pass on inheritance um as soon as i have [[private property]] as a possibility now there is an incentive to try to turn more of nature into my [[private property]] right and to try to turn more of other people's actions in my [[private property]] but when the property is actual real commodities and goods or the agreements that people can do services let's stick with goods because it's easier for right now there's a diminishing return on the value of any of those based on the illiquidity of them or the difficulty of moving them i get more lumber at a certain point i have more lumber than i can use and i [2190](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2190.72s)

can't even move it around to sell all that quickly and so i don't really want all that much more of it right and the same would be true with or of a certain kind or whatever but as soon as we move to a kind of a currency mediated system where i can sell it in real time and turn it all into something that has no intrinsic value but the optionality for every form [2209](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2209.5s)

of value well now there's no fungibility fungibility now and obviously we started with things that had intrinsic value but still got used to mediate it like gold but then you know we got to fiat so i'm just going to do this huge jump to fiat because roots of the current system even though it has no intrinsic value [2229](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2229.119s)

what the value that it has is maximum speed of optionality right maximum optionality and maximum kind of liquidity and speed and so in that situation there's no diminishing return on getting more like more is more right and uh whether i want to convert that money into military power or convert it into public opinion through median campaigns and stuff or convert it [2256](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2256.18s)

into technological power of one kind or another kind or land ownership i the money allows me the ability to do all that so you can think of it as just units of power or units of [[game theory]] right units of ooda loop and um then when you add money on money dynamics to it compounding interest as the beginning then of course all of the financial [2277](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2277.119s)

services that um become possible with more more capital but just compounding interest not only is it not a diminishing response to as i get more and more or it becomes less valuable to me because i can't use it fast enough now as i get more money it is actually exponentially making money on itself so when i have [[private property]] i have the ability to turn all of that into fungible units of capital and it makes money on itself there is now a maximum incentive to turn as much of the world as possible into capital in my holding and because other people are and they could use that against me there is now an [[arms race]] for me to do it faster than that guy right and that's decentralized and um now of course we can see that when we're talking about there are types of power that don't directly just relate to dollars right the number of twitter following is one or the amount of covert political influence or [2339](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2339.76s)

um uh you know many many other things there are military generals that have more total power than the amount of money they have but obviously they influence a huge amount of money in terms of military assets how much they cost and things like that um and so that's why i say i don't want to [2359](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2359.32s)

reduce it exclusively to money but if we had to pick a single metric that has the most kind of optionality for all other types of metrics that would be the one so if if we're thinking about molok as a whole we can see that whether we're talking about the environmental issues or whether we're talking about the increasing polarization because of [[social media]] algorithms or whether we're talking about um you know any of these things the rapid race that is not orienting towards safety enough on new technologies that this set of dynamics is underneath it right and this is why that kind of frame scott alexander and others have put forward which is who is engineering this thing well molok is engineering this [2400](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2400.839s)

thing right like that that thing overall and now this is where i want to stop and go into the what is a misaligned agi what is a [[paperclip maximizer]] for a moment because it actually makes moloch clearer and then moloch makes it clearer do you want to um construct the kind of paper clip maximizer scenario for people [2423](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2423.22s)

um you know a lot of the people in the kind of ai risk space so the [[paperclip maximizer]] is like a [[thought experiment]] that is basically of like an extreme super intelligence gone wrong because just because you can build something that is by definition super intelligent you know in that it's you know and if we define intelligence in what i think is [2444](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2444.76s)

the best definition which is uh the ability to optimize and navigate a very broad range of terrains in order to achieve whatever your goals are so if we can achieve you know define intelligence is that your ability to basically get stuff done across a wide range of environments um that does not necessarily guarantee that you also have the wisdom to decide [2468](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2468.339s)

what your goals should be in the first place um it's called the orthogonality thesis the idea that like you know you know maybe intelligence and wisdom are perfectly aligned but there's actually very large possibility that they are completely unaligned you know they're just orthogonal to one another and so the [[paperclip maximizer]] is like the extreme you know a silly uh example of that into you know an arbitrary example whereby you you know let's say you want to build um a machine you you are you you have a factory that builds paper clips um and then you happen to get hold of a super intelligence um that will help you build them as fast as possible so you make maximum profit [2509](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2509.32s)

um your super intelligence then is able to because it's so capable at navigating a broad range of goals uh turn every atom that it comes across into more paper clips uh until the universe becomes tiled with them um so yeah that's it it's basically a very somewhat oversimplified but at the same time kind of comically salient example [2530](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2530.859s)

of a deeply misaligned uh but nonetheless super intelligent system right so um just to construct a couple key parts of it paperclip is obviously a a silly and kind of cute on purpose example of whatever it is right whatever commodity whatever widget that you're optimizing [2552](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2552.16s)

for and without even saying super intelligence let's just say increasingly good increasingly competent and generalizable [[artificial intelligence]] gets applied to the corporation which is already happening everywhere and it has just two features which is it can work to achieve a goal whatever its objective function is in this case make more paper clips and it [2576](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2576.04s)

can recursively improve itself so it gets better at doing that thing that's key right um so of course at first it does a bunch of stuff that we want that figures out how to turn off lights when people aren't there to save cost and energy and how to make more efficient [[supply chain|supply chains]] and negotiate better deals and all those kinds of things and just makes a more [2596](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2596.079s)

efficient business and of course the reducto at absurdum is once it has done all of the easy good stuff it still has the objective function make more paper clips then it has to start doing stuff that is not just obviously easy good right where there's some trade-offs that are happening somewhere else but the things that are being harmed are not part of [2614](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2614.14s)

its objective function right its objective function isn't makes the most paper clips wow don't you're doing any harm not no harm anywhere else because that the wow don't do harm anywhere else is actually incredibly hard to specify in the easy computational way which is the heart of what we'd call you know the alignment problem and so if you did have something that could recursively [2632](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2632.859s)

increase its ability to achieve a goal like that and then had enough generalized intelligence that it could out-compete anyone that was competing against it right it could increase its capability faster than say we as humans could and we're like oh  we don't want you to be making paper clips out of our food sources we don't want you to be making paper clips out of but it figures out how to beat us at those games then yes eventually it just starts turning everything into the substrate for paper clips and in general the idea is you have an objective function whatever the ai is optimizing for whatever that is the um and there's different ways that [2669](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2669.88s)

people will describe it in nuance and you know it's worth reading utkowski and bostrom and the other you know kind of um seminal thinkers on what the nature of the agi alignment problem is but roughly if you have an [[artificial intelligence]] it is general and autonomous autonomous meaning it is working on its own you [2689](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2689.8s)

don't have to keep giving it prompts right it's doing its own thing it has agency and where you can't pull the plug on it right that's a key part and it can upgrade its own capability to do whatever it is that it's seeking to do and it can upgrade its capability faster than we can because the smarter one then is capable of making even [2711](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2711.46s)

smarter one and we already see early signs of this we already see ai is starting to create better internal ai functions to be able to achieve their um the goals that are set for them the idea of do we want an autonomous general intelligence that is comprehensively smarter than us that is trying to fulfill a goal before we know that it's fulfillment of [2739](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2739.359s)

that goal isn't going to really mess stuff up for us right we're like no no we don't want that thing right we would like to prevent that um because it's entirely possible that it could do some uh things that are totally not what we want in pursuing that goal so obviously if its goal was to maximize gdp there's [2759](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2759.099s)

a lot of nasty ways to maximize gdp it can go up on with war it can go up with addiction it can go up with and whatever it is that the objective function is there's a lot of perverse instantiations of that thing being fulfilled in a way that totally messes up other stuff and so the agi alignment question is can we actually ensure that before the thing is truly a general autonomous intelligence [2781](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2781.359s)

that it is aligned aligned with our interests our intention our good or something what the nature of alignment means is actually a deep question i'm going to put that on hold for a moment but roughly aligned with us such that that much power would be a safe thing for us to have exist and what i the [[thought experiment]] of an intelligence that was say as much more intelligent than us as we are than chimps or ants or whatever looking at how our increase in intelligence has voted for all the other inhabitants of the planet uh uh you know that's a very concerning thought mm-hmm now this is where i want to actually use the analogy of that an autonomous so it's doing stuff on its own right it's it's auto poetic it's self-authoring it's self-upgrading and it's orienting towards an objective function and i would basically like to say that you could call the current global system [2847](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2847.06s)

and we just to simplify it let's call it global capitalism even though it's not that um we calling it molok would be better right um but let's just talk about the capitalism part because the metrics are kind of clear you could say that it is already a general auto poetic super intelligence [2868](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2868.359s)

it is has an objective function which is to convert as much of the world people's creativity ideas labor natural resources everything into capital and so that's the paper clips right and which is interesting because it has no real value just optionality for real value and there's always this assumption that there's more real value out there [2894](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2894.76s)

but that stops being true forever right so if i the tree sequester co2 and produces oxygen and i need to breathe oxygen and it does a lot of other important things like supports pollinators and cleans the water and stabilizes topsoil and all these things for me but if i cut the tree down turn it into lumber there's still enough oxygen for me i didn't actually ruin my if i cut the whole acre of trees down they're still oxygen in fact there's no differentially seeming less oxygen for me doing that but now i have the money of all the the lumber of these trees and i can do real tangible for me and my family or my corporation with that and so the optionality value i get allows me to still access real value but [2935](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2935.8s)

i'm destroying real value in the process so i'm going to say well you're not destroying it because you're making lumber well yes lumber is actually radically less complex than a tree that has less total types of value that it does so we're converting the self-organizing self-repairing complex world and into an increasingly simple or complicated fragile world that has less [2955](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2955.599s)

types of value to less types of actors the tree has value of many different types to many different types of actors right so you can't just say well it's carbon sequestration but no it's it's stabilizing topsoil it's yeah a million things biodiversity etc yeah so when we talk about [[artificial intelligence]] we talk about what type of computer system it runs on [2981](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=2981.16s)

it hardware wise it runs on cpus or gpus or tpus or whatever it is and what type of algorithms that it runs and uh so what's it's interesting that we can already say humans are general intelligence and capitalism is running parallel process across all humans right and we know when you think about the cloud and why parallel process was [3007](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3007.14s)

so powerful it's also igp user so powerful um capitalism is basically as a decentralized incentive system incentivizing all humans should both do novelty search figure out new ways of making money and exploitation take the existing ways and do the most of them that you can those that do better at it [3024](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3024.54s)

get more influence in the system and in turn influence the system in ways that support them to do more of it those that oppose the system are also opposing those who are doing well at the system so even though the system as a whole doesn't have agency those who do keep and check those that would oppose them so it is as if the system has agency and you go from barter to [3043](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3043.98s)

currency to fiat currency to fractional reserve banking to ai high-speed trading of derivatives and credit default swaps and that is basically the recursive upregulation of the algorithm right it is getting more and more capable of doing more and more financialization of the world to incentivize people to do more and more things though so you can see that [3071](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3071.28s)

you've got something that is already running on all these general intelligences and as a result is super intelligent it is has an objective function the objective function is misaligned with the long-term well-being of the world and it advances narrow value metrics at the x and it's not that like you know the the but everything's getting better [3095](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3095.04s)

pinker rossling type arguments or like saying but look at how many paper clips we have aren't we all stoked that we're getting cheaper paper clips and multiply by all the types of paper clips like yes narrow metrics are being advanced at the cost of lots of wide metrics it ended up being critical to either the breakdown of life support so you get catastrophe or the breakdown of the quality of life [3116](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3116.16s)

you get [[dystopia]] how would that then it was sort of using that definition apply to the the difficulty of the alignment problem with an agi so here's the thing we think about the super intelligence and a misaligned superintelligence is a very scary idea and if you want a sense beyond the silly [[paperclip maximizer]] how scary it is um read some of people like eliezer utkowski and others on what agi misalignment means superintelligence and slime it means um before we go all the way to moloch we could already say that collectives of lots of people interacting in particular ways whether we say a public corporation [3163](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3163.859s)

or say a nation-state let's say we take a large public corporation right well beyond what we have evolutionary history for we had evolutionary history for tribal type size things below the [[dunbar number]] where everybody could talk to everybody so possibly the human scale fx and whatever could perpetuate through the whole thing at much larger scale of [3184](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3184.44s)

changes which is why the beginning of large civilizations are to have different properties um so you take a public corporation who's in control of it kind of nobody right like the people in the corporation answer up to the executive team answer to the ceo the ceo answers to the board the board answers to the shareholders the the board has a [[fiduciary responsibility]] to maximize profit returns to shareholders the shareholders are pension funds and whatever where the managers of the shareholders are trying to get money back to the 401ks and the whoever is in there there end up being pieces of law that are bound up through the whole thing which is the the liability limiting status of the corporation that [3227](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3227.28s)

can privatize gains and socialize losses and the [[fiduciary responsibility]] of the uh directors to maximize shareholder profit and the on and on so who's really in charge of it you can get rid of a ceo and put a new one in and get rid of a director put a new one in you can sell some of the shares get new shareholder it's kind of this thing that gets set in motion where it has an objective [3247](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3247.92s)

function which is now maximized profit within the domain of how it figured out how to do that thing and so but because it is engaging it's running on all these human intelligences which are already general intelligence so it can do things that the humans can do plus things that none of those humans on their own could do right it takes a lot of humans together to do a large [3268](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3268.26s)

hadron collider or a hubble or an exon right it has a capacity that nobody could do on its own so it is super intelligent it's beyond human intelligence in that way not just in a narrow way because it's engaging people that are already generally intelligent and then beyond just that it's already engaging computation so it's engaging the narrow but very powerful data [3287](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3287.28s)

processing and now we add ai to that and so we can say already that a nation-state or a public corporation is kind of a cybernetic general intelligence that is already misaligned can you just define what you mean by cybernathic cybernex just the field of study for any type of system [3313](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3313.26s)

that kind of self-regulates right how the control mechanisms work how the regulatory mechanisms work so as a corporation has feedback loops it has feed forward loops it has regulatory processes to be able to maintain what it's doing and upgrade what it's doing right nation safe has that any and but it's it is dealing with internal but also [3335](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3335.4s)

external pressures that force it to be what it is maybe google didn't want to release its large language model yet but as soon as its business model gets attacked by microsoft releasing one and adding it to being in the possibility of search and it has to and so this is where the [[multipolar trap]] the molokian type dynamic comes in is the individual organization is not [3357](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3357.059s)

totally sovereign because it's for it to keep existing it has to deal with the pressures defined by others and so either a sociopath can start something that then everybody has to deal with or everyone assuming the other one is about to do it next and no sociopath has a situation that is functionally sociopathic we talk about a corporate person 14th amendment kind of [3379](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3379.68s)

giving personhood rights to a corporation in this weird way if we were to talk so this there's already a framework for thinking of it as an agent right the super intelligence as an agent but the [[fiduciary responsibility]] to maximize profit makes it kind of an obligate sociopath right that kind of thing um has to take the opportunities it has [3399](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3399.9s)

so long as it is not illegal within the confines of law but it can work to change law which is what all quite all big corporations lobby right um and that ends up being one of the very profitable things that a company does is rather than the regulator limiting it too much at figuring out how to get the regulator to change regulation more aligned with its [3418](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3418.2s)

interests um and so for it to say hey uh a shareholder profit maximization [[fiduciary responsibility]] in an oil oil company and solving [[climate change]] are incommensible a shareholder fiduciary profit maximization and military industrial contractors and a world of [3438](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3438.839s)

peace that would de-necessitate all the demand or in commensurable right and so um then those competing with each other right so then any of the a groups are like hey i can't really do safety because the other ones we all kind of have to race at this and then maybe the whole us says we don't want to regulate it because we would rather our guys get [3459](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3459.359s)

there before china gets there um because whoever has it's going to run the world we'd at least like it to be us companies so that molokian dynamic makes it to where each of these cybernetic superintelligence is interacting with each other creates a meta cybernetic super intelligence that you can call moloch right which is why i wanted to talk with you about it is you [3479](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3479.839s)

can see moloch as an [[emergent property]] of the systems of incentives and the uh dynamics of coordination that are built into the system where it is employing human general intelligence it's employing computational capabilities and increasingly [[artificial intelligence]] and the whole rest of our tech stack [3508](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3508.44s)

it is up regulating through competitive dynamics but up regulating in this narrow benefit kind of way and so we could say this thing that is driving [[climate change]] and driving [[species extinction]] and [[dead zone|dead zones]] and oceans and coral loss and desertification and [[arms race|arms races]] and polarization and all like that that is a misaligned super intelligence [3535](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3535.74s)

that nobody can pull a plug on it's already autonomous it's already nobody can pull the plug and it is building ai because corporations actually the people it's not actually the people within they think they're the ones building it well but they're building it within corporations that have a [[fiduciary responsibility]] for profit maximization that are in [[multipolar trap|multipolar traps]] with other companies that are racing to do it that have to look at how do we commercialize this thing whatever it is right or they're building it within [[nation state|nation states]] that have to be able to compete with another [[nation state]] and what that means is that some narrow value metrics that define what wins the competition get prioritized over a wide value metrics [3577](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3577.76s)

and so it is fair to say that we already have a misaligned auto poetic superintelligence running the world running all running on and running all of the people to various degrees it is already employing all of the computational power it is developing more computational power in ai the ai is being built by it [3601](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3601.799s)

in service of itself so the ai risk scenario that utkowski or boston brothers put forward of a thing where you can't pull the plug it is upgrading its capacity to do what it does it has an objective function it's pursuing but it harms stuff that we wouldn't want harmed in its subjective function in the pursuit of it what i'm arguing is that we are already there [3624](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3624.24s)

and it is our [[world system]] and that ai is simply accelerating it and that we don't have to get to agi to have the effect of it because you already have gi we already have general intelligence in the form of the corporation's [[nation state|nation states]] and the overall system where then adding ai even if it is not fully generalized to that system you already have something that [3651](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3651.96s)

is autonomous in general that is now getting increasingly potent capacities even if it's within a bunch of narrow domains right and so before we get to the case of just autonomous ai cut be being its own risk the existing ai in this landscape is driving the entire risk landscape is driving the overall is accelerating the topology that is already in place [3678](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3678.24s)

and this is why i said i think the misaligned agi as a [[thought experiment]] helps people understand moloch but what the reality of moloch helps people understand that without getting to a total agi that the nature of the risk there is already happening then with that we have to say what would it take to prevent those risks and it's a different calculus it's a different way [3707](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3707.16s)

of thinking about it so what do we do let me make it a little bit more tangible first and talk about sub agi within this context this molokian [[metacrisis]] context what are the actual risks of ai look like i'll give a few different categorizations and i want to say i am not an ai alignment expert or a uh ai [3732](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3732.059s)

risk expert there are a lot of experts at places like mary and redwood and um other places that i think people should listen to pay a lot of attention to i'm familiar enough of those arguments and then very specifically with the [[metacrisis]] argument to see how it relates and that's what i'm speaking to here um there's something very unique about ai relative to all other forms of technology i'll speak to the deeper part in a moment but to begin with synthetic biology is very powerful like obviously it's very very powerful uh there are awesome applications there are awful applications but synthetic bio does not automatically give us the ability to make better drones doesn't give us the ability to make better high-speed trading doesn't give us the ability to make better nukes nukes don't automatically give us better bio weapons ai gives us better all of them right that's an important thing is that ai has the capacity to do optimization across all the things which means the [3800](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3800.7s)

good things which is what we all want we want to have ai work on protein folding for immuno oncology to cure cancer and on um receptor sites for new drug discovery and to make [[supply chain|supply chains]] more efficient and things like that but everything that ai can optimize it can also break you run it in reverse right to the the ai that was doing drug discovery i think [3823](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3823.859s)

it was oak ridge national laboratories was ran in reverse and came up with a bunch of chemical weapons very rapidly and uh minus sign in front of it essentially and an ai that can optimize [[supply chain|supply chains]] can also optimize exactly how to break them right can optimize terrorist attacks on them even just through cyber and things like that a ai that can do protein folding for immuno [3848](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3848.4s)

oncology can also make up bio weapons and so the first principle is that as you it's very hard to advance aia right it takes massive gpu farms it takes only a few companies in the world that can even do the chip manufacturing to do that kind of thing um it takes a lot of computer science [3870](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3870.359s)

talent massive amounts of data and etc but once it's developed and then it is connected to the internet like a large language model can run on a lot less compute than it's trained on it takes a lot to train it doesn't take that much to run it right that's a big deal and it takes a lot to figure it out once it's been figured out [3891](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3891.9s)

and you publish the paper is and this is one of the key things is by building software you know it's very hard to build you know you need programming knowledge and so on to build a software but once you've built it and you've built the sort of user interface any any old schmuck can use it and get and like reap the benefits from it so if you have a company like a google or an [3910](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3910.319s)

open ai or whatever that says hey we're going to put safety parameters on this i think there's a bunch of arguments against why that is even if they could do it won't be adequate but they can't because um you know you had the i think it was llama or alpaca the stanford meta one that ended up getting leaked through some github leak and then somebody died [3933](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3933.96s)

downloaded it onto a computer started sharing it and that means the full power of something that probably cost tens of millions of dollars to train is now unlocked and will be available for anybody to use for all the purposes so the safeties not there there are some projects working on decentralized ai that are about to make at least gpt 3.5 level kind of unlocked widely available [3958](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3958.619s)

that thing is impossible to avoid and so the thing to understand is that it takes a lot of work to develop the new capacity once it's developed the barrier of entry to be able to do the things that it allows to be done has been radically lowered for everybody that all the good things come from that we can all do more creative stuff that's exciting all the bad things come from [3982](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=3982.2s)

that and so there's this principle that we could say all technology is dual use right you're developing it for some positive purpose but it has a military application but it's not just dual every technology is kind of omni use meaning it will get used for all the uses that people have incentives to use it for who are capable of using it so however much you lower the barrier of entry anyone [4006](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4006.2s)

who can go into that barrier of entry now we'll use it for the things they have incentives for so you develop it for we're going to cure cancer but now you've got that ability for biological engineering really easily available widely right um and so i think the the couple things the first thing to [4030](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4030.619s)

understand about ai is ai can make better cyber weapons better [[nuclear weapon|nuclear weapons]] better drone weapons better all of those things better info weapons better population centric weapons and so it increases the capacity to increase all the other risks in a way no other thing does you could kind of say well does that right energy every industry [4053](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4053.48s)

needs energy yeah but it's not doing the novelty search part of figuring out new better ways to make those domains it's simply just allowing them to do more of it right the ai both allows you to scale the stuff but also allows the innovation of way new better stuff so that's novel that's a very novel thing about it so the first principle is anything ai can optimize it can break [4074](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4074.72s)

you develop it for one purpose it'll get used for all the purposes if anybody can figure out how to use it for you try to make safeties and whatever but you create an incentive now for a bunch of cat and mouse type dynamics on how to utilize that and obviously we can think about the ones it just involve synthetic media and increasing [[hypernormal stimuli]] and ubiquitous deep fakes and really dreadful things like that right like there's in terms of population centric warfare and breakdown of government and public trust and there's a lot that are very very near term so this is one set of risks anything that the ai can optimize it can also break that's kind of the bad actor case [4119](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4119.299s)

but the other case the more molokian case is just accelerating the thing that is already happening the externalities that aren't included in the optimization function is accelerating the externalities when we're already hitting the tipping points on the externalities and so you could say yeah but ai is going to make it to where we can produce [4140](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4140.96s)

things so much more efficiently that it'll actually save the environment um not that there isn't a way to do that and those are the things that i want us to pursue but we're not on track for that you've got this kind of jevin's paradox that when you increase the efficiency of energy you don't actually use less energy use more energy because [4164](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4164.179s)

now energy is cheaper which opens up a whole bunch of new markets that weren't open before the same as truth compute you make compute cheaper you use more compute not less compute and so ai make some stuff more efficient more efficient just means there are more things to which i can apply energy to get more energy as long as there's positive return we will go for it now to not go [4182](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4182.239s)

for all those things you can't do it purely incentive you have to do with deterrent with agreement with law with some other thing which is not adequate and in speed to the overall situation right now so one risk of ai is it anything you can use to optimize you can also use it to break it increases all of the other [4204](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4204.32s)

risks in a way nothing else increases all the other risks it increases the total complexity of the risk landscape etc the other one is that even when you're using it for the positive purposes and you're succeeding at whatever your positive purposes are you're also speeding up externality right as we're hitting the tipping points the next problem is you're increasing the info complexity i think yukowski calls it um inscrutable matrices of floating point numbers talking about the large language models and like nobody actually knows what the is going on inside of them right so these kind of black boxes so the only thing that could figure out is your ai actually doing the thing it's supposed to do or not how do [4242](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4242.239s)

you if we wanted to if we wanted a law in some way to be able to regulate it or adjudicate what's happening it would take another ai that's more powerful to do it and so now you end up getting an increase toward you of the race towards the info singularity where people can't actually make sense or adjudicate any of what's happening right the total complexity of [4268](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4268.4s)

everything is beyond our ability to process and that just means the unsolvability of everything increases so what i'm saying is that if you add something as fastly recursive and powerful as the increasingly generalized ais that we have to an already misaligned super intelligence near the [4289](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4289.239s)

boundary points of breakdown that's a problem and that we should figure out alignment first so what i'm suggesting is in the way that the ai risk community is saying we should really try to figure out alignment before we race forward on developing more powerful ais i'm saying yes we really need to figure out alignment but it doesn't just mean [4312](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4312.26s)

alignment of the ais it means alignment of the existing general intelligences these cyber general the capitalist model essentially so it's not fair to call it capitalism it's more like [[game theory]] capitalism just makes it easy to think about because there is actually a metric and it it turns out that that you know the capital can be used for [4333](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4333.98s)

population-centric warfare or [[supply chain]] or most any of those things it ends up being kind of a unit of power pretty widely but yes that system that and it's hard to even call it a system right that set of um [[perverse incentive|perverse incentives]] and the coordinations that arise from it is misaligned reaching criticality alignment in that [4355](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4355.28s)

thing has to be figured out because a misaligned context cannot develop aligned ai it can't emerge from it because that's i think some people's hope is that just give it enough capability and some emergent magic will essentially come this is i think why the orthogonality thesis is important which is to say it is possible to get very [4380](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4380.54s)

good at optimizing and not get a good at picking good goals right those are two separate things and we already see that in the world we're already much more good at creating tech than we are at creating a world that everybody thinks is a world that really makes sense right so this is the [[exponential tech]] gives us the power of gods we have not yet seen to demonstrate the love and [4404](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4404.0s)

wisdom of god's needed to bind it we have to figure that thing out or this thing kind of caps out same with the ai right and we've handed it over to the shitty god as well the worst one so ai has fast enough feedback loops and enough power that it is one of the only things that could help change the other thing in time change [4428](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4428.679s)

the paper clip maximizing nature of the global system right of the molex system in time but only if it was developed in association with the cybernetic systems that were actually aligned and aligned here doesn't just mean with our intent aligned means actually with our our long-term well-being and this is one of the critical issues is when we talk about alignment aligning ais with human [4452](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4452.0s)

intent would not be great because human intent is not awesome so far right like that's kind of the point is that um whether we are looking at the overfishing of the oceans or proxy wars or whatever it is we're like we want to give exponentially more power to this species with its intent cotton [[multipolar trap|multipolar traps]] the way that it is it is not a good [4477](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4477.32s)

steward of power in the bronze age it wasn't in iron age it wasn't it still isn't but with exponentially more power exponential externalities and exponential conflict both eventually break the finite playing field and so how do we and you know solving those [[coordination failure]] [[multipolar trap]] type dynamics are necessary for the wisdom to be able to prevail adequately um so i am very hopeful of the very of the uniquely positive things to support coordination that computational capabilities and [[artificial intelligence]] in particular can help with they are not being developed for those purposes and in contexts that have the right frameworks [4524](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4524.0s)

and the right incentives currently it's very much the opposite thing so rather than build them to be able to change moloch they are being built by moloch in its service even though no one building them would say that but the nature of the capital that they are building it with has that built right into it inherently yeah so the sort of attic of the thing that to me is the anti-molecular but i don't even like to call it the anti-molex because it is something that operates that by calling it the anti-molex it says it's on the same plane as it's the same dimensionality and it's something higher than that uh is this thing you know if malik is the god of lose lose games [4568](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4568.64s)

negative some games what's the god of positive sum games i call it win-win everyone has different names for it you call it omnia it is that the direction of the type of [[artificial intelligence]] we need to build and if so how would we go about doing that could we use a whole bunch of info technologies [4593](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4593.54s)

including [[artificial intelligence]] including we can already see that all the problems that we saw in the social dilemma that you show in your second molok video and some in beauty words the [[social media]] related ones well that's it's that is actually already a certain kind of ai right it's ai that is curating the news feed aligned with an objective function the [4617](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4617.06s)

objective function is either time on site or engagement or something like that some combo of metrics and so of course if the objective function is to maximize your engagement things that are addictive will do things that as you often tribalize will do and so it gets to take now what's interesting about the moloch part is so far facebook or tick tock or whatever has not been creating [4641](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4641.0s)

its own content but it has been incentivizing all people to create content that will rank and as the people pay attention to what ranks or doesn't as you saw beauty filters were one thing and there are a lot of other things even so far as that all of the legacy media now does stuff that will make it rank on facebook and twitter or whatever because that's increasingly where the eyeballs [4659](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4659.48s)

are coming from so even that which would seem like an alternative source is still actually influenced by that thing so the to be able because it's so powerful to direct all human attention that's the thing right so whatever the algorithm is it's going to direct all human attention it's also going to direct all the innovation in the direction of what wins [4678](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4678.02s)

that algorithm and so and then because it's customizing the news feed for every person it's split testing what do the people click on and engage with and share and etc so it's basically just objectively maximizing for personal engagement without paying attention to if it is positive or negative [[reward circuit|reward circuits]] and it happens to be that negative [[reward circuit|reward circuits]] are easier to hack than positive ones most of the time and because the positive ones you want to get the off the computer and go do other stuff and then doom scroll but the doom scroll mass share thing the negative [[reward circuit|reward circuits]] feed better on so that's already an example of a bad objective function and an ai that has made it to where there are [4717](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4717.8s)

almost no adolescent girls with a good body image right dysmorphia is kind of ubiquitous it has made it to where polarization is as extreme as it is and on and on like the externalities are massive what it's done to attention span and um now with synthetic media we're talking about not just an ai that can curate but [4738](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4738.38s)

can create now you can imagine a feedback between those when one is creating things it will be maximally sticky to you based on personal dynamics split testing multiple created ones and the other one is curating but now not just getting all the humans to do decentralized create creation but also the as you can see that kind of feedback but of course if [4760](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4760.58s)

we had if we change the objective function of [[social media]]'s ai right and rather and you did things like uh it's not just how much engagement it gets which can be fighting that is polarizing but something that say gets positive engagement across political divides and ideologic divides and [4781](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4781.76s)

memetic clusters which is stuff that we could do right we could do we have the info science to do that which means that it's identifying places where there is shared agreement or shared perception and upregulating that rather than the most divisive stuff you'd have a totally different world and the technology could totally do that um it would not be as good for ad sales [4800](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4800.78s)

right now because it actually wouldn't keep people online quite as long and so this is where the fiscal model up the application of the tech because we're not just saying all [[social media]] is bad we're saying the incentives make us develop the  up versions of it right and so we could make [[social media]] that was exposing people to different ideas [4823](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4823.159s)

rather than reinforcing their existing ones and growing their network to include people of very different types and like so similarly it rewards them to go out and touch cross essentially and you know almost rewards you for time spent off the app or something like that there's probably two they're a lot i mean now one of the challenges is of course the ai coming you can say we're [4845](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4845.12s)

just giving people what they want right but it's manufacturing demand on bad [[reward circuit|reward circuits]] in the same way mcdonald's can say we're giving people what we want but you have a much more obligation yeah exactly right and so on the other side you're like well it sounds kind of um paternalistic that we should pick what are the good [[reward circuit|reward circuits]] and but it's like no if you're influence if if you have that much asymmetric influence over humans to not take some responsibility for what the statistical changes in their life are is actually silly right now do we want the corporations to do that themselves do you want the government to do it we don't trust any authorities adequately right now for good reason so there's [4884](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4884.54s)

some very deep conversation around how do we ensure that the power of that technology is optimizing for things that actually increase quality of life in meaningful ways um but the same with all new types of ai could we use it to radically improve governance where we could actually be able to have the large language model [4904](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4904.1s)

see what people's beliefs and sentiments across the entire space are and find the topics that actually a lot of people agree on that super majorities would agree on and start there and be able to make platforms for candidates to actually be able to represent the the wills of the people better because we actually can see what they feel and believe at scale and [4922](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4922.1s)

um would it be possible for it to work on identifying things that a lot of people would uh believe or at least for it to give information about the stack ranking of the distribution of values to a proposition crafting process so that it could craft better propositions right so we're not saying that there are not awesome applications including maybe [4941](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4941.84s)

maybe the ai applications that are critical to be able to fix these moloch dynamics because they can actually help coordination right but if you are not trying to actually understand the [[coordination failure|coordination failures]] deeply enough and say what should we really be trying to solve in terms of fixing coordination that makes this whole technosphere compatible with [4961](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4961.699s)

the biosphere compatible with [[human nature]] compatible with meaningful definitions of human flourishing it if it is as powerful as it is serving anything other than that the externalities in those areas will become increasingly catastrophic or dystopic or both are there any like promising projects that you think are trying to harness ai in a in this sort [4983](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=4983.9s)

of again i don't like to say antimonic in this win-win way or you know using info technologies in a in a way that is more aligned with what is actually good for [[human nature]] in the biosphere i think we can see like audrey tang's work in taiwan with creating digital democracy where they look for unlikely [5006](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5006.159s)

consensus and they're starting to apply large language models um and things like that i i believe and um and you know a lot of the work that say the ethereum and some of the web 3 communities tried to do with public goods is obviously thinking about some elements of that and um yeah it's not that nobody is but nobody with the giant [5027](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5027.94s)

capital in gpu farms and etc has that as their primary objective um and the primary objectives of all of those ones have milwaukee and dynamics involved even if they also have some good dynamics involved and that's a problem and that's what i would most hope to start to shift is in recognition of the total power that is there the total number of things that will be [5055](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5055.78s)

affected by it the downstream end order effects of it the speed the nature the irreversibility and there is a precautionary principle thing that uh who's right utkowski or cristiano on a alignment or whatever it is well given that most of the people who have studied ai alignment very deeply are concerned with the pace and [5078](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5078.76s)

direction with which we're moving that's a pretty good sign that we should pay attention and where there is disagreements between experts but where there so there's where there's radical uncertainty but also maximum consequentiality and irreversibility go as fast as possible is not the right answer what if there was a way to just remove [5103](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5103.06s)

the competition side of it entirely like i know it sounds very pie in the sky but like if all the major companies you know which yes let's say the companies themselves are as by definition misaligned or like aligned with mollock but if the companies would essentially hang up their competition hat not compete and work together as a sort of single entity almost like a [5124](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5124.719s)

you know uh a collaborative science project essentially would that solve the problem fascinatingly when we talk about the way that laws get built that end up supposedly having good purposes and they do but they also end up being part of moloch like the [[fiduciary responsibility]] the director or whatever uh currently i [5147](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5147.64s)

think anti-trust laws would try to prevent that thing from happening um that you're mentioning and so the government would actually have to get involved but uh would i like to see the major uh ai labs together with the major academic researchers together with the regulators take very seriously [5171](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5171.42s)

uh short medium long-term futures of ai that factor all these types of assessments that factor the molokian dynamics the [[metacrisis]] the way that the ai that is the safeties won't totally last right they will get decentralized the safeties will be taken off it will be used for all purposes the kind of omni-use nature of it and to look at what does responsible [5196](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5196.8s)

movement forward in light of that look like i um i don't see any good answers that don't involve that yeah because that's the thing but the one thing mollock requires you know one thing [[game theory]] requires is competition of some kind some kind of either fabricated or real scarcity and in this instance there isn't really you [5221](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5221.679s)

know there are no intellectual powerhouses trying to compete to be the first to do a thing everyone is working on the same project together it's the ultimate cooperation thing um but yes it seems like a very almost intractable salute you know method because it would actually need to be an international [5241](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5241.179s)

collaboration as well it's not just you know i mean we do have the one advantage that it seems like the majority of the major players at least are all in western hemisphere and they all speak the same language and to degree share similar values um you know we're not well maybe not so much that but that is one starting point [5260](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5260.139s)

um there are some very advanced ai labs in china um and the government-run ones versus the corporate ones are slightly different um there's obviously ai that is not just large language models for public deployment that is still also risky for many other purposes i think the the large language models for public deployment has a unique case because when the whole world starts uh utilizing them for the many many purposes they will it'll become nearly impossible to roll it back once we see the risks that are associated because we will have created so much economic and cultural dependence on it um so i think even if all we're talking [5303](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5303.46s)

about is as a start all the major gpu farms and large language models being in cooperation that would be a thing but i do think it does need to be wider than that and not just corporate and international uh and i think the i don't think there are any [[existential risk|existential risks]] near term if we don't i don't think large language models destroy the world i think there will be problems and then we'll create solutions to those and whatever but i think it absolutely accelerates the overall [[metacrisis]] across many many vectors which is why one has to actually take that seriously to then really try to take responsibility for this thing [5354](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5354.04s)

for which there is so much incentive to not take responsibility absolutely is there any like specific call to action you want to put out there if there happened to be any you know major technology technologists listening to this i mean to to begin with it's really to engage with the wider risk arguments not just in super intelligent agi taking [5378](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5378.04s)

off but the acceleration of all of the risks that the release of these things cause to really engage with those arguments seriously and to really think about the world that's creating and be like all right despite the humongous incentive for me to rush ahead with this uh is the risk calculus high enough the speed and the irreversibility high enough that i am actually um inclined to figure out something to to figure out better coordination around this and to simply start engaging in um more percentage of their total energy [5424](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5424.6s)

going into risk analysis than opportunity advancement to have that more attached to the actual governance and choice making i think all companies working on agi should kill the fiduciary agreement to maximize shareholder profit obviously um some have talked about that i think that should be killed and i think the earnest real engagement [5445](https://www.youtube.com/watch?v=KCSsKV5F4xc&t=5445.32s)

between the ai labs the people in ai safety research and regulators should be a very actively happening thing don't be more lucky i can print me one of those bumper stickers