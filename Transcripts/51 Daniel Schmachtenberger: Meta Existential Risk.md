---
tags: transcript
aliases:
youtube_id: a6ejROOyHW0
---

<div class="yt-container"><iframe src="https://www.youtube.com/embed/a6ejROOyHW0"></iframe></div>

now entering that kind podcast network music today's episode is brought to you by kiki the easy safe and simple way to protect your bitcoin ether litecoin and many other digital assets there's no time like the present to protect yourself from hackers malware and viruses visit kp calm to order your secure hardware all today and use the code humanist 10 for a limited time 10% [44](https://www.youtube.com/watch?v=a6ejROOyHW0&t=44.11s)

discount hello everybody so before i start today's episode i just wanted to do a life update so i've joined the mit media labs digital currency initiative as their head of community and long-term societal impact which i'm pretty excited about a really nice group of blind people there so that means that i'm gonna shut down my patreon i've shut down my patreon and [68](https://www.youtube.com/watch?v=a6ejROOyHW0&t=68.71s)

that this podcast will likely continue but possibly in a different form with the media lab or with the dci or whatever so we'll see and i just another quick note there my this podcast was recorded before i joined the dci so clearly these are my views and do not reflect those of the digital currency initiative of the dci so with that let's get to today's episode and it's i [91](https://www.youtube.com/watch?v=a6ejROOyHW0&t=91.87s)

interview danish mactan burger today and daniel's done a lot of thinking about these you know kind of complicated complex macro systems that we live in and that's just a quick i mean i don't know if it's a warning it's also kind of excitement daniel and i really like operating at these levels of abstraction so that's where we got when we today about one big topic which is a meta x [113](https://www.youtube.com/watch?v=a6ejROOyHW0&t=113.62s)

risk and so this is not only thinking about kind of traditional and bi extras i mean [[existential risk]] so this is not traditional thing about like [[existential risk|existential risks]] like [[climate change]] nuclear security [[biosecurity]] or ai alignment instead it's asking a question what actually creates those x risks what creates those [[existential risk|existential risks]] and you can think of this as something like two [135](https://www.youtube.com/watch?v=a6ejROOyHW0&t=135.4s)

kinds of processes where we've had evolution as a process and evolution was a generally a nice process that um thank you evolution for creating me evolution had micro rivalry but created macro anti fragility right so the like the earth as a entity that was a a blossoming anti fragile symbiotic nature thing but we've humans have created this new process as a result of [163](https://www.youtube.com/watch?v=a6ejROOyHW0&t=163.23s)

abstraction and design and technology and science and this kind of process it creates it has micro rivalry in the sense of like market competition but it doesn't have macro antifragility right we don't have if you look at the world today or something like the [[industrial revolution]] [[industrial revolution]] was created by a process and that process doesn't have a built into that process [184](https://www.youtube.com/watch?v=a6ejROOyHW0&t=184.62s)

is not something that says hey let's internalize our externalities or you know let's close these loops and instead we get something like [[climate change]] and so these this process tack is a process gives us these [[multipolar trap|multipolar traps]] with this [[exponential tech]] that will produce these exponentially harmful externalities eventually and so that's the world that we live in and we need to [209](https://www.youtube.com/watch?v=a6ejROOyHW0&t=209.16s)

switch away we essentially need to create a new kind of process design a new kind of process that kind of takes the best of both worlds here where we have where we create macro anti-fertility but it's not created as an emergent process of evolution and the micro rivalry but instead is actually kind of by design by by us as humans so that's primarily what we talk about [232](https://www.youtube.com/watch?v=a6ejROOyHW0&t=232.88s)

today and just as a quick note you know we put a lot of context before this so it's definitely juicy in the first 30 minutes but it gets especially juicy in the last 30 minutes or we go deep on this so with that hope you enjoy the episode music hello everybody my name is rhys landmark and you're listening to another episode [255](https://www.youtube.com/watch?v=a6ejROOyHW0&t=255.57s)

of creating a humanist [[blockchain]] future in this podcast we take a [[systems thinking]] approach to doing good in the world we have a couple different series to focus on different system scopes and today we're focusing on series a macro systems where he asked the question where are we as humanity headed and to dive into that question i'm very happy to introduce danish mactan berger to [273](https://www.youtube.com/watch?v=a6ejROOyHW0&t=273.63s)

this show daniel loves exploring various topics of civilizational significance trying to shake them for good and he's one of the best systems vocal processors i know and much of my own thinking has been shaped by him so daniel thanks for being on the show and welcome my friend me too okay so we're really gonna push the question where are we as humanity headed today and let's gonna start at [292](https://www.youtube.com/watch?v=a6ejROOyHW0&t=292.29s)

the top here and before talking kind of too deeply about our current context or what's happening daniel how do you think about kind of goals like what do you think about goals for our civilization or when you're trying to like you know optimize for a metric or something like that what are your big goals mm-hmm so before talking about what i think good goals would be i think the question [315](https://www.youtube.com/watch?v=a6ejROOyHW0&t=315.21s)

you're actually asking about is is a goal-oriented framework actually a adequate or good framework or they're other frameworks we want to take and so obviously western culture has been and particularly industrial post scientific technological and even computational culture has been very oriented to goals kind of process and we can attribute both lots of successes and most of the [342](https://www.youtube.com/watch?v=a6ejROOyHW0&t=342.75s)

failures of the world or many of the failures of the world to that process because if i define a goal and it doesn't include everything that will be affected by the nature of what i do to get there then whatever is affected that i didn't define as part of my goal as the space where externalities can happen and even in the you know very well-intentioned nonprofit you know [366](https://www.youtube.com/watch?v=a6ejROOyHW0&t=366.35s)

whatever kinds of sectors we see all the time having things like okay so we have a goal to end extreme poverty and childhood hunger globally or within a particular area that seems like a pretty awesome goal as well she's ever seen the child who's hungry or mother watching her child be hungry the tricky thing is if my only goal is to end hunger i can do it through unruh [390](https://www.youtube.com/watch?v=a6ejROOyHW0&t=390.72s)

noble agricultural methods that dump nitrogen into the river deltas removing [[dead zone|dead zones]] in the ocean even faster moving us towards actually extinction as a species and collapse of the biosphere writ large which is okay so we prevented some people from dying over the very short term and made it more likely that everyone dies with medium term yeah so at minimum when we think about goals we [417](https://www.youtube.com/watch?v=a6ejROOyHW0&t=417.3s)

could at least just start by this hand waving kind of thing of saying our goal is x whatever x is right stop extreme poverty and just add this little caveat which is our goal is to do that and not externalize harm to anything else that matters and now when you start exploring how do i know what else might be affected that could have harm externalize that isn't obvious in very [444](https://www.youtube.com/watch?v=a6ejROOyHW0&t=444.15s)

[[complex system|complex systems]] and with technology that has lots of different causal dynamics that are associated and then the question of what are other things that matter and what is my ethical and existential framework for assessing that then you start to realize that that kind of hand wave leads it way to a much deeper process of what we might call omni consideration yeah let me pause you [470](https://www.youtube.com/watch?v=a6ejROOyHW0&t=470.729s)

for a second actually there because i think that there's a lot of good juicy stuff a as you said we're kind of so used to thinking of this goal framework and it's almost it's hard to get out of it in some ways and like you say i super girth you say once we create a goal whatever that goal is x and you also say what are all the externalities of x and how can we make sure that we're you know [491](https://www.youtube.com/watch?v=a6ejROOyHW0&t=491.97s)

minimizing the negative externalities and upping the positive externalities of that x is there i guess to go a little bit deeper on that this kind of goals perspective and you kind of start to allude to the stomach's is like there are there are people in the world so i would say i'm like a relative kind of effective altruism list in terms of i agree with you know generally agree with [513](https://www.youtube.com/watch?v=a6ejROOyHW0&t=513.169s)

like consequentialism and kind of outcome-focused ends focused mindsets those are all kind of goal they have kind of goals as a part of them so how do you think about something like effective altruism as kind of a mostly a goal focused or kind of an outcome focused mindset is that going to be is that good for shaping the world positively effective altruism is a [531](https://www.youtube.com/watch?v=a6ejROOyHW0&t=531.98s)

valuable step in the consideration process of saying if we really care about having actions that have a positive impact on life at large how do we go about influencing our choice making basis of what will be effective and how can we use very effective tools like rationality to inform better choice so insofar as it's asking what are the right epistemic frameworks to inform [565](https://www.youtube.com/watch?v=a6ejROOyHW0&t=565.64s)

good choice that's a great step now of course i'm going to say that it is a limited step and the limits are actually fatal and the direction and permeate rates have to happen so here's a couple examples when we talk about the limits of goals because the goal is always embedded within a larger context and so you have to factor it's in the embedding functions and that's where the topic of [595](https://www.youtube.com/watch?v=a6ejROOyHW0&t=595.79s)

externality comes and how even think through externality we're mostly only going to forecast within domains that we already know a lot so as we're getting into exponential technology where there are faster rates of change more complicated more [[complex system|complex systems]] etc then there's more stuff that's going to happen that we don't forecast well and faster coupling from the micro context [619](https://www.youtube.com/watch?v=a6ejROOyHW0&t=619.87s)

to the macro context it used to be that whatever happened in a microwave wasn't going to affect the biosphere or human civilization at large because stone tools just don't have as big a consequence as ai or nanotech biotech do and so because of the larger scale and faster timeline coupling coefficients of micro action to macro dynamics externality is that much bigger of a [643](https://www.youtube.com/watch?v=a6ejROOyHW0&t=643.15s)

topic right and it's an exponentially bigger topic so let's just take small cases and didn't look at what happens when you put that on an [[exponential curve]] so say we take medicine because in medicine we're talking about what is health and but in a body that's a [[complex system]] but we take a complicated approach of trying to define health in [661](https://www.youtube.com/watch?v=a6ejROOyHW0&t=661.759s)

terms of a finite number of metrics so we're going to look at your low density lipoprotein and your high density lipoprotein and a couple different cholesterol metrics and blood pressure and blood sugar and whatever in and as time goes on we look at more and more metrics but then the idea is that there is a range in which we want to see those metrics and so then the goal is at first [680](https://www.youtube.com/watch?v=a6ejROOyHW0&t=680.629s)

we can just say okay your cholesterol is too high our goal is to lower cholesterol that seems like a reasonable thing if i have a reason to think that high cholesterol increases the probability for certain diseases but now i have to dig a little bit deeper in this first step is obvious the second step is more important the first step says okay when i say cholesterol is high [701](https://www.youtube.com/watch?v=a6ejROOyHW0&t=701.179s)

so i want to lower it giving a statin drug actually works it makes sense i'm gonna lower low-density lipoprotein i did not ask why the cholesterol was high in the first place is that possibly a protective mechanism to the arterial system from some type of oxidative damage or toxicological damage or whatever that that's arising from is that a sign of other systems homeo [725](https://www.youtube.com/watch?v=a6ejROOyHW0&t=725.269s)

dynamic systems being out of balance we're just addressing that symptom means underlying other patho ideology will continue is there a side effect on metrics other than ldl and maybe ones that we haven't even measured and done longer-term analysis on of the method that we're using etc so then we can say well the statin works for that but not how the system is more dependent on it [747](https://www.youtube.com/watch?v=a6ejROOyHW0&t=747.169s)

its own ability rated cholesterol is less good why it was off we still don't know and the side effects of the statin might be things like liver toxicity brain toxicities and we say okay well that's not the best method but that is still our method right that's how we do medicine and that's why if you think about medicine we have really amazing acute medicine and really terrible [766](https://www.youtube.com/watch?v=a6ejROOyHW0&t=766.459s)

chronic medicine so in acute medicine meaning if we know the acute onset of the illness like you just got shot and the problem is a bullet that went in we can take the bullet out sew it up and there isn't deeper underline more complex causation pretty straightforward and we're really good at that you just got an acute infection you just got an acute [789](https://www.youtube.com/watch?v=a6ejROOyHW0&t=789.68s)

poisoning we can deal with those things pretty well but when we talk about psychiatric disease or autoimmune disease or neurodegenerative disease or cancer or anything that doesn't have one obvious immediate cause you'll notice that for the most part we don't have anything that actually looks like a real cure or prevention we have symptomatic treatment and we might have things at [811](https://www.youtube.com/watch?v=a6ejROOyHW0&t=811.88s)

slow rates of progression and the reason we don't and so you can't there's a classic example we can cut it a cancer that we can radiate it out we can poison to help but we're not that good at saying why is it growing in a way that it wasn't when the person was younger and that it doesn't and other people and how do we actually address that because it doesn't have one cause it has a whole [831](https://www.youtube.com/watch?v=a6ejROOyHW0&t=831.44s)

bunch of different causes and they're not even the same set of causes in different people so now we have to actually not do a generalized bell curve approach but do a let's understand this [[complex system]] uniquely factoring multifactorial causation delayed causation causal cascades should that we are epistemic limon that good at we do the same thing for the world when we're [854](https://www.youtube.com/watch?v=a6ejROOyHW0&t=854.24s)

saying we want to feed kids in this area and we're not saying what is wrong with the macroeconomy that actually creates poverty and what you know etc like what are the actual causes of this problem and then of a particular solution that we're going to come up with it doesn't address the causal dynamics well what are the side effects of that solution going to be is this hagrid cultural [879](https://www.youtube.com/watch?v=a6ejROOyHW0&t=879.65s)

method going to lead to more nitrogen runoff leading to larger [[dead zone|dead zones]] in the ocean that is actually worse for everyone long term and writ large and so what we find is then we look at the ldl and say okay so we want to look at ldl and these liver enzymes and a few other things and then we try and put more metrics and run optimization on those metrics which didn't we say big data and [901](https://www.youtube.com/watch?v=a6ejROOyHW0&t=901.73s)

and machine learning will help us do but this is the kind of the key thing no matter how many metrics there's a couple things here when we say that goals are limited we also are saying that optimization theory is limited for a couple critical reasons so now i go from two metrics to 200 or 2000 metrics but there's still other things that are happening that are outside of [930](https://www.youtube.com/watch?v=a6ejROOyHW0&t=930.329s)

the metrics that i'm seeking to optimize that are mostly in either the beyond factorable within this model or the actual unknown unknowns but that might be critical to the whole system and so when i'm running optimization on these end metrics i'm also affecting a lot of other stuff that i'm not paying attention to optimize which is this places where i'll [950](https://www.youtube.com/watch?v=a6ejROOyHW0&t=950.16s)

probably externalize harm is i'm driving a system where all those things are interconnected without actually acknowledging they're in the correct and infected in other words our model of a [[complex system]] is not how the [[complex system]] actually works so when we treat the actual reality as the model and we try and optimize for the model the place in which the reality is not the model [968](https://www.youtube.com/watch?v=a6ejROOyHW0&t=968.79s)

gets up and so when you define a [[complex system]] as something that has an [[emergent property]] beyond how you model it when you model it you get certain system dynamics awesome complicated system [[complex system]] is the predicted dynamics of the model don't explain all of the dynamics then you have to say it's very different epistemology other than n-dimensional metrics an [991](https://www.youtube.com/watch?v=a6ejROOyHW0&t=991.29s)

optimization theory to account for what is actually going on so pause for a second so i agree with a lot of that and i want to talk about this sec but so you talked about the i agree there's a bad forecasting piece or we live in this world where we have black swan style events and over hyper interconnected lots of power dynamics of external or lots of sorry exponential's happening [1013](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1013.459s)

and when all that happens if we set our goals and then don't account for the externalities or actions then the sad externalities can turn out to be really big because everything's so interconnected and powerful now and i agree with what you're saying when you think about any of these actions and this gets into how kind of to think about this stuff is like when you think [1032](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1032.03s)

about taking an action before you do it think hey why is there this problem in the first place to try to go to that root cause level and then b what are the externalities of that action so in this final piece which you're getting to here's this map is not the territory thing here and it's especially true for [[complex system|complex systems]] and so i just kind of want to recap [1051](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1051.53s)

that's tougher listener and then have you go and say okay so if we can't use this if i can't just keep adding metrics to myspace is there an ak what is the new epistemology that i should be thinking about that will actually allow me to allow us to to succeed in this new world right so i want to share another framework that will help the [1069](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1069.01s)

consideration here and this is not a happy thought at first but it's a necessary thought which leads us to be more effective in considering how to have happy thoughts around this so it is that it is a lot easier to break than it is to actually protect or build stuff and there is a entropic preference that we have to consider and just to have a very simple kind of prima facie [1100](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1100.39s)

of that the house that i'm in probably took a year to build and the record ball can take it down in minutes and if you think about an old-growth forest and the thousands of years it takes to get the biodiversity and complexity of that forest and how long it takes with a fire or a d9 that take it down you really get a sense of the orders of magnitude of the timelines for construction versus [1123](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1123.97s)

destruction of complicated or [[complex system|complex systems]] and everything from from things like that how long does it take to build a globalized infrastructure and how quickly could we actually break it all the way to how long does it take to build trust and how easy is it to break it and so when you factor that you realize that there's a couple orders of magnitude more ways to break stuff than [1148](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1148.0s)

to build it in faster time processes on that so when i'm factoring externalities and i'm thinking about some new technological capacity and i don't understand actually forecast the possible externalities well because i don't even know that space i'm opening up a new epistemic space with new types of causal dynamics and feedback loops and couplings that are very hard to [1169](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1169.75s)

think about and there's an orientation that there will be more total ways of having destructive effects than constructive and that they will be faster this is something we have to consider very deeply and when you can imagine when we first got stone tools down we killed a lot of things faster than we ever had before and we started the [1193](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1193.45s)

beginning process of [[species extinction]] human induced [[species extinction]] when we first started kind of figuring out fire we burned a lot of down before we got good at containing fire but when you move up to ai and nanotech and biotech and things that are pattern replicating tec meaning that the failure of the thing is not just that it caused a problem but that it causes a cascading [1219](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1219.159s)

problem that once initiated might not be stoppable you really have to change the epistemic framework of good things are mostly good progress at all costs history is written by the winners we kind of ignore the ugly parts and let's keep pushing and focusing on the good side of technology for some very limited definition of good because the destructive applications at that scale [1242](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1242.02s)

are large enough to actually destroy the entire playing field where the positive applications just don't actually get to matter anymore so there's an increasing kind of ad exponential scale oh exponential constructive and destructive forces both the destructive forces become larger than the playing field can handle in the game in so with [[exponential tech]] you either figure out how to prevent and internalize externalities or the game eventually self self terminates and though the type of process that we've used for all of history until now is itself [[self-terminating]] yep so could i you policy for saying they're saying that's a lot of juicy stuff there i'm reminded of two leaves and an ear bar yama's precautionary principle here [1288](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1288.76s)

which is the idea that even if something has you think it's gonna be really good with a 99% chance of being really good but it has a very very small percentage chance of like actual ruin or like [[existential risk]] then it's like well then you shouldn't do it you know and when we live in this world where everything is interconnected and where everything is becoming more powerful [1307](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1307.9s)

than that that lose side that destructive side can become yet becomes we have to change kind of our way is given our [[exponential tech]] so let me you follow up on that for a quick second here which is if we're trying to change our ways here and we're trying to be kind of taken all the externalities into account how yeah how can we begin to start to do that mm-hmm so great [1330](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1330.74s)

question again i'll say one other thing that i think read some of the context great one of the important things to consider when we think about the topic of negative externalities that so one thing we just said is the kind of entropic gradient right so we have to factor that another key thing is the topic of weaponization if i create a new technology and it is the capacity to [1357](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1357.679s)

affect change through the causal lever on choice right and i'm gonna make a choice my technological understanding of some mechanism allowed me to make a technology that's a big lever on my choice so my choice has bigger effect then did before well that same technological capacity can be a lever on lots of kinds of choices right and so one of the things we have to understand [1382](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1382.19s)

is that the moment we actually deploy a type of asymmetric technology then it gets reverse engineered iterated upon and utilized for all purposes that are incentivized by all agents and so i had just had some guys talk with me the other day about an ai project they were working on of how they could use some ai information technology that was novel they just developed to get out some type [1411](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1411.29s)

of what they thought were positive political agendas to solve some current near-term problems and i'm like and you don't think that the other side's that have as much incentive and pretty good intel will pay attention to the new technology you have reverse engineering and utilize it for the other purposes and simply and the and they thought about me like like i guess that will [1433](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1433.16s)

happen and i said now take all of the sides in this multi agent dynamic that all have rival risks incentive to weaponize the technology against each other for their agendas and all you do is up the ante of the playing field and so nukes didn't actually make us safer like they made a safer but we actually have increased risk of [[existential risk]] from nuclear warfare relative to a world [1455](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1455.83s)

that had just never developed nukes okay and so this is the [[multipolar trap]] where each agent doing the thing that seems like it provides and whether the agent is a person or a country or a company or whatever each agent doing the thing that seems like it's best for what they care about in the near-term actually leads to a system disposition that is worse for the system as a whole over the long term [1478](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1478.09s)

and this is what we call a multipolar track or a [[race to the bottom]] right a tragedy of the commons and [[arms race]] are all examples of this and with [[exponential tech]] we have an exponential [[race to the bottom]] type scenario so one thing can say is in terms of goals or necessary criteria of system design time constraints is that we have to solve not just for some multipolar tribe but [1504](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1504.16s)

categorically for the class of [[multipolar trap|multipolar traps]] as long as you have a rival risk incentive to utilize technology in a way that will directly or indirectly cause harm you have a near-term incentive to do that and you have [[exponential tech]] and anytime anyone deploys that everyone will be using it for rivals purposes we we are actually completely yep yep so can i pause [1525](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1525.93s)

you for a second there on that great and so i agree with this and this is in something that you call like the [[generator function|generator functions]] of x risk and these are these these [[multipolar trap|multipolar traps]] and you can look at it when you think about the [[existential risk|existential risks]] that exist today there are things like nuclear security and that's an [[arms race]] it's like you know couldn't we have stopped at like i mean [1545](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1545.05s)

maybe like three nukes or something but now we have thousands of nukes and you think about biotech again it's like people doing things there in these win-lose game dynamics where if they stop and if they don't if they don't go quicker and same with like [[artificial intelligence]] alignment it's like ooh maybe google and baidu and the you know chinese government's new whoever are [1562](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1562.99s)

going to be nice around and be and try to be safe but actually they're gonna be in this win lose game dynamic where if they're the first ones to get agi then they will have lots of rewards and so there's a disincentive to kind of chill and wait for the safety things and something like [[climate change]] obviously is a an example of the that's like a perfect example the tragedy the commons [1582](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1582.1s)

where you have it didn't matter that much at the beginning when we started the [[industrial revolution]] that we were pushing stuff into the atmosphere but as you say once it gets exponential then and we ignore that externality then we're in one of these multiple traps or no one's caring about the earth and then everybody gets screwed up so i agree [1597](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1597.65s)

with all that i also agree with you and saying this weaponization piece is a very interesting one it's and it takes it's weird mindset that we talked about a little bit before with this weird kind of omni considerate mindset where you have to kind of this is both powerful from an individual perspective we tried like a steel man the other person but it's also very powerful from this kind [1615](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1615.8s)

of systems perspective we say hey if i do something and well maybe it's good for me but what happens if everybody does that you know then is it still good and is the thing that i'm using the tool and i just did a podcast on new power these kinds of new power are those tools and those kinds of powerful things how are they going to be used by quote-unquote the other side you need to [1635](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1635.33s)

think about that in terms of this weaponization piece um so i agree with all of you here one thing that i might be worried about just as a heads up is like this to some extent i think that people might get stuck in a trap right now which is where i'll make maybe like a postmodernist trap where it's like you we had been in the the world of modernism we were excited by the world [1656](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1656.42s)

we said hey we're gonna be nationalists we're gonna go go go and then post-modernism came around and we rejected those myths we said no nothing can be real and like everything is kind of kind of gradients or whatever and it feels like it may be a similar thing here where you're like you say like oh man reese we need to think about all the externalities all the time or like how [1674](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1674.929s)

many consideration all time or like how can we how can we then make choices if we have to be worried about the externalities kind of of everything yeah great question and as you can guess and what i'm going to advocate is an adequate at the seeming and ethical framework is not post-modernism nice a couple more parts that we need to construct in here is so one thing [1700](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1700.82s)

regarding weaponization and just a very simple practical thought for people who are wanting to do good i was having this conversation with tom chi the other day and you know he's a really brilliant prototyper inventor robot assess technologists working on good things for the world and we were talking about this problem of organisation and cognizant of that his approach and i think this is a [1730](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1730.73s)

approach that bucky fuller laid out very clearly and and others have is let's take technology that already exists for weaponized purposes and start to popularize how to use it for actually positive purposes so we're not increasing the technological power landscape we're repurposing the shitty power to better power and so we aren't actually going to be driving a [1753](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1753.26s)

multipolar trapping [[race to the bottom]] we are within the current landscape of not having solved from [[multipolar trap|multipolar traps]] categorically yet we're at least driving a race to the top and i think this is a really key idea so when he's looking at how do we take drone tech that already exists for military purposes and sucks we're not gonna make fundamentally better weapon izybelle drone tech but [1772](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1772.46s)

we're going to use it to plant a bunch of trees to pull a bunch of co2 out and stabilize topsoil and other things you have a much lower risk of weaponization in that approach and so i just want to offer that as one very simple kind of strategy that people can think about now that still doesn't mean that a particular approach won't have externalities and doesn't think thought [1796](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1796.279s)

about of course if i take a desertified area unplanned trees for the goal of sequester co2 and so i take whatever genetically engineered plants the questor co2 the fastest and just plant a  ton of it i might get all kinds of ecological problems as a result of anything from mono cropping to invasive species to whatever else right so i still have to think about complexity but [1820](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1820.549s)

i don't think about weaponization increasing the potential of the power landscape as much so that's one consideration next i want to go onto something about multi polar traps but do you want to say anything on that purpose oh no no no you go to multiple traps thank you for checking them so for people who aren't familiar with this topic well yet there's a article on [1845](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1845.149s)

slate star codex meditations on moloch articles on that whole site i think and one of the better introductions the concept of multiple chunks i will share and whether we get to it on this podcast or another time a different set of insights about a categorical solution to [[multipolar trap|multipolar traps]] but just so that people understand them a little bit and why they are so hard to [1871](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1871.24s)

get out of if we you mentioned a couple examples but i want to highlight the system dynamics a little bit more explicitly because as you said this is actually not the source of one x risk this is a [[generator function]] for all x risks because one things about it and so if we don't resolve it categorically then we just kind of move the timescales a tiny bit so let's take a tragedy of [1895](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1895.6s)

the commons it started a long time ago and we just say okay we got a number of tribes that are all seeking to increase their own well-being they are in an implicit competition with each other for the same scarce resources at a certain point that implicit competition economic resource extraction wise can become so problematic that they move into an explicit militaristic competition which [1919](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1919.87s)

we can see how it happened but let's just even start to take first okay i don't want to cut all the trees down because i actually really like the forest i have a whole spirituality about our connection with nature and we need the force to live because that's where the animals live that we hunt and etc and i actually don't need that many trees i just need a few trees some only [1939](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1939.4s)

in cut down those few trees except the population of these other tribes and the total number of tribes has been increasing and they're all cutting down trees and so i know that if i don't cut down the trees it still doesn't ensure that there's a forest because the other guys didn't cut down the trees so there's not gonna be a forest anyways but he's actually my competitor because [1956](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1956.47s)

he's going to use those for weapons weapon making that if i don't also do so he will actually end up killing my people or people be able to outlive us through a famine because of total amount of economic accumulation or whatever it is so since i can't actually keep the forest because of those guys and even if i tried to make a truce with this one tribe there's this other tribe that we [1979](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1979.6s)

just can't even talk to so if anybody does the up thing of cutting down a bunch of trees then we are in a race to all get them first to keep the other guy from having them because there is no incentive and leaving them and because then it's best for us to have them so now we could have done a lot more trees than we need and so but they only do that because they're [1998](https://www.youtube.com/watch?v=a6ejROOyHW0&t=1998.24s)

thinking the same thing about us where the up other tribe to them right and so now we all realize that we have to increase how good our axes and saws are and how many people are cutting down forests to cut it down faster because we're all racing to get that wood or the pigs or the whales or the farmland or whatever a dead trade and so now we are not just destroying the world upon which [2022](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2022.52s)

we depend we are in a race to destroy it as fast as possible and if we don't we assume it will still get destroyed and it will be worse for us and we have no chance of winning right so this is a scenario where anyone does the up thing and it sets a precedent for everyone else to actually be better off near-term if they do the up thing even though it leads to everyone being [2046](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2046.34s)

worse off over the long term so we have a decoupling of the incentive of each agent from the well-being of the other agents in the comments and as a result because of the interdependency of the commons we have a decoupling of our own short-term advantage with our own long-term well-being if i take the case of tragedy of the commons we can see how recalcitrant and tricky those are if we [2067](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2067.55s)

take the case of an [[arms race]] we see the same thing and the tricky part here is we say ok we haven't succeeded with nuclear deep proliferation all that well all that we have is more and more countries having existential level power so we can't even have something like [[mutually assured destruction]] anymore and so an obviously [[exponential tech]] can make that non state level actors and not [2089](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2089.84s)

the increase of all types of dynamics but now let's take a current example let's take ai weapons right weaponized drones etc there is no general in the world that wants to live in the world where those weapons are robic waitis yep there's no military contractor military hawk whatever who thinks that's actually a nicer world to live in everybody knows that our chance of all being [2114](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2114.53s)

killed by had drones goes up if any of us build the things and so it is a comprehensively worse world for all of us and our kids and our grandkids and yeah we're all gonna build the miss fast as we can because the other guys going to build them and we say whoa okay no no we need to create an agreement their efforts being made to do this but all it has to happen is one guy doesn't [2133](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2133.88s)

one group doesn't join the agreement and now we don't want them to get the weaponized drones first so we got to do it or everybody joins the agreement but we're pretty sure they're gonna be defecting on it in secret right because they would have all of the incentive to do so so they keep the agreement and then in a black project they defect on the agreement while continuing to dis [2152](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2152.69s)

inform us that they're keeping it and because we think they're doing that we're also keeping this inform them while trying to spy on the way they're dis informing us and now all of our resources are going to both making the world as bad as we can as fast as we can well dis informing each other and ruining the information ecology maximally right we will go extinct [2171](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2171.8s)

because of [[multipolar trap|multipolar traps]] on our current trajectory if we do not figure out how to solve them these have been the dynamics that have led to the collapse of societies for as long as we've had societies and it's just when the roman empire fell as big as it was it wasn't the whole world when the inca or the mayan or the egyptian of the mesopotamian and aztec empire fell which [2196](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2196.73s)

they all tell it wasn't everything as soon as we have a fully interconnected global [[supply chain]] where there's not a country in the world that could actually build the computer that we're speaking on from scratch the mining refinements hardware tech software tech etcetera is happening across the whole world then we realize that the civilization that we only know [2217](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2217.04s)

how to build civilizations of collapse and that the collapse of a fully globalized civilization and where the environmental damage that's happening is at the level below biosphere not just a particular ecosystem you say oh the way that we have always been multiple our traps are old we have always been that has always caused wars and environmental destruction and etc is at a level of [2237](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2237.56s)

magnitude we don't actually get to make it through so it's not that the problems are different in kind there's different in magnet and speed where that ends up being different in kind because you don't actually get to keep going to the boom-and-bust cycles associated so we either solve for [[multipolar trap|multipolar traps]] which means agency misalignment or the human [2258](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2258.67s)

experiment completes him yeah yeah yeah i like i mean i think i real like what you said they're around i mean like is a simpler term i wish it were not the case but i think that you did a good job of explaining as you said we've only ever built civilizations that have collapsed and we live in a world now and and i think that you're you're correct to say like we you know we both haven't talked [2278](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2278.11s)

about the you know distributed exponential interconnectedness and i think part of the interconnectedness i often talk about like the four billion network smartphones but i think the global [[supply chain]] thing is is is crucial so as well that everything's interconnected through the physical world as well in addition to the information world and i also like what [2292](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2292.06s)

you said about decoupling when we think about these [[multipolar trap|multipolar traps]] its decoupling yourself from kind of the collective in the commons and then it's also decoupling your short-term goals from your long-term goals and then so that's where cryptocurrency is vibrant and exciting but it's not without the share of bad actors exchanges in personal cows can you hacked computers [2316](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2316.12s)

can be infected with malware left unprotected your digital wealth is up for grabs don't let yourself be a victim ppe is a safest and simplest way to protect your bitcoin ether litecoin and other tokenized assets this hardware wallet is a separate device that you control brought to you by the pioneering team and shape-shift keep he works with the wallet software on your computer to [2336](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2336.79s)

manage your private keys and transactions your device has been protected which provides protection if it falls into the wrong hands it's large display lets you carefully view and approve every transaction and if you keep these ever lost or stolen you can safely recover your device without compromising its private keys the bottom line you'll [2354](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2354.58s)

sleep easier knowing that your digital wealth is safe and secure visit keep key calm to order yours today and use the code humanist ten for a limited time 10% discount so i guess to check on one thing here though i think and and from me from like a little bit from an effective alterus perspective is is when you think about these kind of i agree that if we don't [2376](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2376.99s)

solve [[multipolar trap|multipolar traps]] in these [[generator function|generator functions]] of x risk then we will self terminate i fully agree with that but the question is at what time line and so you can imagine a world where we're trying to solve these kind of we're at this super abstract kind of meta level and trying to solve these kind of [[generator function|generator functions]] back x risk wow oops one of the x risks actually [2397](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2397.45s)

killed us you know you can also imagine vice versa where we only solve one of the x risks but then the [[generator function|generator functions]] kill us so how do you think about kind of the cost of delay there in terms of like what stuff we should focus on at which time if you are focusing on winning at the game that is killing everything then you are asleep or insane hi i kind of agree but i bet later let [2421](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2421.53s)

me pause let me let me let me push back on that for a second you can imagine a world in which let's say you and i had the ability to solve an [[existential risk]] let's say we could press a button and [[climate change]] went away or whatever that because we or we could concentrate on solving the [[generator function|generator functions]] of x risk i could see some versions and work on that for like a longer time whatever [2441](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2441.7s)

i could see a version of reality where we were certain people try to solve the the x risk itself rather than the kind of meta level [[generator function|generator functions]] we have to be focused on the [[generator function|generator functions]] because otherwise not only if we try and solve for a specific extras that is a result of the underlying system architectures and dynamics that made that risk and lots of risks like it [2463](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2463.06s)

not only do we not solve the other cases but there is a decent chance that as we've mentioned about goals before we actually solve it in a way that makes something else more likely and and this is why so when i said if we are trying to win at the game that is killing everything basically whenever i've all this games and we're a sleeper in same thing we just either don't understand [2483](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2483.55s)

the phenomena or there or something else wrong then what we also realize is that trying to just keep addressing the symptoms without addressing what's causing them doesn't make any sense either and so then we have to say okay well at minimum i want to focus on one if i don't know how to solve the underlying [[generator function|generator functions]] i at least want to make sure that the actions [2505](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2505.45s)

that i'm doing to the degree that they are successful create precedence where if more people do them the system at least moves in the right direction it's a race to the top rather than the [[race to the bottom]] at least want to make sure i'm doing that meaning i'm creating dynamics where my ability to win is through better partnership rather than better rivalry [2523](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2523.87s)

rather than the example where i figure out that if i get the you know if i figure out how to factory it eyes farms i can produce more meat per dollar and have higher margins on my need moving towards ubiquitous factory farming that is just ruinous for everything that's been a [[multipolar trap]] [[race to the bottom]] the kind of free-range organic movement that is saying no people will [2547](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2547.21s)

actually pay more for the thing that is healthier and better for them that also happens to be better for the world and less ethically terrible let's do something where to the degree we succeeded this and other people follow the system dynamics are better that doesn't categorically solve for the issue of [[multipolar trap]] but it at least says within the kinds of systems we [2563](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2563.98s)

understand like capitalism we're going to move system dynamics in the right direction so one of the things people want to really focus on is is the source of differential value that they're able to provide within any project within capitalism something that to the degree that it's successful and other people try to replicate it create system dynamics that move in the right [2583](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2583.87s)

direction versus wrong direction absolutely one way that people can think about within the current systems how to be in the right direction some people will be focused on how do we actually solve the underlying [[generator function|generator functions]] and we spoke about the [[generator function]] from one perspective which is that rival risk dynamics cause harm and the exponential rivalry causing [2604](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2604.21s)

exponential harm self terminates yep there is another way of speaking about the [[generator function]] that is even more fundamental and abstract and i think it would be worth sharing that if because it speaks to what we have to change if your app let's yeah so then actually i was about to go there cuz i try we're essentially we're we're operating at the level of what i would call meta x risk [2624](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2624.52s)

the [[generator function|generator functions]] of x risk we pushed down for a bit there to the actual s x risks themselves and now let's go actually from one level of abstraction further up so tell me yeah what is the thing that is actually one level above these [[generator function|generator functions]] of x racing yeah so the one [[generator function]] we've talked about rival resetting image we got that [2641](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2641.65s)

i want to give one other example of a [[generator function]] and then we'll go to an even more abstract unifying framework when you mention hey let's say we solve [[climate change]] is that adequate and is it even an appropriate thing to do so so let's take an example where we're not focused on like increasing war through [[exponential tech]] weaponization or whatever but we'll focused on simply [2668](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2668.32s)

environmental collapse dynamics so when we look at environmental damage dynamics all the environmental damage that we look at can be thought of in terms of either depletion of under noble resources or accumulation of waste streams that aren't being processed in real time and so accumulation and depletion when we start to look at the example so i say okay accumulation right [2696](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2696.97s)

what we call toxicity whether i'm talking about nitrogen runoff causing [[dead zone|dead zones]] or the plastic in the ocean or mercury in the water or in the air or uranium or carbon dioxide causing carbonic acid in ocean acidification or carbon dioxide in the air or any of those right the the cfcs are hfcs that were the ozone hole problem those are all special case examples of some kind [2723](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2723.91s)

of accumulation of something that doesn't have a feedback loop that can process it in real time that has some externalize costs on the the antifragility of the ecosystem writ large if i try and focus on one of them and say hey i'm just going to sequester a bunch of co2 but i'm not stopping the nitrogen runoff or the you know any of those other ones the biodiversity loss [2745](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2745.48s)

or whatever then what we'll find as i move the curve a tiny bit but i haven't addressed the fact that there are many many different metrics that are all moving to collapse from the same underlying dynamic similarly on the other side of depletion whether them talking about cutting down the old-growth forests or overfishing the ocean or [[species extinction]] of biodiversity loss or peak nitrogen turkey for us or any of those things those are all examples simile of unremovable resource utilization and an [[exponential curve]] on how that is happening so then we say let's put those both together and we look at that in a natural ecosystem like a forest there is no unruh knowable use of resource and there is no [2788](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2788.47s)

accumulated waste every new thing is made from old things all the old things get turned into new things so you have comprehensive loop closure all the atoms are in closed loop atomic cycling so that the atoms from one thing when it dies become new things at the same speed that things are dying that or train ways the antifragility of nature is a function of a few things but primarily [2809](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2809.92s)

for this example a function of closed loop dynamics the fragility of the human bill world is largely a function of open loop dynamics so when we look at depletion on one side and accumulation on the other those are the two sides of a linear atomic economy or materials resource economy that we can say the underlying dynamic of linear flows rather than closed loop cycles or open [2840](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2840.91s)

loops and a network diagram is the way we define toxicity we're at large then we need to work on the [[generator function]] not of just this one specific accumulation or depletion issue but how do we have a process by which comprehensive loop closure is built into the choice making processes of all humans everywhere yep and as soon as we start thinking about it that way not [2863](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2863.91s)

just like how could we do it somewhere but how do we create a different type of behavioral dynamics from incentive structures to [[collective intelligence]] structures are basically the [[sensemaking]] and choice making dynamics that lead to all agents thinking about and working towards comprehensive loop closure that is now at a meta level to all of the specific instantiations so [2884](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2884.41s)

now we go one level deeper and we say we talked about rival risk dynamics multiplied by [[exponential tech]] self terminates we also talked about linear flows open loop flows on a finite planet also in the bleeding dynamic there is something that unifies both of those so we can flip them meta meta ex risk yep and we noticed that both of these involve specifically human [2911](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2911.03s)

and human choice empowered by technology in relationship with some kind of world or commons right and so if i look at rival risk dynamics if there wasn't [[exponential tech]] if we stayed with stone tools we wouldn't really have any risk of self induced extinction we would just keep killing each other but we wouldn't kill everybody we couldn't have a war that killed everybody and we wouldn't be [2937](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2937.73s)

able to cut down all the treats right and so specifically it requires not just tech but ever-growing tech right tech feedback curves and similarly if our tools were limited to a size where like say we have just fishing lines not mile-long drift nets we wouldn't fish all the fish out of the ocean but as soon as we've got mile-long drift nets and we've got d nines taking down [2964](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2964.04s)

forests and slash-and-burn whatever else then again we start to see these fragility dynamics get larger than the playing field can handle because you've got movements that are faster than the antifragility can process now we say why is that and here's the key insight for all the people that are thinking about how can we use the technology to make the world better and use technology in a [2983](https://www.youtube.com/watch?v=a6ejROOyHW0&t=2983.87s)

fundamentally evolutionary way and the fact that people might say well look nature's rival risks and so we take market dynamics as kind of an example of a social darwinism of looking at how nature works and applying it to ourselves and am i talking about something that sounds like it is not how nature works well kind of yes and no and but there's this is really really [3009](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3009.91s)

critical because this is one place where people think they're thinking through formal systems but they're the wrong formal systems in nature pre humans let's say pre homo habilis in the development of evolving technology or developing technology we see micro rivalry meaning this lion and this gazelle obviously in the moment of the chase [3040](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3040.32s)

have a rival risk relationship with each other the lion wants what the gazelle doesn't wanna be the lion doesn't want and they're kind of mutually exclusive so cool we we have a rival risk dynamic they're at a micro level and yet gazelles as a whole species and lions as a whole species depend upon each other and if they if either one died the other one would probably go extinct and so [3061](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3061.9s)

then we say how is it that in nature micro local rivalry leads to macro symbiosis and interdependence and is it the same in humans or not because obviously social darwinism and lazier capitalism says yes it is the same compete and the best happens and we innovate and those who produce the best goods and services and the best values and proliferate and have the most money [3083](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3083.86s)

because money is a concentrated form of choice making but they they did the good things with joy so they should have more choice name id that's the idea right and it is gibberish and it's important to understand really critically why it's gibberish as soon as we developed technology and i'm going to define what i mean by this very specifically in a moment we change the fundamental system [3109](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3109.6s)

dynamics and the same system dynamics do not and can not apply so let's go to a chimpanzee who obviously uses tools or a bird with a nest or a beaver or whatever right like they are tool users if i give a chimpanzee or if a chimpanzee just finds on its own a rock and it wants to cut some vine and it just starts rubbing the rock across the vine it experientially notices how good this [3134](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3134.37s)

particular rock is at cutting it has a eminent experience of that if it picks up another rock and they experience it cutting faster they will use the sharper rock if you give them a knife they will experience a cutting posture and they will use the knife but they have never invented knives and there's a reason is that our current standard narrative said this start with homo habilis this kind [3159](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3159.4s)

of tool evolving capacity rather than just tool using capacity so let's take that so it requires the city for abstraction which developed along with prefrontal cortex etc to be able to use this rock and fill the sharpness in this rock and fill sharpness this rock filler sharpness and understand something that is none of the rocks which is the abstract principle of [3183](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3183.22s)

sharpness what makes something sharp and how could i make something that is more sharp now i'm not focused on the instance of any of the rocks but a principle that they all have in common but that is none of them that abstraction capacity seems to have emerged with some earlier hominid and then have zenith tin homo sapiens and that allows us to say well i would try [3203](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3203.32s)

chipping it with a and to make an arrowhead and then i would make a knife and then i would make a laser right i can keep doubling down on the abstract principle of sharpness i get evolving tech and this is why beavers are kind of doing the same thing they were doing a long time ago with regard to tools it was part of an evolved dynamic and we are changing our entire environment one [3224](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3224.17s)

generation to the next so radically now why does this matter evolution and i know this is a bit of a long one but this is because this is actually the heart of the whole thing so thank you for indulging me evolution is a kind of creative process meaning it is a process by which new stuff comes to exist that didn't exist before and when we think about what the [3248](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3248.26s)

process of evolution is we have some kind of mutation and some kind of selection dynamics right so there is a distribution of things that are pretty similar but a little bit different and then the ones that are a little bit different in a slightly more adaptive way in the moment meaning can either survive or make better end up having a better chance of making it through [3270](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3270.88s)

whatever characteristics led to doing that get double down on etc so what we find is that that process happens extraordinarily slowly not by design it's an unconscious process right there's an unconscious process of mutation and an unconscious process of selection nobody is saying this one seems like a more truly and beautiful approach let's go with it this seems [3295](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3295.72s)

like a good future into perpetuity it's just these guys actually got food better or made it better and that's what gets elected for which can include selecting for things that move in the direction of increasing violence dominance capacity whatever but at least in nature one of the things that happens is that the rival risk dynamics where power is used in a [3314](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3314.71s)

rivalry sway in a micro context there is a coupling of there's a symmetrical coupling of the rates of power change so as a lion as lions are evolving to get to be better hunters over the course of hundreds or thousands of generations gazelles are also evolving to get faster because the gazelles that make it through are better getting away from the lions and the lines that make it better [3338](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3338.98s)

getting the zealots right into there and their reproduction times are similar enough and they their system dynamics are embedded in or related so you don't get a symmetries of power you actually have a radical binding of power symmetry from lion - lion and from gazelle to gazelle and from lion to gazelle and from gazelle to plant and the whole thing right now this is key this is [3360](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3360.4s)

actually the essential criteria that makes micro rivalry lead to macro symbiosis is the symmetrical power binding imagine for a moment that i could make a mutation where lions got a hundred times better at killing in one generation well in that one generation they would kill all the gazelles they'd eat them all and then they would go extinct and so what we would find is [3385](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3385.96s)

that those species that were so much more effective than their environment could actually navigate in that crate resilience - would self terminate because of their own effectiveness in dependent upon of the ecological niche they depend upon they would be debasing the substrate upon which they depend through their over over expression and we see this virus happens virus comes [3410](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3410.35s)

about it is so lethal that it kills the host before it gets to i'm actually propagate and then less virulent versions of that virus get to propagate more and nature actions of selecting for a virus that's moving more in the direction of symbiosis than just lethality right yes yeah and so evolution itself has this symmetrical power binding why this is so [3431](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3431.77s)

critical to understand is because nature does not select for individuals of a species and it doesn't even select for species it seems to over the very short term but as you zoom out over the long term it selects for self-stabilizing ecological niches where the particulars members of a species that seem like they're getting selected for over the short term that are super [3454](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3454.94s)

lethal over the long term self-terminate and so the things that make it through long term are these radically anti fragile symbiotic systems and so we said so okay we get that in nature and we we look at the most badass [[apex predator|apex predators]] in nature whether we're looking at a lion or a shark or whatever and we look at a lion killing a gazelle compared to what a human [[factory farm]] of cows looks like right and we look at a shark eating a tuna compared to a mile long drift more if a mile long like if great whites got mile-long drift nets the whole ocean would have not existed a long time ago and so as soon as we developed technology and the ability to double down on technology we broke the power symmetry and we have to think about two applications humans with [3509](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3509.66s)

regard to other humans and humans with regard to the rest of nature that they depend upon so humans with regard to other humans the most powerful lion is not that much most powerful than then more powerful than the next most powerful lion and it's not even that much more powerful than the average lion or even one of the weaker lions if you think about it like the most powerful [3530](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3530.99s)

lion is maybe 1.1 or 1.2 x more powerful than that the upper incoming lion who will overtake him soon now look at human power distributions and say how much more destructive power does a putin or a trump have than you do a lot more dangerous crazy yeah like the ability to destroy the whole planet versus the ability to like maybe hit somebody right and maybe you can get a gun right and [3561](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3561.37s)

you look at here okay i don't know is that nine is that there's a magnitude it's a lot okay well that's a huge power asymmetry now humans with regard to nature we can cut down the forest much much much faster than forces forest can replenish we can use up resources faster than they can replenish we can create toxicity first link and replenish etc as a result [3586](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3586.49s)

that's why the ohm den loop dynamics matter so much is because nature cannot actually process the speed of transitions that we are making right and because we are increasing our predatory capacity faster than the environment can evolve because the technology is a second creative process what we would call design is not design is a consciously mediated abstraction [3613](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3613.42s)

oriented process evolution is an unconsciously mediated non abstract eminent kind of process evolution leads to radical interdependent complexities technology allows the selection over the short term of something that has independent of its relationship with the rest of the whole and so if we as soon as we get that those are mathematically fundamentally different design gives us [3643](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3643.97s)

complicated systems that are all fragile you notice everything that humans build the house that i'm in can't repair itself the way that a fight that have forced what if a fire took in the computer i'm on doesn't repair itself knowing my body does so design gives us a finite number of parameters that we can then externally build something for right complicated systems but very very [3663](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3663.38s)

powerful complexity antifragility happens by self-organization we don't really know how to do that but we do know how to debase it we know how to have the complicated debase the complex substrate upon which it depends that's how we get increasing fragility as we have collapse so why this matters is technology itself is a fundamentally new creative process in the history of local [3688](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3688.19s)

universe it breaks the power symmetry and as a result it makes micro rivalry turn into macro rivalry rather than micro rivalry turn into macro symbiosis as a result rather than get increasing antifragility we get increasing fragility towards inevitable collapse dynamics and so what that means is the relationship between the creative process called technology creating [3715](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3715.66s)

complicated systems and the relationship between the evolutionary creative process creating [[complex system|complex systems]] the relationship between those is the thing that we actually have to change and modulate because we still have rival risk basis for choice humans choosing against each other in a reverse way that came from evolution but now extended through design technology complicated [3739](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3739.599s)

systems the our understanding of causal principles optimization in a way where the rival risk dynamics are no longer checked in a way that creates any balance so what we actually need is a new creative process a new process by which new stuff comes into existence that is neither evolution nor design it is a third thing or not designing parts to we're not designing individual pieces [3767](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3767.009s)

of technology that empower rival risk choice-making basis we are and we're not waiting for anti rivalry at a macro level through [[evolutionary process]] we are actually designing self-stabilizing complex ecological niches and designing in the relationship between the complicated and the complex to be itself metastable that what we would call evolution by design or you know [3795](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3795.67s)

sometimes referred to as transcendental design is a actual different mathematical process of new stuff coming into being than either evolution or designers and we can see that at the heart of it design is all about the understanding of causation we can understand the law we can understand how a causal dynamic works you can make a tool that allows us to manipulate that [3820](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3820.15s)

law right so science is the understanding of causation and then technology is the application of it to have more effective causation to be able to cause stuff at larger scale but our basis for choice science and give us a good basis for choice because its first person in science studies third person which is why it said we say what is not what on our [3838](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3838.78s)

basis for choice is still coming from social darwinism so coming from a rivalries evolutionary basis those two together is an unstable system heading towards collapse we actually need it in a theory of choice beyond games theory beyond social darwinism that can be an adequate container for our causal dynamics and to be able to actually hold the relationship between choice and causation appropriate to have metastability that is if you want to talk about the macro level of the [[generator function|generator functions]] of x risk it is the relationship between choice and causation within the field of change and the necessity for a theory of and systems of right choice making at the level of individuals and collectives that can steward the level of causal [3883](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3883.329s)

power that we have through technology in ways that will create systems that are actually anti fragile and meta state mmm it thank you for the conclusion there and let's just for the listeners and this is gonna be i'm just gonna wrap up and then we'll kind of transition to a full wrap-up mode but as you said there first we have we've already been talking about - did the [[generator function|generator functions]] of s extra store one of them as the and rival risk games with distributed exponential technology with interconnectedness leads to self-determination and you can think about the loop closures as another part of this where you have from a stock and flow perspective you have depletion or accumulation of various things and when those go to zero or in those two go to [3923](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3923.65s)

infinity that's also really bad for us so those two things our [[generator function|generator functions]] about extras but in fact as you're saying here there's one kind of one meta level above that which is to say that both of those things are the result of this new kind of process new technological process and that process is different than the [[evolutionary process]] and [[evolutionary process]] as you said is really good at taking if you think from a multi scale perspective it's really good at taking the micro rivalry and trends you know transforming it into macro you know symbiosis or anti fragility but technology does not do that technology does not have that micro to macro kind of thing and so we need to do is kind of think about the relationship between these two processes [3961](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3961.69s)

we've created a new process aka technology and we've had this whole process aka evolution we have to think what is the relationship between those things and how can we make a third kind of process that as you say is something kind of a combination of them that allows us to do the self designed bottom-up anti fragile creation [3977](https://www.youtube.com/watch?v=a6ejROOyHW0&t=3977.97s)

so with that daniel i know we're out of time here um and just for our listeners too as a quick note a you can go check out some of daniels work at civilization emerging calm and i'd recommend that because lots of the ideas here are there the other thing to note is that likely this will not be the last podcast between daniel and i because you know he left it at a very juicy edge there which [4002](https://www.youtube.com/watch?v=a6ejROOyHW0&t=4002.46s)

is what is the life we have science as their theory of kind of truth or whatever the world what is our theory of ethics in the world was there processed for that we need to dive into that we need to dive into how this relates to our current information revolution how this relates to black swans and the precautionary principle in progress and also what you as a person can do about [4023](https://www.youtube.com/watch?v=a6ejROOyHW0&t=4023.49s)

all this uh-huh so well there'll be another one likely but dana was there anything else that you would like to say to our listeners either a place that they can find you on the interwebs or or in the last thought that was a very concise and valuable wrap-up i appreciate it and yeah i'm very happy to come back and take the next step because as you said we shared some frameworks [4046](https://www.youtube.com/watch?v=a6ejROOyHW0&t=4046.58s)

without actually sharing the rest of the frameworks necessary to understand it or how to act with those frameworks so i'm happy to come back and do that you already shared my blog so these topics are interesting to people i look forward to hearing questions thoughts that come up