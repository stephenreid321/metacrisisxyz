<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.24" dur="5.519">hey guys so this is a bit of an unusual</text><text start="3.0" dur="5.099">one compared to my usual format because</text><text start="5.759" dur="3.601">this is a long form conversation that</text><text start="8.099" dur="3.361">was actually meant to be part of my</text><text start="9.36" dur="3.899">upcoming win-win podcast that&apos;s going to</text><text start="11.46" dur="5.399">be launching in a few weeks but the</text><text start="13.259" dur="5.461">topic of it is so urgent that Daniel who</text><text start="16.859" dur="6.241">I&apos;m talking to and I decided to release</text><text start="18.72" dur="7.5">this early you will have noticed how mad</text><text start="23.1" dur="4.86">the rate of AI progress is getting and I</text><text start="26.22" dur="3.66">say mad in like every sense of the word</text><text start="27.96" dur="3.78">it&apos;s incredibly exciting and there&apos;s so</text><text start="29.88" dur="3.24">much cool stuff coming out like just I</text><text start="31.74" dur="3.36">can&apos;t even keep up with it</text><text start="33.12" dur="4.88">but at the same time</text><text start="35.1" dur="6.0">it&apos;s it&apos;s also a little like</text><text start="38.0" dur="5.14">overwhelming to the extent that you know</text><text start="41.1" dur="3.54">we are creating technologies that we</text><text start="43.14" dur="3.02">don&apos;t even understand and unleashing</text><text start="44.64" dur="5.28">them and connecting them to the internet</text><text start="46.16" dur="5.68">and it feels like the potential risks</text><text start="49.92" dur="4.86">and harms of that are not being properly</text><text start="51.84" dur="5.28">internalized into the sort of General</text><text start="54.78" dur="4.619">calculus of all the different people</text><text start="57.12" dur="4.98">doing this so yeah in this conversation</text><text start="59.399" dur="4.381">I&apos;m talking to Daniel schmaxenberger who</text><text start="62.1" dur="3.48">is frankly just one of the smartest</text><text start="63.78" dur="3.78">people I&apos;ve ever met I if you&apos;ve been</text><text start="65.58" dur="4.2">following my more recent content around</text><text start="67.56" dur="3.66">this topic of mollock and Game Theory</text><text start="69.78" dur="4.32">and when is competition healthy and</text><text start="71.22" dur="4.079">unhealthy his thinking has inspired a</text><text start="74.1" dur="2.64">lot of this content so it&apos;s a real</text><text start="75.299" dur="4.981">pleasure to be finally talking to him</text><text start="76.74" dur="6.0">and specifically in this conversation we</text><text start="80.28" dur="3.839">get into I think a sort of blend of</text><text start="82.74" dur="3.239">topics that haven&apos;t really been</text><text start="84.119" dur="4.14">discussed in this way before we talk</text><text start="85.979" dur="4.68">about the nature of Game Theory and</text><text start="88.259" dur="4.381">mollic and how it interplays with our</text><text start="90.659" dur="4.5">wider sort of capitalistic economic</text><text start="92.64" dur="6.479">system and how that also then interplays</text><text start="95.159" dur="5.941">in the development of AI so if any of</text><text start="99.119" dur="3.54">those topics interest you and they</text><text start="101.1" dur="3.839">should because if you live on this</text><text start="102.659" dur="3.841">planet this will affect you for good or</text><text start="104.939" dur="4.14">for worse then you should take the time</text><text start="106.5" dur="4.799">to watch this conversation in full</text><text start="109.079" dur="3.781">um it&apos;s one of the most fascinating</text><text start="111.299" dur="3.78">chats I&apos;ve ever had it&apos;s also one of the</text><text start="112.86" dur="4.399">most terrifying so let me know what you</text><text start="115.079" dur="2.18">think</text><text start="118.92" dur="4.199">yeah so I am really happy for us to be</text><text start="121.32" dur="3.06">having this conversation today you and I</text><text start="123.119" dur="3.241">have</text><text start="124.38" dur="5.76">talked about</text><text start="126.36" dur="5.7">moloch and the relationship of the uh</text><text start="130.14" dur="3.599">kind of moloch metaphor to the overall</text><text start="132.06" dur="3.539">state of the world and what I sometimes</text><text start="133.739" dur="3.36">call The Meta crisis for a couple years</text><text start="135.599" dur="4.561">now and you&apos;ve put out these</text><text start="137.099" dur="5.581">exceptionally good educational videos on</text><text start="140.16" dur="4.92">moloch in uh expressing itself in</text><text start="142.68" dur="5.639">different environments in social media</text><text start="145.08" dur="5.28">and General Media uh I hope everybody</text><text start="148.319" dur="2.941">has watched those</text><text start="150.36" dur="3.3">um</text><text start="151.26" dur="5.76">we&apos;re in the moment right now where</text><text start="153.66" dur="4.799">there is this uh rapid race on the</text><text start="157.02" dur="3.18">development of artificial intelligence</text><text start="158.459" dur="4.621">Technologies development and public</text><text start="160.2" dur="4.86">deployment of them uh this we&apos;re</text><text start="163.08" dur="4.86">recording this shortly after gpt4 has</text><text start="165.06" dur="5.22">been publicly released and uh then after</text><text start="167.94" dur="4.26">so many of the other companies that have</text><text start="170.28" dur="4.039">ai capability have also had to release</text><text start="172.2" dur="4.44">their large language models in response</text><text start="174.319" dur="5.081">and so what I&apos;m actually really wanting</text><text start="176.64" dur="6.48">to talk about is AI risk and</text><text start="179.4" dur="7.26">a way of thinking about the totality of</text><text start="183.12" dur="4.52">the AI risk landscape that is</text><text start="186.66" dur="4.98">um</text><text start="187.64" dur="5.379">for me uh clarifying and a little</text><text start="191.64" dur="3.12">different than the way iris is usually</text><text start="193.019" dur="3.181">talked about and maybe unifying across</text><text start="194.76" dur="2.88">different categories of risk and they</text><text start="196.2" dur="3.539">give us some insight into how to think</text><text start="197.64" dur="3.3">about what protecting against it might</text><text start="199.739" dur="5.101">require</text><text start="200.94" dur="6.96">and the moloch</text><text start="204.84" dur="5.759">frame I find</text><text start="207.9" dur="5.58">gives incredibly valuable Insight in</text><text start="210.599" dur="4.381">thinking about the AI risk frame and the</text><text start="213.48" dur="4.38">AGI</text><text start="214.98" dur="4.32">uh misalignment issue is very helpful in</text><text start="217.86" dur="3.5">thinking about the moloch issue I think</text><text start="219.3" dur="5.46">the the two metaphors clarify each other</text><text start="221.36" dur="5.739">and then I think the actual moloch type</text><text start="224.76" dur="4.699">Dynamics give rise to the AI risk</text><text start="227.099" dur="4.621">scenarios that I am most concerned about</text><text start="229.459" dur="3.401">and so that&apos;s what I&apos;m wanting to talk</text><text start="231.72" dur="2.579">about which is why I was particularly</text><text start="232.86" dur="4.379">interested in you and I having this</text><text start="234.299" dur="4.561">conversation since you&apos;re holding the</text><text start="237.239" dur="5.941">mantle of helping the world understand</text><text start="238.86" dur="7.68">the moloch Dynamics and uh so with that</text><text start="243.18" dur="4.86">I would love if you would share</text><text start="246.54" dur="2.759">what the moloch thing is about for</text><text start="248.04" dur="3.44">people who don&apos;t already know I know</text><text start="249.299" dur="5.401">most of your listeners will already know</text><text start="251.48" dur="4.96">but uh why are we referring to it that</text><text start="254.7" dur="4.86">way what is the phenomena why is it</text><text start="256.44" dur="5.699">interesting so so probably the most</text><text start="259.56" dur="5.52">concise definition I can give is</text><text start="262.139" dur="6.12">that it&apos;s the god of negative some games</text><text start="265.08" dur="5.94">like unhealthy competitive situations so</text><text start="268.259" dur="4.201">by that I mean like a system of bad</text><text start="271.02" dur="5.22">incentives</text><text start="272.46" dur="5.34">that incentivize agents within that</text><text start="276.24" dur="3.959">system you know players within that game</text><text start="277.8" dur="5.76">to sacrifice more and more of their</text><text start="280.199" dur="5.22">other values in order to win</text><text start="283.56" dur="3.48">the narrow you know win within that</text><text start="285.419" dur="2.461">narrow domain in other words win the</text><text start="287.04" dur="3.719">game</text><text start="287.88" dur="4.379">and by doing this sort of sacrifice of</text><text start="290.759" dur="4.761">of all these other values they&apos;re</text><text start="292.259" dur="7.321">essentially taking selfish actions that</text><text start="295.52" dur="5.86">externalize harms to everybody else</text><text start="299.58" dur="3.72">um both within that game and also you</text><text start="301.38" dur="4.2">know even people outside of it you know</text><text start="303.3" dur="4.679">to the wider system as a whole and hence</text><text start="305.58" dur="5.22">making the game a negative something</text><text start="307.979" dur="7.5">Okay so</text><text start="310.8" dur="7.02">sometimes when uh trying to describe the</text><text start="315.479" dur="4.261">generality of the instances aren&apos;t</text><text start="317.82" dur="4.8">already clear it&apos;s it&apos;s hard for people</text><text start="319.74" dur="5.519">so could you give a couple examples of</text><text start="322.62" dur="4.62">what that looks like and why thinking</text><text start="325.259" dur="6.421">about it as uh</text><text start="327.24" dur="6.36">a God is at all an interesting frame</text><text start="331.68" dur="4.799">um like why is that even the frame</text><text start="333.6" dur="6.12">that&apos;s Rising yeah so an example I gave</text><text start="336.479" dur="6.961">in my first molok video uh the beauty</text><text start="339.72" dur="5.64">Wars is about these Beauty filters that</text><text start="343.44" dur="4.86">have now become completely commonplace</text><text start="345.36" dur="4.26">on Instagram and in fact on most social</text><text start="348.3" dur="2.94">media platforms</text><text start="349.62" dur="3.48">and</text><text start="351.24" dur="5.399">the reason why these things are sort of</text><text start="353.1" dur="5.76">so particularly monarchy is that</text><text start="356.639" dur="4.321">every everyone who&apos;s trying to sort of</text><text start="358.86" dur="3.66">play the the beauty influencer game or</text><text start="360.96" dur="3.84">any kind of influencer game frankly on</text><text start="362.52" dur="3.959">these platforms you get directly</text><text start="364.8" dur="3.239">rewarded with more likes and follows if</text><text start="366.479" dur="2.94">your pictures look better if your face</text><text start="368.039" dur="3.061">looks better you know your complexion&apos;s</text><text start="369.419" dur="3.361">clearer and these filters started</text><text start="371.1" dur="2.879">appearing where Not only would they make</text><text start="372.78" dur="3.479">your Facebook smoother but they were</text><text start="373.979" dur="4.021">just like tweak your features in really</text><text start="376.259" dur="3.121">really subtle ways</text><text start="378.0" dur="4.259">um you know just like make your eyes a</text><text start="379.38" dur="4.8">little bit bigger and and and</text><text start="382.259" dur="4.321">um make your face basically converge</text><text start="384.18" dur="4.2">upon this whatever these like what seem</text><text start="386.58" dur="3.42">like normalized beauty standards but do</text><text start="388.38" dur="3.48">successfully seem to hijack people&apos;s</text><text start="390.0" dur="3.96">brains people do like these and like</text><text start="391.86" dur="3.119">just you know to show you how like how</text><text start="393.96" dur="4.019">full these things are like I would</text><text start="394.979" dur="4.741">upload a picture of myself that I loved</text><text start="397.979" dur="3.241">and then I would apply the filter to it</text><text start="399.72" dur="3.96">and then I would compare the two side by</text><text start="401.22" dur="4.08">side and I would now no longer like the</text><text start="403.68" dur="3.18">original picture and therefore it means</text><text start="405.3" dur="3.0">you know it makes you hate your natural</text><text start="406.86" dur="4.619">face</text><text start="408.3" dur="6.119">and yet despite knowing this</text><text start="411.479" dur="6.481">because you know you know you&apos;d get the</text><text start="414.419" dur="5.22">direct reward uh of of getting more</text><text start="417.96" dur="3.9">likes and follows by using these things</text><text start="419.639" dur="3.961">and then on top of that you know that</text><text start="421.86" dur="3.0">everyone else is using them as well and</text><text start="423.6" dur="2.219">so if you don&apos;t use them then you&apos;re</text><text start="424.86" dur="2.399">essentially going to get like left</text><text start="425.819" dur="3.72">behind the curve you&apos;re no longer going</text><text start="427.259" dur="4.741">to be competitive in in the influencing</text><text start="429.539" dur="5.1">game it&apos;s a really classic example of</text><text start="432.0" dur="3.96">monarchy bad incentives and driving a</text><text start="434.639" dur="3.06">kind of like race to the bottom where</text><text start="435.96" dur="3.12">everyone ends up miserable no one wants</text><text start="437.699" dur="3.12">to be using these things but feels like</text><text start="439.08" dur="4.619">they have no choice if they want to stay</text><text start="440.819" dur="4.38">competitive another example would be</text><text start="443.699" dur="5.22">climate change pollution you know</text><text start="445.199" dur="6.72">pollution from countries that are trying</text><text start="448.919" dur="4.801">to grow their GDP you know essentially</text><text start="451.919" dur="4.461">not get left behind their competitors</text><text start="453.72" dur="7.86">get left behind other countries and</text><text start="456.38" dur="7.48">externalizing the the costs of their GDP</text><text start="461.58" dur="4.98">growth to the Atmosphere by polluting</text><text start="463.86" dur="4.38">with CO2 so essentially a tragedy of the</text><text start="466.56" dur="5.4">commons type situation</text><text start="468.24" dur="7.14">and then a third example is the the</text><text start="471.96" dur="6.299">Classic arms race a country notices the</text><text start="475.38" dur="6.48">competitor is developing</text><text start="478.259" dur="5.641">some new type of hypersonic missile</text><text start="481.86" dur="4.92">um or autonomous weapons and so on and</text><text start="483.9" dur="4.44">even if they don&apos;t want to ex spend a</text><text start="486.78" dur="3.0">bunch of their GDP on these very</text><text start="488.34" dur="3.18">expensive new types of weapons they feel</text><text start="489.78" dur="2.46">like they have no choice because if they</text><text start="491.52" dur="3.48">don&apos;t then they&apos;re going to be</text><text start="492.24" dur="4.26">vulnerable to their enemies great so</text><text start="495.0" dur="3.96">first it&apos;s worth noting that each of</text><text start="496.5" dur="4.56">these are already abstractions across a</text><text start="498.96" dur="4.139">lot of cases right when we describe an</text><text start="501.06" dur="4.02">arms race</text><text start="503.099" dur="4.681">whether it&apos;s these two countries or</text><text start="505.08" dur="4.98">these two countries and whether it&apos;s on</text><text start="507.78" dur="3.72">Hypersonic missiles or AI weapons or</text><text start="510.06" dur="3.779">bioweapons those are each different</text><text start="511.5" dur="5.159">instances so the generalization across</text><text start="513.839" dur="4.5">the class of if anyone is developing</text><text start="516.659" dur="3.44">better weapons Technologies everybody</text><text start="518.339" dur="4.38">else has to develop</text><text start="520.099" dur="4.3">correspondingly the counters to those</text><text start="522.719" dur="3.12">weapons and the same type of weapons or</text><text start="524.399" dur="3.601">they kind of lose by default because</text><text start="525.839" dur="3.601">it&apos;s a situation where anyone does</text><text start="528.0" dur="3.56">something that increases their own</text><text start="529.44" dur="4.32">Security in a certain way that also</text><text start="531.56" dur="3.52">inexorably decreases the security of</text><text start="533.76" dur="2.88">others unless they do some counter</text><text start="535.08" dur="4.02">response</text><text start="536.64" dur="4.92">um so there&apos;s lots of different examples</text><text start="539.1" dur="3.78">of an arms race but arms races as a</text><text start="541.56" dur="3.36">whole is already a big generalization</text><text start="542.88" dur="3.12">tragedy of the commons is the same right</text><text start="544.92" dur="1.979">because we can be looking at the</text><text start="546.0" dur="4.98">situation where we&apos;re talking about</text><text start="546.899" dur="6.721">overfishing or wailing or deforestation</text><text start="550.98" dur="4.799">or desertification or CO2 and these are</text><text start="553.62" dur="5.18">all cases where</text><text start="555.779" dur="6.441">uh the overall Commons is being degraded</text><text start="558.8" dur="6.34">by every actor pursuing their own</text><text start="562.22" dur="4.42">near-term incentives right the actual</text><text start="565.14" dur="3.66">incentives laid out in the economic</text><text start="566.64" dur="4.02">landscape but to recognize that when we</text><text start="568.8" dur="3.36">look at every environmental issue facing</text><text start="570.66" dur="3.84">the world</text><text start="572.16" dur="3.78">no one is trying to extinct all the</text><text start="574.5" dur="3.36">species</text><text start="575.94" dur="5.1">nobody is trying to desertify the planet</text><text start="577.86" dur="6.18">nobody wants climate change or the</text><text start="581.04" dur="4.919">venusification of the planet and yet the</text><text start="584.04" dur="3.2">entire world is making it happen right</text><text start="585.959" dur="4.621">and that&apos;s</text><text start="587.24" dur="5.26">so when we look at features of the world</text><text start="590.58" dur="4.319">that nobody wants</text><text start="592.5" dur="4.38">and that are bad for everyone why can&apos;t</text><text start="594.899" dur="4.981">we change them this is where the moloch</text><text start="596.88" dur="5.16">frame comes in right and we can see that</text><text start="599.88" dur="4.079">in both the arms race everyone&apos;s like</text><text start="602.04" dur="3.239">look I don&apos;t want to necessarily live in</text><text start="603.959" dur="2.82">the world with the autonomous weapons or</text><text start="605.279" dur="3.12">the bioweapons but we have to because</text><text start="606.779" dur="3.06">they&apos;re going to and if we all make the</text><text start="608.399" dur="2.581">agreement that we&apos;re not going to how do</text><text start="609.839" dur="2.761">we know they&apos;re keeping the agreement</text><text start="610.98" dur="3.9">and they&apos;re not lying and defecting in</text><text start="612.6" dur="3.9">some underground military base so we</text><text start="614.88" dur="3.78">have to assume under partial information</text><text start="616.5" dur="3.6">that they are doing the thing because</text><text start="618.66" dur="3.0">the risk deaths would be too high if we</text><text start="620.1" dur="3.12">assume the other way so under partial</text><text start="621.66" dur="3.48">information we have to assume that worst</text><text start="623.22" dur="3.6">case you know do the same thing they&apos;re</text><text start="625.14" dur="3.6">assuming the same thing so because of</text><text start="626.82" dur="4.04">the inability for trust and coordination</text><text start="628.74" dur="5.7">we get this kind of race to the bottom</text><text start="630.86" dur="4.96">and uh the same is true in all these</text><text start="634.44" dur="3.6">various scenarios so we see a lot of</text><text start="635.82" dur="5.22">features of the world</text><text start="638.04" dur="4.859">that it seems like are comprehensively</text><text start="641.04" dur="3.419">bad for everyone trending in a much</text><text start="642.899" dur="4.081">worse Direction nobody can really do</text><text start="644.459" dur="5.041">anything about and nobody wants</text><text start="646.98" dur="4.56">and so these properties are kind of the</text><text start="649.5" dur="4.92">emergent properties of bad coordination</text><text start="651.54" dur="4.859">and so you have in other places describe</text><text start="654.42" dur="5.039">moloch as the god of coordination</text><text start="656.399" dur="5.821">failures or basically the principle of</text><text start="659.459" dur="4.681">coordination failures the reason to talk</text><text start="662.22" dur="4.02">about it as a you know a God or</text><text start="664.14" dur="3.48">something is to say like okay well since</text><text start="666.24" dur="4.08">no agent</text><text start="667.62" dur="4.92">is trying to make it this way</text><text start="670.32" dur="4.019">what is making it this way is there some</text><text start="672.54" dur="3.6">kind of emergent agency or some</text><text start="674.339" dur="3.961">underlying System Dynamics we can think</text><text start="676.14" dur="6.18">of it as underlying System Dynamics</text><text start="678.3" dur="5.64">and uh you know I think you and I and</text><text start="682.32" dur="4.56">many people in ours here both came</text><text start="683.94" dur="4.8">across this Frame from Scott Alexander&apos;s</text><text start="686.88" dur="3.019">meditations on moloch paper that</text><text start="688.74" dur="6.18">references</text><text start="689.899" dur="7.781">uh both this great poem on moloch and uh</text><text start="694.92" dur="4.62">you know a number of pieces in popular</text><text start="697.68" dur="3.719">culture</text><text start="699.54" dur="3.72">um and if people haven&apos;t read it</text><text start="701.399" dur="4.38">everybody should read Scott Alexander&apos;s</text><text start="703.26" dur="3.3">paper on meditations on moloch</text><text start="705.779" dur="2.761">um</text><text start="706.56" dur="4.62">because of what it&apos;s trying to get to is</text><text start="708.54" dur="4.88">if every environmental issue from dead</text><text start="711.18" dur="6.659">zones in the ocean to</text><text start="713.42" dur="6.4">uh Plastics and wasting plastic into all</text><text start="717.839" dur="4.141">of these issues nobody wants but also</text><text start="719.82" dur="3.86">nobody can stop because the cost of</text><text start="721.98" dur="4.02">someone stopping it</text><text start="723.68" dur="4.599">disadvantages them relative to everyone</text><text start="726.0" dur="4.14">else if everyone else is going to</text><text start="728.279" dur="3.601">continue to externalize that cost to the</text><text start="730.14" dur="5.16">commons rather than internalize it and</text><text start="731.88" dur="6.0">decrease their profit margins and so how</text><text start="735.3" dur="4.38">do we deal with that thing and if all of</text><text start="737.88" dur="3.24">the things that are moving us towards</text><text start="739.68" dur="3.24">increased likelihood for Global</text><text start="741.12" dur="3.24">catastrophic risk or at least many of</text><text start="742.92" dur="2.58">them have this in common this is an</text><text start="744.36" dur="4.44">underlying feature that we have to</text><text start="745.5" dur="4.8">really understand right and um so you</text><text start="748.8" dur="4.8">could call it the god of coordination</text><text start="750.3" dur="5.219">failures of the unhealthy kind of game</text><text start="753.6" dur="3.299">Dynamics not the ones that upregulate</text><text start="755.519" dur="3.06">every because yes an arms race</text><text start="756.899" dur="4.141">upregulates everyone&apos;s capacity in a</text><text start="758.579" dur="3.901">certain way but it is also of regulating</text><text start="761.04" dur="2.82">a capacity that everyone wishes we</text><text start="762.48" dur="2.94">didn&apos;t have that is only relevant</text><text start="763.86" dur="3.0">because everybody else has it right if</text><text start="765.42" dur="5.159">we could all just agree to decrease</text><text start="766.86" dur="6.84">military spending by a factor of 10 and</text><text start="770.579" dur="4.56">reinvest all of that in healthcare and</text><text start="773.7" dur="3.18">um infrastructure and everything the</text><text start="775.139" dur="3.481">world would be better by everyone&apos;s</text><text start="776.88" dur="3.959">standards so we&apos;re not saying that there</text><text start="778.62" dur="4.62">are no types of competition that lead to</text><text start="780.839" dur="4.141">positive some Dynamics but there are</text><text start="783.24" dur="3.48">these other ones</text><text start="784.98" dur="3.06">so</text><text start="786.72" dur="3.98">um</text><text start="788.04" dur="6.84">so it&apos;s very interesting like</text><text start="790.7" dur="5.98">moloch can be seen as uh a kind of way</text><text start="794.88" dur="3.48">of looking at generative dynamics that</text><text start="796.68" dur="4.74">lead to the overall state of global</text><text start="798.36" dur="4.86">catastrophic risk right that there are</text><text start="801.42" dur="3.0">other places where I&apos;ve talked about the</text><text start="803.22" dur="2.76">meta crisis and tried to give a</text><text start="804.42" dur="3.3">formalization of it we can link that</text><text start="805.98" dur="4.859">here so I won&apos;t do it at length but I&apos;ll</text><text start="807.72" dur="6.48">just very briefly say</text><text start="810.839" dur="6.421">the metacrisis thesis is that we are at</text><text start="814.2" dur="5.699">a unique time in history where there are</text><text start="817.26" dur="4.379">are an increasing number of global</text><text start="819.899" dur="3.301">catastrophic risks with increasing</text><text start="821.639" dur="3.541">probabilities</text><text start="823.2" dur="4.98">and that has never been the case like</text><text start="825.18" dur="5.04">this before where the attractor state of</text><text start="828.18" dur="4.98">increasing catastrophe</text><text start="830.22" dur="4.82">is the most likely attractor state of</text><text start="833.16" dur="4.34">the future across many different</text><text start="835.04" dur="5.799">dimensions of how that could play out</text><text start="837.5" dur="5.019">and the other attractor State and maybe</text><text start="840.839" dur="3.481">I&apos;ll I&apos;ll come back to that I&apos;ll explain</text><text start="842.519" dur="3.601">this one a little bit first</text><text start="844.32" dur="4.259">it is</text><text start="846.12" dur="4.44">um a catastrophic risk is not new</text><text start="848.579" dur="4.32">civilizations have faced war and have</text><text start="850.56" dur="4.5">faced famine and have faced uh plagues</text><text start="852.899" dur="3.841">and have faced self-induced</text><text start="855.06" dur="3.719">environmental ruin Easter Island in many</text><text start="856.74" dur="3.539">cases previously they were just local</text><text start="858.779" dur="3.0">they weren&apos;t global</text><text start="860.279" dur="3.481">and that was because the overall</text><text start="861.779" dur="4.021">civilizations were local we didn&apos;t have</text><text start="863.76" dur="4.199">fully globalized Supply chains where</text><text start="865.8" dur="4.38">everything depended upon six Continental</text><text start="867.959" dur="4.921">you know radical interdependent type</text><text start="870.18" dur="4.08">things and when we could destroy a local</text><text start="872.88" dur="3.3">environment we couldn&apos;t destroy the</text><text start="874.26" dur="4.019">biosphere writ large or oceans or</text><text start="876.18" dur="4.8">something so obviously it&apos;s our level of</text><text start="878.279" dur="5.341">technological capacity that allows us to</text><text start="880.98" dur="4.859">have a global civilization</text><text start="883.62" dur="3.54">that allows what happened to all</text><text start="885.839" dur="3.661">previous civilizations which was</text><text start="887.16" dur="3.9">civilizations did go through growth</text><text start="889.5" dur="4.019">curves where they had Peaks and then</text><text start="891.06" dur="4.8">they failed and they kind of all failed</text><text start="893.519" dur="4.401">right at least that&apos;s a overarching</text><text start="895.86" dur="5.4">architecture we see in</text><text start="897.92" dur="4.479">collapse of complex societies by taintor</text><text start="901.26" dur="4.439">and other books like that kind of</text><text start="902.399" dur="5.101">describe some of the Dynamics but we are</text><text start="905.699" dur="3.0">for the first time facing that in a</text><text start="907.5" dur="4.199">global way</text><text start="908.699" dur="4.5">so it is not unprecedented the thing</text><text start="911.699" dur="3.26">about civilizational collapse it&apos;s just</text><text start="913.199" dur="4.561">unprecedented to think about it globally</text><text start="914.959" dur="4.541">but obviously the Egyptian the Mayan the</text><text start="917.76" dur="5.22">Roman the all the previous Empires</text><text start="919.5" dur="6.3">failed for various reasons uh we didn&apos;t</text><text start="922.98" dur="4.14">actually have world ending Tech we</text><text start="925.8" dur="3.479">didn&apos;t have the capacity to ruin</text><text start="927.12" dur="3.899">everything rapidly until World War II in</text><text start="929.279" dur="4.321">the bomb the bomb allowed something</text><text start="931.019" dur="4.981">where a rapid escalation could destroy</text><text start="933.6" dur="5.22">kind of everything that was novel there</text><text start="936.0" dur="4.5">were hundreds of 200 000 years of homo</text><text start="938.82" dur="3.06">sapien history before that we couldn&apos;t</text><text start="940.5" dur="2.94">destroy everything quickly and then we</text><text start="941.88" dur="3.36">could so that was a bright line in this</text><text start="943.44" dur="3.959">hand and that was very recent</text><text start="945.24" dur="5.039">and</text><text start="947.399" dur="4.74">we couldn&apos;t ruin the entire</text><text start="950.279" dur="3.36">planetary we couldn&apos;t reach planetary</text><text start="952.139" dur="4.741">boundaries and mess up the biosphere</text><text start="953.639" dur="4.801">until industrial Tech but the industrial</text><text start="956.88" dur="3.18">Tech doesn&apos;t get there rapidly like the</text><text start="958.44" dur="3.54">nukes it takes a few hundred years of</text><text start="960.06" dur="3.839">its proliferation for cumulative effects</text><text start="961.98" dur="2.94">right but we went from half a billion</text><text start="963.899" dur="2.161">people</text><text start="964.92" dur="2.88">be</text><text start="966.06" dur="3.779">for the Industrial Revolution to 8</text><text start="967.8" dur="4.74">billion people we increase the resource</text><text start="969.839" dur="6.18">consumption per capita moving into the</text><text start="972.54" dur="5.039">industrial World by 100x plus and that&apos;s</text><text start="976.019" dur="3.18">utilizing resources from the earth</text><text start="977.579" dur="2.94">faster than they can be replaced turning</text><text start="979.199" dur="2.64">them into trash from pollution faster</text><text start="980.519" dur="3.481">than they can be processed running the</text><text start="981.839" dur="4.201">environment on both sides with an</text><text start="984.0" dur="3.36">exponential economic growth curve that</text><text start="986.04" dur="3.359">just to keep up with compounding</text><text start="987.36" dur="3.9">interest has to become exponential and</text><text start="989.399" dur="3.721">to not over inflate that currency has to</text><text start="991.26" dur="3.12">equate to more goods and services on a</text><text start="993.12" dur="3.54">linear materials economy you don&apos;t get</text><text start="994.38" dur="3.72">to do that thing forever so industrial</text><text start="996.66" dur="4.859">Tech</text><text start="998.1" dur="5.64">bound a linear materials economy turning</text><text start="1001.519" dur="5.221">the earth into trash and pollution</text><text start="1003.74" dur="4.98">through a commodity cycle faster than it</text><text start="1006.74" dur="5.339">can be replenished attached to an</text><text start="1008.72" dur="6.299">exponential curve of Finance</text><text start="1012.079" dur="5.101">um utilizing growing industrial Tech and</text><text start="1015.019" dur="3.601">globalization is what creates all the</text><text start="1017.18" dur="3.659">planetary boundaries we&apos;re facing of</text><text start="1018.62" dur="5.52">which climate change is one but species</text><text start="1020.839" dur="5.761">Extinction and biodiversity loss and</text><text start="1024.14" dur="4.08">um on and on and on the entire planetary</text><text start="1026.6" dur="3.3">boundaries framework as a result of that</text><text start="1028.22" dur="2.94">that is the result of tech without the</text><text start="1029.9" dur="4.679">industrial Tech we couldn&apos;t have done</text><text start="1031.16" dur="5.82">that right so cavemen can&apos;t mess up the</text><text start="1034.579" dur="5.161">entire planet right Stone Age tools even</text><text start="1036.98" dur="5.219">Bronze Age tools can&apos;t do that nor can</text><text start="1039.74" dur="5.939">they have a war that kills everything</text><text start="1042.199" dur="5.1">so then nor can they spread memes</text><text start="1045.679" dur="3.421">they&apos;re not informationally connected</text><text start="1047.299" dur="3.781">either</text><text start="1049.1" dur="4.86">um although their means spread much much</text><text start="1051.08" dur="6.0">slower and more locally right</text><text start="1053.96" dur="5.04">um and so that is also the result of the</text><text start="1057.08" dur="4.56">tech right you and I are talking</text><text start="1059.0" dur="4.919">via satellites right now right via</text><text start="1061.64" dur="4.62">literal outer space type communication</text><text start="1063.919" dur="3.901">for this thing to be able to happen on</text><text start="1066.26" dur="3.14">computers that were generated in six</text><text start="1067.82" dur="4.8">continent Supply chains</text><text start="1069.4" dur="5.74">and um that are more advanced in the</text><text start="1072.62" dur="5.76">things that ran the Manhattan Project</text><text start="1075.14" dur="6.36">um that are available to all of us and</text><text start="1078.38" dur="4.5">but so so the cumulative effects of</text><text start="1081.5" dur="3.059">industrial Tech bring us to planetary</text><text start="1082.88" dur="3.539">boundaries and kind of increasing</text><text start="1084.559" dur="3.601">fragility where there used to be a lot</text><text start="1086.419" dur="3.301">of people who lived on local subsistence</text><text start="1088.16" dur="2.82">not dependent on the total grid and</text><text start="1089.72" dur="2.699">there were a lot less total people now</text><text start="1090.98" dur="2.819">there&apos;s a lot more total people and</text><text start="1092.419" dur="3.0">they&apos;re almost all dependent on the grid</text><text start="1093.799" dur="3.721">not local subsistence or the fragility</text><text start="1095.419" dur="4.14">of those things is radically higher so</text><text start="1097.52" dur="3.36">you both get fragility of the planet and</text><text start="1099.559" dur="3.36">fragility of the human life support</text><text start="1100.88" dur="3.78">systems multiplied by a lot more people</text><text start="1102.919" dur="3.421">which can of course also escalate to</text><text start="1104.66" dur="5.58">violence when things start to break down</text><text start="1106.34" dur="5.82">things like that so uh then you get the</text><text start="1110.24" dur="3.36">bomb is the example the first fully</text><text start="1112.16" dur="2.94">existential Tech and for the first time</text><text start="1113.6" dur="3.66">in history we actually had to make an</text><text start="1115.1" dur="3.84">entire world system to not use our new</text><text start="1117.26" dur="3.06">tech whereas before that every time we</text><text start="1118.94" dur="2.64">had new tech there was always a race to</text><text start="1120.32" dur="2.88">deploy it as fast as we could for a</text><text start="1121.58" dur="4.5">strategic Advantage this is a situation</text><text start="1123.2" dur="5.4">where nobody can win everybody loses</text><text start="1126.08" dur="4.56">so mutually assured destruction and the</text><text start="1128.6" dur="4.8">entire post-world War II world of the</text><text start="1130.64" dur="4.56">Redwoods Financial system the UN Etc was</text><text start="1133.4" dur="2.76">all how do we make a world system that</text><text start="1135.2" dur="2.339">doesn&apos;t</text><text start="1136.16" dur="3.96">do that thing</text><text start="1137.539" dur="4.02">um right and it happens to be that that</text><text start="1140.12" dur="3.299">was successful which is why we haven&apos;t</text><text start="1141.559" dur="4.801">had a kinetic World War III since then</text><text start="1143.419" dur="6.301">we&apos;re closer than ever to that right now</text><text start="1146.36" dur="5.58">um with a proxy war between NATO and</text><text start="1149.72" dur="4.26">Russia being as close to not proxied as</text><text start="1151.94" dur="5.28">it is and other things in the Horizon</text><text start="1153.98" dur="6.0">but um nutrition destruction doesn&apos;t</text><text start="1157.22" dur="4.5">work when you have many many players</text><text start="1159.98" dur="3.9">that have catastrophe weapons and many</text><text start="1161.72" dur="4.319">types of catastrophe weapons which is</text><text start="1163.88" dur="4.74">the scenario we have now it&apos;s like a lot</text><text start="1166.039" dur="5.88">it&apos;s like a local it&apos;s a local minimum</text><text start="1168.62" dur="5.28">but there&apos;s like ton it&apos;s a very tiny uh</text><text start="1171.919" dur="3.12">minimum wage just a little nudge and it</text><text start="1173.9" dur="2.88">could fall off down the hill there&apos;s</text><text start="1175.039" dur="6.481">many routes for it to topple off down</text><text start="1176.78" dur="6.96">the hill yeah yeah and the</text><text start="1181.52" dur="4.5">the other thing is that a major part of</text><text start="1183.74" dur="3.66">the post-world War II solution was one</text><text start="1186.02" dur="2.88">of the major reasons for War as you</text><text start="1187.4" dur="4.62">mentioned before was</text><text start="1188.9" dur="4.92">competition over resources and if major</text><text start="1192.02" dur="2.94">nations want to be able to grow their</text><text start="1193.82" dur="3.12">economic quality of life for everybody</text><text start="1194.96" dur="3.54">they want more stuff to not have to</text><text start="1196.94" dur="3.239">invade each other to take their stuff</text><text start="1198.5" dur="3.84">how can everybody get more stuff</text><text start="1200.179" dur="3.841">simultaneously well</text><text start="1202.34" dur="2.6">we can create an exponential monetary</text><text start="1204.02" dur="4.8">system</text><text start="1204.94" dur="5.979">and globalization and free trade and</text><text start="1208.82" dur="3.66">much more industrialization and just</text><text start="1210.919" dur="2.521">take stuff from nature faster so that</text><text start="1212.48" dur="3.0">everybody can have more stuff</text><text start="1213.44" dur="3.239">exponentially right super positive some</text><text start="1215.48" dur="3.24">dynamic</text><text start="1216.679" dur="3.481">except you can&apos;t take stuff from nature</text><text start="1218.72" dur="2.52">forever and be able to keep doing that</text><text start="1220.16" dur="3.54">so you start hitting planetary</text><text start="1221.24" dur="6.12">boundaries and we&apos;re right at that point</text><text start="1223.7" dur="4.859">and then when our own inability to keep</text><text start="1227.36" dur="3.0">growing without taking other people&apos;s</text><text start="1228.559" dur="4.681">stuff comes now the conflict type</text><text start="1230.36" dur="4.26">Dynamic so apart to the the planetary</text><text start="1233.24" dur="3.84">boundaries we&apos;re facing are actually</text><text start="1234.62" dur="4.62">partially the nature the result of the</text><text start="1237.08" dur="4.5">solution to not World War III which is</text><text start="1239.24" dur="4.02">why also the degrothers have to factor</text><text start="1241.58" dur="3.18">that degrowth ends up driving World War</text><text start="1243.26" dur="3.9">III if you don&apos;t have other ways of</text><text start="1244.76" dur="5.7">tending to the fact that many people</text><text start="1247.16" dur="5.58">would not voluntarily choose austerity</text><text start="1250.46" dur="3.719">in the presence of less stuff they would</text><text start="1252.74" dur="4.08">choose war in the other if they thought</text><text start="1254.179" dur="4.081">they could win but then the sort of so</text><text start="1256.82" dur="3.96">then the flip side argument is like okay</text><text start="1258.26" dur="3.779">so yes to an extent technology has</text><text start="1260.78" dur="3.18">gotten us into this mess but it sounds</text><text start="1262.039" dur="3.601">like we should pile on more technology</text><text start="1263.96" dur="3.719">to expand those essentially those</text><text start="1265.64" dur="3.24">planetary boundaries</text><text start="1267.679" dur="2.941">um to be able to more efficiently</text><text start="1268.88" dur="3.179">extract resources and thus sort of keep</text><text start="1270.62" dur="2.76">the house of cards going</text><text start="1272.059" dur="4.321">no</text><text start="1273.38" dur="5.76">we can say for sure that</text><text start="1276.38" dur="5.94">Luddite Solutions don&apos;t work</text><text start="1279.14" dur="4.86">even if they would be better because</text><text start="1282.32" dur="3.599">unless because of multipolar traps</text><text start="1284.0" dur="3.66">because of moloch because the tech</text><text start="1285.919" dur="4.081">equals power and if somebody says hey</text><text start="1287.66" dur="4.259">this Tech is causing harm so we&apos;re not</text><text start="1290.0" dur="5.22">going to do it they also just lose in</text><text start="1291.919" dur="5.461">the short term to whoever does right and</text><text start="1295.22" dur="3.839">so okay we think AI weapons are bad so</text><text start="1297.38" dur="3.179">we&apos;re not going to build them we think</text><text start="1299.059" dur="3.6">okay great then you&apos;re going to be</text><text start="1300.559" dur="5.401">destroyed by whoever does so</text><text start="1302.659" dur="4.861">um so we unless you can get Universal</text><text start="1305.96" dur="4.44">agreement</text><text start="1307.52" dur="5.18">you can&apos;t just lose an arms race and</text><text start="1310.4" dur="4.98">this has been one of the challenges of</text><text start="1312.7" dur="4.839">whether it was China engaging with Tibet</text><text start="1315.38" dur="4.1">whether it was colonialists engaging</text><text start="1317.539" dur="4.681">with the Native Americans whether it was</text><text start="1319.48" dur="5.62">Genghis Khan or any of his guys engaging</text><text start="1322.22" dur="5.88">with more peaceful tribes that were</text><text start="1325.1" dur="3.9">smaller uh the peaceful tribes lose it</text><text start="1328.1" dur="2.88">were</text><text start="1329.0" dur="4.26">and they also lose at population games</text><text start="1330.98" dur="4.5">right like the ones that are going to</text><text start="1333.26" dur="5.279">unrenewably use the planet to grow their</text><text start="1335.48" dur="4.8">population faster so we are in this</text><text start="1338.539" dur="6.241">unique situation where the result of</text><text start="1340.28" dur="5.94">that is an exponentiation right it&apos;s the</text><text start="1344.78" dur="2.879">who has made it through or the people</text><text start="1346.22" dur="2.579">that both win at War and when at</text><text start="1347.659" dur="3.661">economic growth</text><text start="1348.799" dur="5.221">and so we have</text><text start="1351.32" dur="5.339">radically more Warfare like total</text><text start="1354.02" dur="4.5">warfare potential and that is radically</text><text start="1356.659" dur="3.961">more distributed and radically more</text><text start="1358.52" dur="4.32">externalities on the environment and all</text><text start="1360.62" dur="4.5">the fragilities associated and that</text><text start="1362.84" dur="3.959">situation ends up leading to</text><text start="1365.12" dur="3.84">catastrophic breakdown of everything so</text><text start="1366.799" dur="4.62">basically so far the answer has been</text><text start="1368.96" dur="4.199">when at the race or lose the race itself</text><text start="1371.419" dur="3.841">is self-terminating this is kind of the</text><text start="1373.159" dur="3.961">meta crisis hypothesis so we take the</text><text start="1375.26" dur="3.659">next step and not only did the</text><text start="1377.12" dur="3.78">post-world War II model</text><text start="1378.919" dur="4.561">in in doing the good thing of we didn&apos;t</text><text start="1380.9" dur="4.399">have nuclear war yet right we didn&apos;t</text><text start="1383.48" dur="4.62">have a kinetic war between superpowers</text><text start="1385.299" dur="3.941">it all it also increased total Global</text><text start="1388.1" dur="3.24">fragility</text><text start="1389.24" dur="4.98">increase the movement towards all the</text><text start="1391.34" dur="4.8">planetary boundaries and we proliferated</text><text start="1394.22" dur="4.02">a heap of other technologies that are</text><text start="1396.14" dur="3.96">truly catastrophic now that unlike nukes</text><text start="1398.24" dur="3.48">are not easy to control nukes are</text><text start="1400.1" dur="3.42">extremely hard to make uraniums not in</text><text start="1401.72" dur="3.78">many places it&apos;s hard to enrich you can</text><text start="1403.52" dur="4.26">see where it is with satellites because</text><text start="1405.5" dur="4.799">it&apos;s radioactive and so you can limit it</text><text start="1407.78" dur="5.1">and only have a G9 or whatever that</text><text start="1410.299" dur="5.281">actually has nuclear capabilities</text><text start="1412.88" dur="4.32">and limiting Iran and many countries</text><text start="1415.58" dur="3.599">from getting it has been a major part of</text><text start="1417.2" dur="4.5">the world order right</text><text start="1419.179" dur="3.541">um but when we&apos;re talking about cyber</text><text start="1421.7" dur="3.78">weapons</text><text start="1422.72" dur="5.64">or grown weapons or bioweapons or the</text><text start="1425.48" dur="5.579">types of attacks that AI makes possible</text><text start="1428.36" dur="4.26">or other types of exponential Tech these</text><text start="1431.059" dur="2.821">do not require</text><text start="1432.62" dur="3.0">uh</text><text start="1433.88" dur="4.14">something that has to be mined in one</text><text start="1435.62" dur="5.58">particular area in the same way these do</text><text start="1438.02" dur="4.86">not require Nation like top level nation</text><text start="1441.2" dur="3.12">state level capabilities once they&apos;re</text><text start="1442.88" dur="3.659">developed for any purpose they&apos;re pretty</text><text start="1444.32" dur="4.859">much more easily accessible</text><text start="1446.539" dur="4.101">and what that means is and we always</text><text start="1449.179" dur="4.081">talk about exponential tech</text><text start="1450.64" dur="4.06">democratizing power right decentralizing</text><text start="1453.26" dur="3.06">and democratizing and decentralizing</text><text start="1454.7" dur="3.24">democratized sounds nice in some ways</text><text start="1456.32" dur="4.14">when you don&apos;t like concentrations of</text><text start="1457.94" dur="4.56">power and the abuses they&apos;re in but the</text><text start="1460.46" dur="4.339">democratization of catastrophe weapons</text><text start="1462.5" dur="5.64">is has a downside</text><text start="1464.799" dur="5.681">and one is when we&apos;re talking about not</text><text start="1468.14" dur="4.74">just a few nation states but lots of</text><text start="1470.48" dur="3.9">nation states and non-state actors and</text><text start="1472.88" dur="3.24">people who you can&apos;t even tell whom</text><text start="1474.38" dur="3.419">having those capabilities you can&apos;t put</text><text start="1476.12" dur="4.38">nutrition destruction or Force Nash</text><text start="1477.799" dur="5.061">equilibriums in the same way</text><text start="1480.5" dur="4.559">um what it portends for kind of just</text><text start="1482.86" dur="3.699">disgruntled misanthropes of which there</text><text start="1485.059" dur="3.661">are more as the other issues are</text><text start="1486.559" dur="4.441">advancing in technological unemployment</text><text start="1488.72" dur="3.959">increases and people migrating because</text><text start="1491.0" dur="3.659">of climate change increases and all</text><text start="1492.679" dur="3.181">those types of things</text><text start="1494.659" dur="3.361">um</text><text start="1495.86" dur="3.96">and then some of the exponential texts</text><text start="1498.02" dur="3.24">just even cause the ability to cause</text><text start="1499.82" dur="4.8">pretty catastrophic stuff by accident</text><text start="1501.26" dur="4.68">right whether kovid was from a lab leak</text><text start="1504.62" dur="3.059">or not the idea that if you&apos;re doing</text><text start="1505.94" dur="3.78">gain of function research and synthetic</text><text start="1507.679" dur="4.261">synthetic bional lab that it can leak</text><text start="1509.72" dur="3.72">and as you&apos;re doing lots more of it that</text><text start="1511.94" dur="3.479">the probability of that increases like</text><text start="1513.44" dur="6.359">that&apos;s not even intentional that&apos;s</text><text start="1515.419" dur="7.861">and as easy as it is for a lab leak you</text><text start="1519.799" dur="5.281">know of that type it&apos;s way easier for AI</text><text start="1523.28" dur="3.96">leak because it&apos;s connected to the</text><text start="1525.08" dur="4.04">internet it&apos;s actually almost very hard</text><text start="1527.24" dur="4.86">not to have those types of things happen</text><text start="1529.12" dur="5.08">so what we&apos;re saying is that we&apos;re at a</text><text start="1532.1" dur="4.319">novel point in history that World War II</text><text start="1534.2" dur="4.2">is a novel Point first truly catastrophe</text><text start="1536.419" dur="4.081">weapon now we&apos;re at the point where we</text><text start="1538.4" dur="4.92">have multiple types of cadastory weapons</text><text start="1540.5" dur="4.98">many actors that have them no good Force</text><text start="1543.32" dur="6.18">Nash equilibrium planetary boundaries</text><text start="1545.48" dur="5.579">fragility and that we&apos;re not saying that</text><text start="1549.5" dur="3.179">lots of things aren&apos;t getting better of</text><text start="1551.059" dur="3.72">course all of the</text><text start="1552.679" dur="3.48">Pinker and Friends arguments about the</text><text start="1554.779" dur="3.541">things that are getting better are the</text><text start="1556.159" dur="3.601">point is that they&apos;re getting better at</text><text start="1558.32" dur="3.18">the cost of other things that are being</text><text start="1559.76" dur="3.48">made worse where externalities are being</text><text start="1561.5" dur="3.9">driven the things that are being made</text><text start="1563.24" dur="5.58">worse are getting very near criticality</text><text start="1565.4" dur="5.399">points and tipping points that change</text><text start="1568.82" dur="3.719">the game fundamentally so you have a</text><text start="1570.799" dur="2.941">world of increased catastrophic risk and</text><text start="1572.539" dur="3.961">of course you have Cascades between</text><text start="1573.74" dur="4.5">these because you can have well before</text><text start="1576.5" dur="3.36">climate change and whether it&apos;s just</text><text start="1578.24" dur="3.539">from CO2 or whether it&apos;s from the</text><text start="1579.86" dur="4.74">localized effects of deforestation or</text><text start="1581.779" dur="5.341">whatever we do have increasing extreme</text><text start="1584.6" dur="5.1">weather events so then you get human</text><text start="1587.12" dur="4.38">migration and do we see likely</text><text start="1589.7" dur="4.52">possibilities for much larger amounts of</text><text start="1591.5" dur="8.64">human migration in the near future yes</text><text start="1594.22" dur="7.54">and can that lead to Resource Wars which</text><text start="1600.14" dur="3.96">can lead to escalating Wars if they hit</text><text start="1601.76" dur="4.32">already tense geopolitical environments</text><text start="1604.1" dur="4.92">whether it&apos;s India Pakistan or whether</text><text start="1606.08" dur="4.86">it&apos;s you know so many issues like that</text><text start="1609.02" dur="3.0">um so we can see that whether we&apos;re</text><text start="1610.94" dur="3.06">talking about</text><text start="1612.02" dur="3.6">large-scale military Dynamics or</text><text start="1614.0" dur="3.96">breakdown of Supply chains in human</text><text start="1615.62" dur="4.38">systems or what exponential Tech can add</text><text start="1617.96" dur="4.38">they all actually kind of cascade into</text><text start="1620.0" dur="5.279">each other they have the capacity so</text><text start="1622.34" dur="5.04">there is some need to tend to</text><text start="1625.279" dur="3.721">and that what you do to make one of them</text><text start="1627.38" dur="3.299">better can often make another one worse</text><text start="1629.0" dur="3.6">right so people will propose hey we need</text><text start="1630.679" dur="4.261">to tax carbon heavily and properly price</text><text start="1632.6" dur="4.86">carbon where the the price of the tax</text><text start="1634.94" dur="4.2">allows us to sequester the CO2 but if</text><text start="1637.46" dur="4.26">everyone doesn&apos;t internationally to do</text><text start="1639.14" dur="4.98">it and say the US does or Europe does</text><text start="1641.72" dur="4.26">and China doesn&apos;t and that equals a</text><text start="1644.12" dur="4.08">radical change to GDP which gets</text><text start="1645.98" dur="5.46">reinvested in military plus overall</text><text start="1648.2" dur="4.74">geopolitical diplomacy then you&apos;re also</text><text start="1651.44" dur="4.979">changing the balance of power in the</text><text start="1652.94" dur="4.92">world as a resultant so this is one of</text><text start="1656.419" dur="2.701">these classic cases where the way you</text><text start="1657.86" dur="3.179">make one thing better can make other</text><text start="1659.12" dur="4.559">things worse so how do you kind of</text><text start="1661.039" dur="3.841">factor all that together so this is I I</text><text start="1663.679" dur="3.24">took longer than I wanted but that&apos;s</text><text start="1664.88" dur="5.34">roughly The Meta crisis thesis right and</text><text start="1666.919" dur="5.041">so moloch is one way of looking at one</text><text start="1670.22" dur="4.559">of the generative Dynamics it gives rise</text><text start="1671.96" dur="6.48">to this a comment that often arises on a</text><text start="1674.779" dur="5.76">lot of my videos I&apos;ve noticed is that</text><text start="1678.44" dur="4.08">people really</text><text start="1680.539" dur="5.941">you know they&apos;re sensing that there is</text><text start="1682.52" dur="5.399">some malevolent force that is you know</text><text start="1686.48" dur="3.179">essentially making the world you know</text><text start="1687.919" dur="4.201">making it hard for people to coordinate</text><text start="1689.659" dur="3.961">making it so that we seem to be trending</text><text start="1692.12" dur="4.02">more towards like</text><text start="1693.62" dur="4.919">greater militarization and a greater War</text><text start="1696.14" dur="3.24">you know risk of war and so on but they</text><text start="1698.539" dur="3.12">can&apos;t</text><text start="1699.38" dur="3.96">they they end up ascribing it to like</text><text start="1701.659" dur="3.241">you know like a q Anon type Theory or</text><text start="1703.34" dur="3.959">something like that you know it&apos;s like</text><text start="1704.9" dur="3.779">oh it&apos;s a shadowy cabal of Elites it&apos;s</text><text start="1707.299" dur="2.941">the elites who are driving this and so</text><text start="1708.679" dur="3.781">on it&apos;s like there&apos;s some truth to that</text><text start="1710.24" dur="3.299">and that like Elites have more power and</text><text start="1712.46" dur="2.64">therefore have a little bit more</text><text start="1713.539" dur="4.861">responsibility in driving a molecule</text><text start="1715.1" dur="5.1">process but there&apos;s no I wish there was</text><text start="1718.4" dur="2.82">a centralized cabal who were like</text><text start="1720.2" dur="2.76">drawing because then at least then we&apos;d</text><text start="1721.22" dur="3.36">have some some easier ways like okay</text><text start="1722.96" dur="3.24">we&apos;ve got we know who the enemy is and</text><text start="1724.58" dur="3.3">the enemy is physical and real and like</text><text start="1726.2" dur="3.12">therefore you could take it out but it&apos;s</text><text start="1727.88" dur="4.08">it&apos;s more distributed than that because</text><text start="1729.32" dur="4.92">it&apos;s it&apos;s this like nebulous collection</text><text start="1731.96" dur="3.78">of bad incentives that we happen to call</text><text start="1734.24" dur="3.0">Marlo because we need to give it a name</text><text start="1735.74" dur="3.12">we need to give it a face so that we can</text><text start="1737.24" dur="3.539">understand it</text><text start="1738.86" dur="4.74">um but yeah it&apos;s it&apos;s</text><text start="1740.779" dur="6.301">I I think it&apos;s an kind of an important</text><text start="1743.6" dur="4.8">point to to to to sort of hammer home to</text><text start="1747.08" dur="2.459">people because they they&apos;re looking for</text><text start="1748.4" dur="2.82">an enemy and they&apos;re looking for a</text><text start="1749.539" dur="4.02">scapegoat but all the while that they</text><text start="1751.22" dur="3.9">keep blaming it on like constantly just</text><text start="1753.559" dur="2.701">purely blaming it on the elites they&apos;re</text><text start="1755.12" dur="2.34">missing that&apos;s not going to solve the</text><text start="1756.26" dur="4.019">problem you can kill all the elites and</text><text start="1757.46" dur="4.98">molecules will be there yes so this is</text><text start="1760.279" dur="4.321">why looking at the moloch type Dynamics</text><text start="1762.44" dur="3.66">the coordination failures are very</text><text start="1764.6" dur="4.74">useful for understanding lots of</text><text start="1766.1" dur="5.04">features of the world is that</text><text start="1769.34" dur="4.02">um</text><text start="1771.14" dur="5.159">the in the various environmental issues</text><text start="1773.36" dur="5.4">the various Market type races that end</text><text start="1776.299" dur="4.141">up being racist to the bottom or that</text><text start="1778.76" dur="3.12">bring way more risk I&apos;ll give you</text><text start="1780.44" dur="3.06">another great example with the race to</text><text start="1781.88" dur="3.96">AI right now or the ones with social</text><text start="1783.5" dur="5.46">media that happened</text><text start="1785.84" dur="4.8">the there&apos;s a perverse incentive to</text><text start="1788.96" dur="3.12">focus more on the opportunity and less</text><text start="1790.64" dur="3.48">on the risk</text><text start="1792.08" dur="4.86">of any new technology even though any</text><text start="1794.12" dur="5.22">new technology will do both because</text><text start="1796.94" dur="3.599">if I say whoa there might be real risk</text><text start="1799.34" dur="2.699">in this this is very powerful people</text><text start="1800.539" dur="3.12">could use this for various purposes we</text><text start="1802.039" dur="3.781">want to do a real thorough deep risk</text><text start="1803.659" dur="4.201">analysis before releasing this thing and</text><text start="1805.82" dur="4.92">not release it wrongly we want to do</text><text start="1807.86" dur="5.28">some real safe to fail testing and</text><text start="1810.74" dur="5.4">someone else is like we do some</text><text start="1813.14" dur="4.56">box checking risk analysis and then talk</text><text start="1816.14" dur="3.18">about all the awesome upsides in Rush</text><text start="1817.7" dur="3.12">ahead they get first mover Advantage</text><text start="1819.32" dur="4.079">they get more</text><text start="1820.82" dur="5.339">um investment they get Metcalf law and</text><text start="1823.399" dur="5.9">winning the network Dynamics and so</text><text start="1826.159" dur="5.221">there is a perverse incentive against</text><text start="1829.299" dur="4.961">thoughtful consideration and</text><text start="1831.38" dur="5.58">precautionary principle and so we see</text><text start="1834.26" dur="5.76">that lead got put in gasoline</text><text start="1836.96" dur="5.699">for some really simple thing of</text><text start="1840.02" dur="5.039">engine knocking that knocked a billion</text><text start="1842.659" dur="4.981">points of IQ off the planet and 4x the</text><text start="1845.059" dur="4.801">aggressiveness of Everybody by literally</text><text start="1847.64" dur="5.039">atomizing lead that we had to pull out</text><text start="1849.86" dur="4.26">of deep oars and brain toxic to find the</text><text start="1852.679" dur="3.12">whole planet and took like 80 years</text><text start="1854.12" dur="3.539">before we finally outlawed the thing and</text><text start="1855.799" dur="3.061">the effects that that had on the entire</text><text start="1857.659" dur="2.4">population of choice they made are</text><text start="1858.86" dur="4.919">irreversible</text><text start="1860.059" dur="5.641">and the same with DDT and parathione and</text><text start="1863.779" dur="3.421">malathion and on and on where we or</text><text start="1865.7" dur="3.18">cigarettes where we don&apos;t regulate the</text><text start="1867.2" dur="3.3">thing until way after the harms have</text><text start="1868.88" dur="2.96">been so clear but as we&apos;re getting Tech</text><text start="1870.5" dur="4.679">that has</text><text start="1871.84" dur="5.38">and and Elon and many people have talked</text><text start="1875.179" dur="3.6">about this for a long time as we&apos;re when</text><text start="1877.22" dur="2.819">we&apos;re dealing with AI when we&apos;re dealing</text><text start="1878.779" dur="3.961">with synthetic bio when we&apos;re dealing</text><text start="1880.039" dur="4.441">with technology that has rapidly much</text><text start="1882.74" dur="3.6">more rapid and much more scaled and</text><text start="1884.48" dur="4.319">consequential and complex types of</text><text start="1886.34" dur="3.839">effects if you wait until it hits a</text><text start="1888.799" dur="3.6">certain point to try to regulate it&apos;s</text><text start="1890.179" dur="3.661">too late now you have radical</text><text start="1892.399" dur="3.241">irreversibility</text><text start="1893.84" dur="2.9">and um</text><text start="1895.64" dur="5.7">so</text><text start="1896.74" dur="7.179">the we saw on we saw Chad GPT get to 100</text><text start="1901.34" dur="4.26">million users in a fraction of the time</text><text start="1903.919" dur="3.781">that it took Tick Tock or Facebook or</text><text start="1905.6" dur="3.66">anyone previously and obviously it has a</text><text start="1907.7" dur="2.4">lot more total power and things that it</text><text start="1909.26" dur="4.86">can do</text><text start="1910.1" dur="6.36">and so uh we this isn&apos;t a situation</text><text start="1914.12" dur="4.38">where we want to have an anti-incentive</text><text start="1916.46" dur="3.48">against precautionary principle maximum</text><text start="1918.5" dur="3.24">incentive on race</text><text start="1919.94" dur="4.38">where The Regulators are inherently</text><text start="1921.74" dur="4.08">slower and more stumbly than the</text><text start="1924.32" dur="3.12">um ones incentivizing it right like</text><text start="1925.82" dur="3.18">there&apos;s that that is also part of the</text><text start="1927.44" dur="3.66">moloch dynamic</text><text start="1929.0" dur="4.5">which starts to bring us to the AI</text><text start="1931.1" dur="4.439">conversation</text><text start="1933.5" dur="4.14">did you want to talk about the relation</text><text start="1935.539" dur="5.221">between molec and capitalism before we</text><text start="1937.64" dur="3.84">get to that yeah</text><text start="1940.76" dur="4.1">um</text><text start="1941.48" dur="3.38">I&apos;ll start by saying</text><text start="1946.22" dur="5.16">the competition the you know cold war</text><text start="1948.679" dur="4.98">between the USSR and the USA</text><text start="1951.38" dur="4.26">was not</text><text start="1953.659" dur="3.721">which was being framed as two different</text><text start="1955.64" dur="3.96">political economies right communism and</text><text start="1957.38" dur="4.74">capitalism and competition was not</text><text start="1959.6" dur="4.5">capitalism right and the up</text><text start="1962.12" dur="4.98">things that were happening inside of the</text><text start="1964.1" dur="4.62">USSR were not capitalism so we&apos;re not</text><text start="1967.1" dur="3.48">talking like the critique that we&apos;re</text><text start="1968.72" dur="4.26">about to offer of capitalism is not</text><text start="1970.58" dur="4.8">saying some previous economic system or</text><text start="1972.98" dur="4.26">political economy was better and</text><text start="1975.38" dur="3.84">actually Morlock instantiated itself</text><text start="1977.24" dur="4.2">through those systems as well</text><text start="1979.22" dur="3.179">capitalism was more effective and it did</text><text start="1981.44" dur="3.18">get selected because it was more</text><text start="1982.399" dur="3.961">effective at both good things and</text><text start="1984.62" dur="3.96">up things right which is kind of what</text><text start="1986.36" dur="4.439">you mentioned it wins a war but it has</text><text start="1988.58" dur="4.079">to sacrifice important stuff to do so so</text><text start="1990.799" dur="4.921">the thing that</text><text start="1992.659" dur="5.281">that can be reductionist and when</text><text start="1995.72" dur="3.6">certain critical metrics but harm other</text><text start="1997.94" dur="3.0">stuff in the process where eventually</text><text start="1999.32" dur="3.54">the cumulative effects of those harms or</text><text start="2000.94" dur="3.479">either catastrophe or dystopia world</text><text start="2002.86" dur="3.419">that nobody really wants</text><text start="2004.419" dur="4.26">and those are the two primary attractors</text><text start="2006.279" dur="4.02">right now you have catastrophes and to</text><text start="2008.679" dur="3.6">prevent all the catastrophes to make</text><text start="2010.299" dur="3.421">sure that people can&apos;t build catastrophe</text><text start="2012.279" dur="3.301">weapons in their basement what type of</text><text start="2013.72" dur="3.24">surveillance is needed to make sure that</text><text start="2015.58" dur="3.719">you have enough controls on all the</text><text start="2016.96" dur="4.86">things if you really have the ability to</text><text start="2019.299" dur="4.26">control the entire</text><text start="2021.82" dur="3.12">landscape of things that could lead to</text><text start="2023.559" dur="2.941">catastrophic risks that are radically</text><text start="2024.94" dur="3.359">decentralized</text><text start="2026.5" dur="4.679">most of those Solutions look pretty</text><text start="2028.299" dur="4.5">dystopic and so we want a future a third</text><text start="2031.179" dur="4.141">attractor future that is neither</text><text start="2032.799" dur="4.681">catastrophes nor dystopias and without</text><text start="2035.32" dur="3.959">saying what it is first let&apos;s just say</text><text start="2037.48" dur="3.96">that it&apos;s not those things and we can</text><text start="2039.279" dur="4.02">almost all universally agree that we</text><text start="2041.44" dur="3.66">would prefer not to those things which</text><text start="2043.299" dur="3.6">means that we need something that has</text><text start="2045.1" dur="4.079">the power to be able to prevent</text><text start="2046.899" dur="3.96">catastrophic risk but also needs checks</text><text start="2049.179" dur="3.901">and balances on its own power right</text><text start="2050.859" dur="4.02">doesn&apos;t have unchecked power and and</text><text start="2053.08" dur="3.0">capturability or corruption or those</text><text start="2054.879" dur="3.24">types of dynamics that are then</text><text start="2056.08" dur="3.66">uncheckable</text><text start="2058.119" dur="2.701">it&apos;s beyond the scope of this video to</text><text start="2059.74" dur="2.76">talk about what that third attractor</text><text start="2060.82" dur="2.7">solution is</text><text start="2062.5" dur="4.139">um</text><text start="2063.52" dur="5.04">but uh so we have you know when we talk</text><text start="2066.639" dur="4.5">about capitalism as a kind of dominant</text><text start="2068.56" dur="4.079">global economic system and of course we</text><text start="2071.139" dur="3.601">don&apos;t have pure laissez-faire capitalism</text><text start="2072.639" dur="4.381">we have this kind of hybrid</text><text start="2074.74" dur="5.22">um political economy but roughly this</text><text start="2077.02" dur="4.379">thing it does not subsume all of moloch</text><text start="2079.96" dur="3.6">like we said moloch was operating under</text><text start="2081.399" dur="4.381">feudalism and under</text><text start="2083.56" dur="3.599">communism under other systems and in the</text><text start="2085.78" dur="3.66">competitions between them it is more</text><text start="2087.159" dur="6.18">fair to think of it as kind of</text><text start="2089.44" dur="6.54">um the god of Game Theory uh but as</text><text start="2093.339" dur="4.861">capitalism being such a powerful part of</text><text start="2095.98" dur="3.78">that stack we can think of it as a</text><text start="2098.2" dur="5.28">metaphor for a moment</text><text start="2099.76" dur="5.22">and say capitalism</text><text start="2103.48" dur="3.54">is</text><text start="2104.98" dur="4.56">well let&apos;s just start by the a couple</text><text start="2107.02" dur="4.38">key aspects of the incentive</text><text start="2109.54" dur="4.319">Dynamics</text><text start="2111.4" dur="3.98">vast majority of human history and</text><text start="2113.859" dur="4.441">tribal type</text><text start="2115.38" dur="4.42">Dynamics pre-agriculture all of our kind</text><text start="2118.3" dur="3.779">of genetic fitness in that environment</text><text start="2119.8" dur="4.92">we didn&apos;t have the ability to store a</text><text start="2122.079" dur="4.26">lot of surplus right that happened post</text><text start="2124.72" dur="4.28">to plow and Grain and storage</text><text start="2126.339" dur="5.821">Technologies and whatever</text><text start="2129.0" dur="4.96">uh in which case there were all these</text><text start="2132.16" dur="3.36">kind of sayings in various tribes the</text><text start="2133.96" dur="3.6">best place to store extra food is in</text><text start="2135.52" dur="4.559">your neighbor&apos;s belly because rots</text><text start="2137.56" dur="5.34">otherwise and you know Etc and tribe and</text><text start="2140.079" dur="4.441">are invested in so but as soon as we</text><text start="2142.9" dur="4.08">start getting to</text><text start="2144.52" dur="4.2">uh private property ownership and the</text><text start="2146.98" dur="3.119">ability for a lot of surplus where I</text><text start="2148.72" dur="3.119">could differentially make it through a</text><text start="2150.099" dur="3.0">famine better than somebody else I could</text><text start="2151.839" dur="2.52">have a higher quality of life than</text><text start="2153.099" dur="3.24">somebody else I could pass on</text><text start="2154.359" dur="3.421">inheritance</text><text start="2156.339" dur="4.381">um</text><text start="2157.78" dur="4.92">as soon as I have private property as a</text><text start="2160.72" dur="3.66">possibility now there is an incentive to</text><text start="2162.7" dur="3.36">try to turn more of nature into my</text><text start="2164.38" dur="3.0">private property right and to try to</text><text start="2166.06" dur="4.2">turn more of other people&apos;s actions in</text><text start="2167.38" dur="6.78">my private property but when the</text><text start="2170.26" dur="5.76">property is actual real</text><text start="2174.16" dur="4.26">Commodities and goods</text><text start="2176.02" dur="4.68">or the agreements that people can do</text><text start="2178.42" dur="4.8">Services let&apos;s stick with Goods because</text><text start="2180.7" dur="4.44">it&apos;s easier for right now there&apos;s a</text><text start="2183.22" dur="4.5">diminishing return on the value of any</text><text start="2185.14" dur="4.08">of those based on the illiquidity of</text><text start="2187.72" dur="3.0">them or the difficulty of moving them I</text><text start="2189.22" dur="3.06">get more lumber at a certain point I</text><text start="2190.72" dur="3.0">have more lumber than I can use and I</text><text start="2192.28" dur="3.299">can&apos;t even move it around to sell all</text><text start="2193.72" dur="4.02">that quickly and so I don&apos;t really want</text><text start="2195.579" dur="3.661">all that much more of it right and the</text><text start="2197.74" dur="3.3">same would be true with or of a certain</text><text start="2199.24" dur="3.839">kind or whatever</text><text start="2201.04" dur="4.02">but as soon as we move to a kind of a</text><text start="2203.079" dur="3.841">currency mediated system where I can</text><text start="2205.06" dur="4.44">sell it in real time and turn it all</text><text start="2206.92" dur="5.4">into something that has no intrinsic</text><text start="2209.5" dur="4.26">value but the optionality for every form</text><text start="2212.32" dur="3.2">of value</text><text start="2213.76" dur="4.5">well now there&apos;s no fungibility</text><text start="2215.52" dur="4.66">fungibility now</text><text start="2218.26" dur="3.72">and obviously we started with things</text><text start="2220.18" dur="3.659">that had intrinsic value but still got</text><text start="2221.98" dur="3.3">used to mediate it like gold but then</text><text start="2223.839" dur="2.881">you know we got to Fiat so I&apos;m just</text><text start="2225.28" dur="3.839">going to do this huge jump to Fiat</text><text start="2226.72" dur="4.74">because roots of the current system</text><text start="2229.119" dur="5.96">even though it has no intrinsic value</text><text start="2231.46" dur="7.139">what the value that it has is maximum</text><text start="2235.079" dur="4.901">speed of optionality right maximum</text><text start="2238.599" dur="5.821">optionality and maximum kind of</text><text start="2239.98" dur="6.54">liquidity and speed and so</text><text start="2244.42" dur="4.56">in that situation there&apos;s no diminishing</text><text start="2246.52" dur="5.16">return on getting more like more is more</text><text start="2248.98" dur="4.619">right and uh whether I want to convert</text><text start="2251.68" dur="4.5">that money into military power or</text><text start="2253.599" dur="4.681">convert it into public opinion through</text><text start="2256.18" dur="4.02">median campaigns and stuff or convert it</text><text start="2258.28" dur="4.079">into technological power of one kind or</text><text start="2260.2" dur="3.84">another kind or land ownership I the</text><text start="2262.359" dur="2.76">money allows me the ability to do all</text><text start="2264.04" dur="3.24">that so you can think of it as just</text><text start="2265.119" dur="6.661">units of power or units of Game Theory</text><text start="2267.28" dur="6.6">right units of ooda loop and</text><text start="2271.78" dur="3.6">um then when you add money on money</text><text start="2273.88" dur="3.239">Dynamics to it</text><text start="2275.38" dur="3.3">compounding interest as the beginning</text><text start="2277.119" dur="3.72">then of course all of the financial</text><text start="2278.68" dur="3.72">services that</text><text start="2280.839" dur="3.721">um become possible with more more</text><text start="2282.4" dur="4.32">Capital but just compounding interest</text><text start="2284.56" dur="4.26">not only is it not a diminishing</text><text start="2286.72" dur="3.899">response to as I get more and more or it</text><text start="2288.82" dur="3.18">becomes less valuable to me because I</text><text start="2290.619" dur="3.72">can&apos;t use it fast enough now as I get</text><text start="2292.0" dur="4.68">more money it is actually exponentially</text><text start="2294.339" dur="4.921">making money on itself</text><text start="2296.68" dur="4.62">so when I have private property</text><text start="2299.26" dur="3.24">I have the ability to turn all of that</text><text start="2301.3" dur="2.64">into</text><text start="2302.5" dur="3.839">fungible</text><text start="2303.94" dur="3.6">units of capital and it makes money on</text><text start="2306.339" dur="3.481">itself</text><text start="2307.54" dur="4.52">there is now a maximum incentive to turn</text><text start="2309.82" dur="6.0">as much of the world as possible into</text><text start="2312.06" dur="5.08">capital in my holding and because other</text><text start="2315.82" dur="3.18">people are and they could use that</text><text start="2317.14" dur="3.719">against me there is now an arms race for</text><text start="2319.0" dur="7.26">me to do it faster than that guy</text><text start="2320.859" dur="5.401">right and that&apos;s decentralized and</text><text start="2326.56" dur="4.08">um</text><text start="2327.7" dur="4.56">now of course we can see that when we&apos;re</text><text start="2330.64" dur="3.18">talking about</text><text start="2332.26" dur="3.3">there are types of power that don&apos;t</text><text start="2333.82" dur="3.84">directly just relate to dollars right</text><text start="2335.56" dur="4.2">the number of Twitter following is one</text><text start="2337.66" dur="4.56">or the amount of covert political</text><text start="2339.76" dur="3.48">influence or</text><text start="2342.22" dur="3.48">um</text><text start="2343.24" dur="4.08">uh you know many many other things there</text><text start="2345.7" dur="3.6">are military generals that have more</text><text start="2347.32" dur="4.38">total power than the amount of money</text><text start="2349.3" dur="4.02">they have but obviously they influence a</text><text start="2351.7" dur="3.899">huge amount of money in terms of</text><text start="2353.32" dur="4.019">military assets how much they cost and</text><text start="2355.599" dur="3.721">things like that</text><text start="2357.339" dur="3.541">um and</text><text start="2359.32" dur="3.06">so that&apos;s why I say I don&apos;t want to</text><text start="2360.88" dur="3.0">reduce it exclusively to money but if we</text><text start="2362.38" dur="2.64">had to pick a single metric that has the</text><text start="2363.88" dur="3.06">most kind of</text><text start="2365.02" dur="4.5">optionality for all other types of</text><text start="2366.94" dur="5.399">metrics that would be the one so if if</text><text start="2369.52" dur="4.26">we&apos;re thinking about molok as a whole we</text><text start="2372.339" dur="3.601">can see that whether we&apos;re talking about</text><text start="2373.78" dur="4.68">the environmental issues or whether</text><text start="2375.94" dur="4.919">we&apos;re talking about the increasing</text><text start="2378.46" dur="4.5">polarization because of social media</text><text start="2380.859" dur="3.541">algorithms or whether we&apos;re talking</text><text start="2382.96" dur="3.48">about</text><text start="2384.4" dur="4.92">um you know any of these things the</text><text start="2386.44" dur="5.7">rapid race that is not orienting towards</text><text start="2389.32" dur="4.98">safety enough on new technologies that</text><text start="2392.14" dur="4.68">this set of Dynamics is underneath it</text><text start="2394.3" dur="4.2">right and this is why that kind of frame</text><text start="2396.82" dur="4.019">Scott Alexander and others have put</text><text start="2398.5" dur="4.32">forward which is who is engineering this</text><text start="2400.839" dur="4.26">thing well molok is engineering this</text><text start="2402.82" dur="6.18">thing right like that that thing overall</text><text start="2405.099" dur="7.321">and now this is where</text><text start="2409.0" dur="5.339">I want to stop and go into the what is a</text><text start="2412.42" dur="3.54">misaligned AGI what is a paperclip</text><text start="2414.339" dur="3.421">maximizer for a moment because it</text><text start="2415.96" dur="4.56">actually makes moloch clearer and then</text><text start="2417.76" dur="3.599">moloch makes it clearer do you want to</text><text start="2420.52" dur="2.7">um</text><text start="2421.359" dur="4.681">construct the kind of paper clip</text><text start="2423.22" dur="4.68">maximizer scenario for people</text><text start="2426.04" dur="4.2">um you know a lot of the people in the</text><text start="2427.9" dur="3.719">kind of AI risk space so the paperclip</text><text start="2430.24" dur="4.74">maximizer is like a thought experiment</text><text start="2431.619" dur="6.061">that is basically of like an extreme</text><text start="2434.98" dur="4.08">super intelligence gone wrong</text><text start="2437.68" dur="2.76">because</text><text start="2439.06" dur="3.84">just because you can build something</text><text start="2440.44" dur="4.32">that is by definition super intelligent</text><text start="2442.9" dur="4.26">you know in that it&apos;s you know and if we</text><text start="2444.76" dur="5.22">Define intelligence in what I think is</text><text start="2447.16" dur="4.62">the best definition which is uh the</text><text start="2449.98" dur="5.58">ability to</text><text start="2451.78" dur="5.52">optimize and navigate a very broad range</text><text start="2455.56" dur="3.9">of terrains in order to achieve whatever</text><text start="2457.3" dur="4.14">your goals are so if we can achieve you</text><text start="2459.46" dur="3.42">know Define intelligence is that your</text><text start="2461.44" dur="4.44">ability to basically get stuff done</text><text start="2462.88" dur="5.459">across a wide range of environments</text><text start="2465.88" dur="5.82">um that does not necessarily guarantee</text><text start="2468.339" dur="5.461">that you also have the wisdom to decide</text><text start="2471.7" dur="3.24">what your goals should be in the first</text><text start="2473.8" dur="2.64">place</text><text start="2474.94" dur="3.54">um it&apos;s called The orthogonality Thesis</text><text start="2476.44" dur="4.74">the idea that like you know</text><text start="2478.48" dur="3.78">you know maybe intelligence and wisdom</text><text start="2481.18" dur="2.7">are perfectly aligned but there&apos;s</text><text start="2482.26" dur="3.359">actually very large possibility that</text><text start="2483.88" dur="3.36">they are completely unaligned you know</text><text start="2485.619" dur="3.601">they&apos;re just orthogonal to one another</text><text start="2487.24" dur="5.16">and so the paperclip maximizer is like</text><text start="2489.22" dur="5.46">the extreme you know a silly uh</text><text start="2492.4" dur="4.92">example of That Into You know an</text><text start="2494.68" dur="4.439">arbitrary example whereby you you know</text><text start="2497.32" dur="4.62">let&apos;s say you want to build</text><text start="2499.119" dur="5.041">um a machine you you are you you have a</text><text start="2501.94" dur="4.139">factory that builds paper clips</text><text start="2504.16" dur="3.179">um and then you happen to get hold of a</text><text start="2506.079" dur="3.241">super intelligence</text><text start="2507.339" dur="4.681">um that will help you build them as fast</text><text start="2509.32" dur="5.34">as possible so you make maximum profit</text><text start="2512.02" dur="4.559">um your super intelligence then is able</text><text start="2514.66" dur="4.38">to because it&apos;s so capable at navigating</text><text start="2516.579" dur="4.801">a broad range of goals uh turn every</text><text start="2519.04" dur="4.26">atom that it comes across into more</text><text start="2521.38" dur="3.84">paper clips uh until the universe</text><text start="2523.3" dur="4.08">becomes tiled with them</text><text start="2525.22" dur="3.54">um so yeah that&apos;s it it&apos;s basically a</text><text start="2527.38" dur="3.479">very</text><text start="2528.76" dur="4.8">somewhat oversimplified but at the same</text><text start="2530.859" dur="5.881">time kind of comically salient example</text><text start="2533.56" dur="5.4">of a deeply misaligned uh but</text><text start="2536.74" dur="4.74">nonetheless super intelligent system</text><text start="2538.96" dur="3.36">right so</text><text start="2541.48" dur="2.76">um</text><text start="2542.32" dur="2.94">just to construct a couple key parts of</text><text start="2544.24" dur="4.379">it</text><text start="2545.26" dur="5.22">paperclip is obviously a a silly and</text><text start="2548.619" dur="3.541">kind of cute on purpose example of</text><text start="2550.48" dur="3.599">whatever it is right whatever commodity</text><text start="2552.16" dur="4.86">whatever widget that you&apos;re optimizing</text><text start="2554.079" dur="5.241">for and without even saying super</text><text start="2557.02" dur="4.74">intelligence let&apos;s just say</text><text start="2559.32" dur="3.82">increasingly good increasingly competent</text><text start="2561.76" dur="3.48">and generalizable artificial</text><text start="2563.14" dur="3.3">intelligence gets applied to the</text><text start="2565.24" dur="4.8">corporation which is already happening</text><text start="2566.44" dur="7.44">everywhere and it has just two features</text><text start="2570.04" dur="6.0">which is it can work to achieve a goal</text><text start="2573.88" dur="4.5">whatever its objective function is in</text><text start="2576.04" dur="3.96">this case make more paper clips and it</text><text start="2578.38" dur="3.06">can recursively improve itself so it</text><text start="2580.0" dur="3.599">gets better at doing that thing that&apos;s</text><text start="2581.44" dur="4.32">key right</text><text start="2583.599" dur="4.02">um so of course at first it does a bunch</text><text start="2585.76" dur="4.02">of stuff that we want that figures out</text><text start="2587.619" dur="4.681">how to turn off lights when people</text><text start="2589.78" dur="4.319">aren&apos;t there to save cost and energy and</text><text start="2592.3" dur="3.779">how to make more efficient Supply chains</text><text start="2594.099" dur="3.421">and negotiate better deals and all those</text><text start="2596.079" dur="3.481">kinds of things and just makes a more</text><text start="2597.52" dur="5.4">efficient business and of course the</text><text start="2599.56" dur="5.58">reducto at absurdum is once it has done</text><text start="2602.92" dur="3.6">all of the easy good stuff it still has</text><text start="2605.14" dur="2.82">the objective function make more paper</text><text start="2606.52" dur="3.42">clips</text><text start="2607.96" dur="3.72">then it has to start doing stuff that is</text><text start="2609.94" dur="2.82">not just obviously easy good right where</text><text start="2611.68" dur="2.46">there&apos;s some trade-offs that are</text><text start="2612.76" dur="2.64">happening somewhere else but the things</text><text start="2614.14" dur="2.52">that are being harmed are not part of</text><text start="2615.4" dur="2.459">its objective function right its</text><text start="2616.66" dur="2.88">objective function isn&apos;t makes the most</text><text start="2617.859" dur="3.24">paper clips wow don&apos;t you&apos;re doing any</text><text start="2619.54" dur="3.539">harm not no harm anywhere else because</text><text start="2621.099" dur="3.961">that the wow don&apos;t do harm anywhere else</text><text start="2623.079" dur="4.321">is actually incredibly hard to specify</text><text start="2625.06" dur="4.32">in the easy computational way which is</text><text start="2627.4" dur="5.459">the heart of what we&apos;d call you know the</text><text start="2629.38" dur="4.86">alignment problem and so if you did have</text><text start="2632.859" dur="3.421">something that could recursively</text><text start="2634.24" dur="4.5">increase its ability to achieve a goal</text><text start="2636.28" dur="5.28">like that and then</text><text start="2638.74" dur="4.379">had enough generalized intelligence that</text><text start="2641.56" dur="3.0">it could out-compete anyone that was</text><text start="2643.119" dur="3.601">competing against it right it could</text><text start="2644.56" dur="4.2">increase its capability faster than say</text><text start="2646.72" dur="3.18">we as humans could and we&apos;re like oh</text><text start="2648.76" dur="3.18"> we don&apos;t want you to be making</text><text start="2649.9" dur="3.12">paper clips out of our food sources we</text><text start="2651.94" dur="3.06">don&apos;t want you to be making paper clips</text><text start="2653.02" dur="4.5">out of but it figures out how to beat us</text><text start="2655.0" dur="4.079">at those games then yes eventually it</text><text start="2657.52" dur="3.96">just starts turning everything into the</text><text start="2659.079" dur="4.381">substrate for paper clips and in general</text><text start="2661.48" dur="3.78">the idea is</text><text start="2663.46" dur="3.96">you have an objective function whatever</text><text start="2665.26" dur="2.94">the AI is optimizing for whatever that</text><text start="2667.42" dur="2.46">is</text><text start="2668.2" dur="3.24">the</text><text start="2669.88" dur="3.18">um and there&apos;s different ways that</text><text start="2671.44" dur="3.54">people will describe it in nuance and</text><text start="2673.06" dur="4.68">you know it&apos;s worth reading utkowski and</text><text start="2674.98" dur="5.099">Bostrom and the other you know kind of</text><text start="2677.74" dur="3.9">um seminal thinkers on what the nature</text><text start="2680.079" dur="3.481">of the AGI alignment problem is but</text><text start="2681.64" dur="4.439">roughly</text><text start="2683.56" dur="3.96">if you have an artificial intelligence</text><text start="2686.079" dur="3.721">it is</text><text start="2687.52" dur="4.62">General and autonomous autonomous</text><text start="2689.8" dur="3.6">meaning it is working on its own you</text><text start="2692.14" dur="2.52">don&apos;t have to keep giving it prompts</text><text start="2693.4" dur="2.34">right it&apos;s doing its own thing it has</text><text start="2694.66" dur="3.54">agency</text><text start="2695.74" dur="5.16">and where you can&apos;t pull the plug on it</text><text start="2698.2" dur="6.18">right that&apos;s a key part</text><text start="2700.9" dur="5.76">and it can upgrade its own capability to</text><text start="2704.38" dur="4.08">do whatever it is that it&apos;s seeking to</text><text start="2706.66" dur="4.8">do and it can upgrade its capability</text><text start="2708.46" dur="4.56">faster than we can because the smarter</text><text start="2711.46" dur="2.879">one then is capable of making even</text><text start="2713.02" dur="3.9">smarter one and we already see early</text><text start="2714.339" dur="4.561">signs of this we already see AI is</text><text start="2716.92" dur="4.919">starting to create better internal AI</text><text start="2718.9" dur="4.679">functions to be able to achieve their</text><text start="2721.839" dur="5.76">um the goals that are set for them</text><text start="2723.579" dur="8.401">the idea of do we want</text><text start="2727.599" dur="7.861">an autonomous general intelligence</text><text start="2731.98" dur="7.379">that is comprehensively smarter than us</text><text start="2735.46" dur="6.84">that is trying to fulfill a goal</text><text start="2739.359" dur="4.381">before we know that it&apos;s fulfillment of</text><text start="2742.3" dur="2.88">that goal isn&apos;t going to really mess</text><text start="2743.74" dur="3.48">stuff up for us</text><text start="2745.18" dur="3.12">right we&apos;re like no no we don&apos;t want</text><text start="2747.22" dur="2.58">that thing right we would like to</text><text start="2748.3" dur="3.72">prevent that</text><text start="2749.8" dur="4.38">um because it&apos;s entirely possible that</text><text start="2752.02" dur="4.44">it could do some</text><text start="2754.18" dur="4.919">uh things that are totally not what we</text><text start="2756.46" dur="4.74">want in pursuing that goal so obviously</text><text start="2759.099" dur="4.201">if its goal was to maximize GDP there&apos;s</text><text start="2761.2" dur="3.899">a lot of nasty ways to maximize GDP it</text><text start="2763.3" dur="4.019">can go up on with war it can go up with</text><text start="2765.099" dur="3.841">addiction it can go up with and whatever</text><text start="2767.319" dur="3.741">it is that the objective function is</text><text start="2768.94" dur="4.44">there&apos;s a lot of perverse instantiations</text><text start="2771.06" dur="4.48">of that thing being fulfilled in a way</text><text start="2773.38" dur="5.28">that totally messes up other stuff and</text><text start="2775.54" dur="5.819">so the AGI alignment question is can we</text><text start="2778.66" dur="4.919">actually ensure that before the thing is</text><text start="2781.359" dur="4.381">truly a general autonomous intelligence</text><text start="2783.579" dur="4.141">that it is aligned aligned with our</text><text start="2785.74" dur="3.42">interests Our intention our good or</text><text start="2787.72" dur="2.82">something what the nature of alignment</text><text start="2789.16" dur="2.459">means is actually a deep question I&apos;m</text><text start="2790.54" dur="3.66">going to put that on hold for a moment</text><text start="2791.619" dur="4.681">but roughly aligned with us such that</text><text start="2794.2" dur="4.26">that much power would be a safe thing</text><text start="2796.3" dur="4.14">for us to have exist</text><text start="2798.46" dur="5.52">and</text><text start="2800.44" dur="6.24">what I the thought experiment of an</text><text start="2803.98" dur="4.92">intelligence that was say</text><text start="2806.68" dur="6.439">as much more intelligent than us as we</text><text start="2808.9" dur="7.439">are than chimps or ants or whatever</text><text start="2813.119" dur="6.041">looking at how</text><text start="2816.339" dur="4.381">our increase in intelligence has voted</text><text start="2819.16" dur="2.58">for all the other inhabitants of the</text><text start="2820.72" dur="3.18">planet</text><text start="2821.74" dur="4.5">uh</text><text start="2823.9" dur="3.54">uh you know that&apos;s a very concerning</text><text start="2826.24" dur="2.339">thought</text><text start="2827.44" dur="3.899">mm-hmm</text><text start="2828.579" dur="6.961">now this is where I want to actually use</text><text start="2831.339" dur="6.061">the analogy of that an autonomous</text><text start="2835.54" dur="3.24">so it&apos;s doing stuff on its own right</text><text start="2837.4" dur="4.679">it&apos;s it&apos;s Auto poetic it&apos;s</text><text start="2838.78" dur="4.74">self-authoring it&apos;s self-upgrading and</text><text start="2842.079" dur="2.52">it&apos;s orienting towards an objective</text><text start="2843.52" dur="3.54">function</text><text start="2844.599" dur="5.461">and I would basically like to say that</text><text start="2847.06" dur="5.64">you could call the current global system</text><text start="2850.06" dur="4.14">and we just to simplify it let&apos;s call it</text><text start="2852.7" dur="2.879">global capitalism even though it&apos;s not</text><text start="2854.2" dur="4.44">that</text><text start="2855.579" dur="4.921">um we calling it molok would be better</text><text start="2858.64" dur="3.12">right</text><text start="2860.5" dur="2.94">um but let&apos;s just talk about the</text><text start="2861.76" dur="3.3">capitalism part because the metrics are</text><text start="2863.44" dur="4.919">kind of clear</text><text start="2865.06" dur="6.9">you could say that it is already a</text><text start="2868.359" dur="6.361">general auto poetic super intelligence</text><text start="2871.96" dur="4.1">it is has an objective function which is</text><text start="2874.72" dur="4.139">to</text><text start="2876.06" dur="6.22">convert as much of the world people&apos;s</text><text start="2878.859" dur="6.661">creativity ideas labor natural resources</text><text start="2882.28" dur="7.26">everything into capital</text><text start="2885.52" dur="5.28">and so that&apos;s the paper clips right and</text><text start="2889.54" dur="3.24">which is interesting because it has no</text><text start="2890.8" dur="3.96">real value Just optionality For Real</text><text start="2892.78" dur="3.299">value and there&apos;s always this assumption</text><text start="2894.76" dur="2.819">that there&apos;s more real value out there</text><text start="2896.079" dur="4.681">but that stops being true forever right</text><text start="2897.579" dur="6.181">so if I the tree</text><text start="2900.76" dur="4.74">sequester CO2 and produces oxygen and I</text><text start="2903.76" dur="3.66">need to breathe oxygen and it does a lot</text><text start="2905.5" dur="3.78">of other important things like supports</text><text start="2907.42" dur="3.48">pollinators and cleans the water and</text><text start="2909.28" dur="4.74">stabilizes topsoil and all these things</text><text start="2910.9" dur="4.62">for me but if I cut the tree down turn</text><text start="2914.02" dur="2.76">it into Lumber there&apos;s still enough</text><text start="2915.52" dur="3.36">oxygen</text><text start="2916.78" dur="4.14">for me I didn&apos;t actually ruin my if I</text><text start="2918.88" dur="3.36">cut the whole acre of trees down they&apos;re</text><text start="2920.92" dur="3.72">still oxygen in fact there&apos;s no</text><text start="2922.24" dur="3.839">differentially seeming less oxygen for</text><text start="2924.64" dur="3.179">me doing that but now I have the money</text><text start="2926.079" dur="3.361">of all the the lumber of these trees and</text><text start="2927.819" dur="4.221">I can do real tangible for me and</text><text start="2929.44" dur="6.36">my family or my Corporation with that</text><text start="2932.04" dur="5.92">and so the optionality value I get</text><text start="2935.8" dur="3.9">allows me to still access real value but</text><text start="2937.96" dur="2.639">I&apos;m destroying real value in the process</text><text start="2939.7" dur="1.8">so I&apos;m going to say well you&apos;re not</text><text start="2940.599" dur="3.661">destroying it because you&apos;re making</text><text start="2941.5" dur="4.859">Lumber well yes Lumber is actually</text><text start="2944.26" dur="3.9">radically less complex than a tree that</text><text start="2946.359" dur="4.74">has less total types of value that it</text><text start="2948.16" dur="5.28">does so we&apos;re converting the</text><text start="2951.099" dur="4.5">self-organizing self-repairing complex</text><text start="2953.44" dur="5.399">world and into an increasingly simple or</text><text start="2955.599" dur="5.701">complicated fragile world that has less</text><text start="2958.839" dur="5.52">types of value to less types of actors</text><text start="2961.3" dur="5.16">the tree has value of many different</text><text start="2964.359" dur="3.72">types to many different types of actors</text><text start="2966.46" dur="4.82">right so you can&apos;t just say well it&apos;s</text><text start="2968.079" dur="6.54">carbon sequestration but no it&apos;s it&apos;s</text><text start="2971.28" dur="5.079">stabilizing topsoil it&apos;s yeah a million</text><text start="2974.619" dur="4.521">things biodiversity</text><text start="2976.359" dur="4.801">Etc yeah so when we talk about</text><text start="2979.14" dur="4.36">artificial intelligence we talk about</text><text start="2981.16" dur="6.0">what type of computer system it runs on</text><text start="2983.5" dur="6.839">it Hardware wise it runs on CPUs or gpus</text><text start="2987.16" dur="6.84">or tpus or whatever it is and what type</text><text start="2990.339" dur="6.24">of algorithms that it runs and uh so</text><text start="2994.0" dur="3.9">what&apos;s it&apos;s interesting that we can</text><text start="2996.579" dur="2.821">already say humans are general</text><text start="2997.9" dur="4.14">intelligence</text><text start="2999.4" dur="5.28">and capitalism is running parallel</text><text start="3002.04" dur="5.1">process across all humans</text><text start="3004.68" dur="4.8">right and we know when you think about</text><text start="3007.14" dur="4.439">the cloud and why parallel process was</text><text start="3009.48" dur="3.72">so powerful it&apos;s also igp user so</text><text start="3011.579" dur="2.52">powerful</text><text start="3013.2" dur="2.58">um</text><text start="3014.099" dur="3.26">capitalism is basically as a</text><text start="3015.78" dur="3.839">decentralized incentive system</text><text start="3017.359" dur="3.76">incentivizing All Humans should both do</text><text start="3019.619" dur="3.301">novelty search figure out new ways of</text><text start="3021.119" dur="3.421">making money and exploitation take the</text><text start="3022.92" dur="3.96">existing ways and do the most of them</text><text start="3024.54" dur="4.14">that you can those that do better at it</text><text start="3026.88" dur="3.36">get more influence in the system and in</text><text start="3028.68" dur="3.48">turn influence the system in ways that</text><text start="3030.24" dur="3.72">support them to do more of it those that</text><text start="3032.16" dur="3.06">oppose the system are also opposing</text><text start="3033.96" dur="2.7">those who are doing well at the system</text><text start="3035.22" dur="4.2">so even though the system as a whole</text><text start="3036.66" dur="4.439">doesn&apos;t have agency those who do keep</text><text start="3039.42" dur="4.56">and check those that would oppose them</text><text start="3041.099" dur="6.901">so it is as if the system has agency</text><text start="3043.98" dur="8.28">and you go from Barter to</text><text start="3048.0" dur="7.5">currency to fiat currency to fractional</text><text start="3052.26" dur="6.68">Reserve banking to AI high-speed trading</text><text start="3055.5" dur="6.059">of derivatives and credit default swaps</text><text start="3058.94" dur="4.72">and that is basically the recursive</text><text start="3061.559" dur="4.861">upregulation of the algorithm right it</text><text start="3063.66" dur="4.62">is getting more and more capable of</text><text start="3066.42" dur="2.82">doing more and more financialization of</text><text start="3068.28" dur="3.0">the world</text><text start="3069.24" dur="4.26">to incentivize people to do more and</text><text start="3071.28" dur="4.38">more things though so you can see that</text><text start="3073.5" dur="3.78">you&apos;ve got something that is already</text><text start="3075.66" dur="4.14">running on all these General</text><text start="3077.28" dur="5.76">intelligences</text><text start="3079.8" dur="5.819">and as a result is super intelligent</text><text start="3083.04" dur="4.26">it is has an objective function the</text><text start="3085.619" dur="3.96">objective function is misaligned with</text><text start="3087.3" dur="5.46">the long-term well-being of the world</text><text start="3089.579" dur="5.461">and it advances narrow value metrics at</text><text start="3092.76" dur="3.96">The X and it&apos;s not that like you know</text><text start="3095.04" dur="3.96">the the but everything&apos;s getting better</text><text start="3096.72" dur="3.54">Pinker rossling type arguments or like</text><text start="3099.0" dur="2.76">saying but look at how many paper clips</text><text start="3100.26" dur="4.2">we have aren&apos;t we all stoked that we&apos;re</text><text start="3101.76" dur="4.339">getting cheaper paper clips and multiply</text><text start="3104.46" dur="4.8">by all the types of paper clips like yes</text><text start="3106.099" dur="6.341">narrow metrics are being Advanced at the</text><text start="3109.26" dur="5.28">cost of lots of wide metrics it ended up</text><text start="3112.44" dur="3.72">being critical to either the breakdown</text><text start="3114.54" dur="3.059">of life support so you get catastrophe</text><text start="3116.16" dur="3.26">or the breakdown of the quality of life</text><text start="3117.599" dur="4.681">you get dystopia</text><text start="3119.42" dur="5.159">how would that then</text><text start="3122.28" dur="5.1">it was sort of using that definition</text><text start="3124.579" dur="5.201">apply to the the difficulty of the</text><text start="3127.38" dur="3.9">alignment problem with an AGI so here&apos;s</text><text start="3129.78" dur="3.12">the thing we think about the super</text><text start="3131.28" dur="4.14">intelligence</text><text start="3132.9" dur="6.06">and a misaligned superintelligence is a</text><text start="3135.42" dur="5.399">very scary idea and if you want a sense</text><text start="3138.96" dur="4.379">beyond the silly paperclip maximizer how</text><text start="3140.819" dur="5.401">scary it is</text><text start="3143.339" dur="5.48">um read some of people like Eliezer</text><text start="3146.22" dur="5.339">utkowski and others on what AGI</text><text start="3148.819" dur="5.081">misalignment means superintelligence and</text><text start="3151.559" dur="4.441">slime it means</text><text start="3153.9" dur="5.1">um</text><text start="3156.0" dur="6.0">before we go all the way to moloch we</text><text start="3159.0" dur="4.859">could already say that collectives of</text><text start="3162.0" dur="3.96">lots of people interacting in particular</text><text start="3163.859" dur="4.861">ways whether we say a public corporation</text><text start="3165.96" dur="4.32">or say a nation-state</text><text start="3168.72" dur="3.48">let&apos;s say we take a large public</text><text start="3170.28" dur="3.839">corporation right well beyond what we</text><text start="3172.2" dur="4.08">have evolutionary history for we had</text><text start="3174.119" dur="3.841">evolutionary history for tribal type</text><text start="3176.28" dur="2.76">size things below the Dunbar number</text><text start="3177.96" dur="4.08">where everybody could talk to everybody</text><text start="3179.04" dur="5.4">so possibly the human scale</text><text start="3182.04" dur="4.2">FX and whatever could perpetuate through</text><text start="3184.44" dur="3.119">the whole thing at much larger scale of</text><text start="3186.24" dur="3.0">changes which is why the beginning of</text><text start="3187.559" dur="3.54">large civilizations are to have</text><text start="3189.24" dur="3.0">different properties</text><text start="3191.099" dur="4.621">um so you take a public corporation</text><text start="3192.24" dur="6.48">who&apos;s in control of it kind of nobody</text><text start="3195.72" dur="4.98">right like the people in the corporation</text><text start="3198.72" dur="3.899">answer up to the executive team answer</text><text start="3200.7" dur="3.44">to the CEO the CEO answers to the board</text><text start="3202.619" dur="5.521">the board answers to the shareholders</text><text start="3204.14" dur="4.0">the the</text><text start="3208.38" dur="6.239">board has a fiduciary responsibility to</text><text start="3212.46" dur="3.84">maximize profit returns to shareholders</text><text start="3214.619" dur="3.061">the shareholders are Pension funds and</text><text start="3216.3" dur="2.58">whatever where the managers of the</text><text start="3217.68" dur="4.2">shareholders are trying to get money</text><text start="3218.88" dur="5.16">back to the 401ks and the whoever is in</text><text start="3221.88" dur="3.3">there there end up being pieces of law</text><text start="3224.04" dur="3.24">that are bound up through the whole</text><text start="3225.18" dur="4.62">thing which is the the liability</text><text start="3227.28" dur="4.62">limiting status of the corporation that</text><text start="3229.8" dur="4.38">can privatize gains and socialize losses</text><text start="3231.9" dur="4.5">and the fiduciary responsibility of the</text><text start="3234.18" dur="6.179">uh directors to maximize shareholder</text><text start="3236.4" dur="5.76">profit and the on and on so who&apos;s really</text><text start="3240.359" dur="3.061">in charge of it you can get rid of a CEO</text><text start="3242.16" dur="2.64">and put a new one in and get rid of a</text><text start="3243.42" dur="2.88">director put a new one in you can sell</text><text start="3244.8" dur="3.12">some of the shares get new shareholder</text><text start="3246.3" dur="2.819">it&apos;s kind of this thing that gets set in</text><text start="3247.92" dur="3.659">motion where it has an objective</text><text start="3249.119" dur="4.261">function which is now maximized profit</text><text start="3251.579" dur="3.681">within the domain of how it figured out</text><text start="3253.38" dur="6.42">how to do that thing</text><text start="3255.26" dur="6.28">and so but because it is engaging it&apos;s</text><text start="3259.8" dur="3.42">running on all these human intelligences</text><text start="3261.54" dur="3.42">which are already general intelligence</text><text start="3263.22" dur="3.18">so it can do things that the humans can</text><text start="3264.96" dur="3.3">do plus things that none of those humans</text><text start="3266.4" dur="4.02">on their own could do right it takes a</text><text start="3268.26" dur="5.4">lot of humans together to do a Large</text><text start="3270.42" dur="4.8">Hadron Collider or a Hubble or an Exon</text><text start="3273.66" dur="2.82">right it has a capacity that nobody</text><text start="3275.22" dur="2.58">could do on its own so it is super</text><text start="3276.48" dur="3.839">intelligent it&apos;s beyond human</text><text start="3277.8" dur="3.779">intelligence in that way not just in a</text><text start="3280.319" dur="3.181">narrow way because it&apos;s engaging people</text><text start="3281.579" dur="3.721">that are already generally intelligent</text><text start="3283.5" dur="3.78">and then beyond just that it&apos;s already</text><text start="3285.3" dur="5.1">engaging computation so it&apos;s engaging</text><text start="3287.28" dur="4.079">the narrow but very powerful data</text><text start="3290.4" dur="4.32">processing</text><text start="3291.359" dur="6.181">and now we add AI to that</text><text start="3294.72" dur="4.68">and so we can say already</text><text start="3297.54" dur="5.16">that a nation-state or a public</text><text start="3299.4" dur="6.12">corporation is kind of a</text><text start="3302.7" dur="5.28">cybernetic</text><text start="3305.52" dur="5.099">general intelligence that is already</text><text start="3307.98" dur="5.28">misaligned can you just Define what you</text><text start="3310.619" dur="5.521">mean by cybernathic cybernex just the</text><text start="3313.26" dur="5.16">field of study for any type of system</text><text start="3316.14" dur="3.9">that kind of self-regulates right how</text><text start="3318.42" dur="3.48">the control mechanisms work how the</text><text start="3320.04" dur="4.44">regulatory mechanisms work so as a</text><text start="3321.9" dur="4.86">corporation has feedback loops it has</text><text start="3324.48" dur="4.44">feed forward Loops it has regulatory</text><text start="3326.76" dur="3.599">processes to be able to maintain what</text><text start="3328.92" dur="5.399">it&apos;s doing and upgrade what it&apos;s doing</text><text start="3330.359" dur="5.041">right Nation safe has that any and but</text><text start="3334.319" dur="3.361">it&apos;s</text><text start="3335.4" dur="3.959">it is dealing with internal but also</text><text start="3337.68" dur="4.02">external pressures that force it to be</text><text start="3339.359" dur="4.5">what it is maybe Google didn&apos;t want to</text><text start="3341.7" dur="3.84">release its large language model yet but</text><text start="3343.859" dur="3.661">as soon as its business model gets</text><text start="3345.54" dur="4.079">attacked by Microsoft releasing one and</text><text start="3347.52" dur="3.42">adding it to being in the possibility of</text><text start="3349.619" dur="3.181">search and it has to and so this is</text><text start="3350.94" dur="5.899">where the multipolar Trap the molokian</text><text start="3352.8" dur="4.039">type Dynamic comes in is</text><text start="3357.059" dur="4.02">the individual organization is not</text><text start="3359.04" dur="3.72">totally Sovereign because it&apos;s for it to</text><text start="3361.079" dur="3.78">keep existing it has to deal with the</text><text start="3362.76" dur="5.46">pressures defined by others</text><text start="3364.859" dur="4.74">and so either a sociopath can start</text><text start="3368.22" dur="4.379">something that then everybody has to</text><text start="3369.599" dur="5.22">deal with or everyone assuming the other</text><text start="3372.599" dur="4.561">one is about to do it next and no</text><text start="3374.819" dur="4.861">sociopath has a situation that is</text><text start="3377.16" dur="4.86">functionally sociopathic we talk about a</text><text start="3379.68" dur="3.72">corporate person 14th Amendment kind of</text><text start="3382.02" dur="3.72">giving personhood rights to a</text><text start="3383.4" dur="3.959">corporation in this weird way if we were</text><text start="3385.74" dur="3.06">to talk so this there&apos;s already a</text><text start="3387.359" dur="3.121">framework for thinking of it as an agent</text><text start="3388.8" dur="3.84">right the super intelligence as an agent</text><text start="3390.48" dur="4.02">but the fiduciary responsibility to</text><text start="3392.64" dur="4.38">maximize profit</text><text start="3394.5" dur="5.4">makes it kind of an obligate sociopath</text><text start="3397.02" dur="6.48">right that kind of thing</text><text start="3399.9" dur="5.64">um has to take the opportunities it has</text><text start="3403.5" dur="3.359">so long as it is not illegal within the</text><text start="3405.54" dur="3.0">confines of law but it can work to</text><text start="3406.859" dur="4.021">change law which is what all quite all</text><text start="3408.54" dur="3.539">big corporations Lobby right</text><text start="3410.88" dur="2.88">um and that ends up being one of the</text><text start="3412.079" dur="3.121">very profitable things that a company</text><text start="3413.76" dur="3.299">does is rather than the regulator</text><text start="3415.2" dur="3.0">limiting it too much at figuring out how</text><text start="3417.059" dur="2.881">to get the regulator to change</text><text start="3418.2" dur="3.419">regulation more aligned with its</text><text start="3419.94" dur="3.179">interests</text><text start="3421.619" dur="5.281">um</text><text start="3423.119" dur="5.281">and so for it to say hey</text><text start="3426.9" dur="3.84">uh</text><text start="3428.4" dur="4.14">a shareholder profit maximization</text><text start="3430.74" dur="3.54">fiduciary responsibility in an oil oil</text><text start="3432.54" dur="4.1">company and solving climate change are</text><text start="3434.28" dur="4.559">incommensible a shareholder fiduciary</text><text start="3436.64" dur="5.74">profit maximization and military</text><text start="3438.839" dur="5.22">industrial contractors and a world of</text><text start="3442.38" dur="5.4">peace that would de-necessitate all the</text><text start="3444.059" dur="5.181">demand or in commensurable right and so</text><text start="3447.78" dur="4.02">um</text><text start="3449.24" dur="4.54">then those competing with each other</text><text start="3451.8" dur="3.12">right so then any of the a groups are</text><text start="3453.78" dur="2.4">like hey I can&apos;t really do safety</text><text start="3454.92" dur="2.699">because the other ones we all kind of</text><text start="3456.18" dur="3.179">have to race at this and then maybe the</text><text start="3457.619" dur="3.24">whole us says we don&apos;t want to regulate</text><text start="3459.359" dur="3.421">it because we would rather our guys get</text><text start="3460.859" dur="3.361">there before China gets there</text><text start="3462.78" dur="3.059">um because whoever has it&apos;s going to run</text><text start="3464.22" dur="5.58">the world we&apos;d at least like it to be us</text><text start="3465.839" dur="6.601">companies so that molokian dynamic</text><text start="3469.8" dur="4.319">makes it to where each of these</text><text start="3472.44" dur="3.54">cybernetic superintelligence is</text><text start="3474.119" dur="3.72">interacting with each other creates a</text><text start="3475.98" dur="3.859">meta cybernetic super intelligence that</text><text start="3477.839" dur="4.74">you can call moloch right which is why I</text><text start="3479.839" dur="7.361">wanted to talk with you about it is you</text><text start="3482.579" dur="7.381">can see moloch as an emergent property</text><text start="3487.2" dur="7.5">of the systems of incentives</text><text start="3489.96" dur="7.74">and the uh dynamics of coordination that</text><text start="3494.7" dur="6.3">are built into the system</text><text start="3497.7" dur="5.399">where it is employing human general</text><text start="3501.0" dur="3.359">intelligence it&apos;s employing</text><text start="3503.099" dur="3.96">computational</text><text start="3504.359" dur="4.081">capabilities and increasingly artificial</text><text start="3507.059" dur="3.06">intelligence and the whole rest of our</text><text start="3508.44" dur="4.379">Tech stack</text><text start="3510.119" dur="5.7">it is up regulating through competitive</text><text start="3512.819" dur="5.901">Dynamics but up regulating in this</text><text start="3515.819" dur="6.901">narrow benefit kind of way</text><text start="3518.72" dur="7.599">and so we could say this thing that is</text><text start="3522.72" dur="5.76">driving climate change and driving</text><text start="3526.319" dur="3.54">species Extinction and dead zones and</text><text start="3528.48" dur="4.379">oceans and coral loss and</text><text start="3529.859" dur="5.881">desertification and arms races and</text><text start="3532.859" dur="5.46">polarization and all like that</text><text start="3535.74" dur="4.44">that is a misaligned super intelligence</text><text start="3538.319" dur="3.3">that nobody can pull a plug on</text><text start="3540.18" dur="3.0">it&apos;s already autonomous it&apos;s already</text><text start="3541.619" dur="5.72">nobody can pull the plug</text><text start="3543.18" dur="4.159">and it is building AI</text><text start="3548.339" dur="3.72">because corporations actually the people</text><text start="3550.5" dur="3.299">it&apos;s not actually the people within they</text><text start="3552.059" dur="2.881">think they&apos;re the ones building it well</text><text start="3553.799" dur="2.581">but they&apos;re building it within</text><text start="3554.94" dur="3.3">corporations that have a fiduciary</text><text start="3556.38" dur="4.32">responsibility for profit maximization</text><text start="3558.24" dur="4.98">that are in multi-polar traps with other</text><text start="3560.7" dur="4.919">companies that are racing to do it that</text><text start="3563.22" dur="4.92">have to look at how do we commercialize</text><text start="3565.619" dur="4.141">this thing whatever it is right or</text><text start="3568.14" dur="3.0">they&apos;re building it within nation states</text><text start="3569.76" dur="3.299">that have to be able to compete with</text><text start="3571.14" dur="4.32">another nation state and what that means</text><text start="3573.059" dur="4.701">is that some narrow value metrics that</text><text start="3575.46" dur="5.58">Define what wins the competition get</text><text start="3577.76" dur="6.64">prioritized over a wide value metrics</text><text start="3581.04" dur="6.299">and so it is fair to say that we already</text><text start="3584.4" dur="4.56">have a misaligned auto poetic</text><text start="3587.339" dur="4.321">superintelligence running the world</text><text start="3588.96" dur="4.8">running all running on and running all</text><text start="3591.66" dur="4.26">of the people</text><text start="3593.76" dur="4.5">to various degrees</text><text start="3595.92" dur="4.02">it is already employing all of the</text><text start="3598.26" dur="3.539">computational Power</text><text start="3599.94" dur="6.659">it is developing more computational</text><text start="3601.799" dur="7.02">power in AI the AI is being built by it</text><text start="3606.599" dur="5.281">in service of itself</text><text start="3608.819" dur="4.921">so the AI risk scenario that utkowski or</text><text start="3611.88" dur="3.419">Boston Brothers put forward of a thing</text><text start="3613.74" dur="3.3">where you can&apos;t pull the plug it is</text><text start="3615.299" dur="3.3">upgrading its capacity to do what it</text><text start="3617.04" dur="3.24">does it has an objective function it&apos;s</text><text start="3618.599" dur="3.061">pursuing but it harms stuff that we</text><text start="3620.28" dur="3.96">wouldn&apos;t want harmed in its subjective</text><text start="3621.66" dur="6.12">function in the pursuit of it what I&apos;m</text><text start="3624.24" dur="6.599">arguing is that we are already there</text><text start="3627.78" dur="5.819">and it is our world system</text><text start="3630.839" dur="6.301">and that AI is simply accelerating it</text><text start="3633.599" dur="5.581">and that we don&apos;t have to get to AGI</text><text start="3637.14" dur="4.8">to have the effect of it because you</text><text start="3639.18" dur="4.32">already have gi we already have general</text><text start="3641.94" dur="3.119">intelligence in the form of the</text><text start="3643.5" dur="4.799">corporation&apos;s nation states and the</text><text start="3645.059" dur="6.901">overall system where then adding AI even</text><text start="3648.299" dur="5.641">if it is not fully generalized to that</text><text start="3651.96" dur="3.839">system you already have something that</text><text start="3653.94" dur="4.679">is autonomous in general</text><text start="3655.799" dur="4.5">that is now getting increasingly potent</text><text start="3658.619" dur="5.641">capacities even if it&apos;s within a bunch</text><text start="3660.299" dur="6.181">of narrow domains right and so before we</text><text start="3664.26" dur="5.22">get to the case of just autonomous AI</text><text start="3666.48" dur="6.78">cut be being its own risk</text><text start="3669.48" dur="5.879">the existing AI in this landscape is</text><text start="3673.26" dur="4.98">driving the entire risk landscape is</text><text start="3675.359" dur="6.5">driving the overall is accelerating the</text><text start="3678.24" dur="3.619">topology that is already in place</text><text start="3683.839" dur="5.74">and this is why I said I think the</text><text start="3686.7" dur="5.879">misaligned AGI as a thought experiment</text><text start="3689.579" dur="4.681">helps people understand moloch but what</text><text start="3692.579" dur="3.661">the reality of moloch helps people</text><text start="3694.26" dur="5.4">understand that without getting to a</text><text start="3696.24" dur="6.24">total AGI that the nature of the risk</text><text start="3699.66" dur="4.5">there is already happening then with</text><text start="3702.48" dur="4.68">that we have to say what would it take</text><text start="3704.16" dur="4.8">to prevent those risks and it&apos;s a</text><text start="3707.16" dur="4.399">different calculus it&apos;s a different way</text><text start="3708.96" dur="5.82">of thinking about it</text><text start="3711.559" dur="4.421">so what do we do</text><text start="3714.78" dur="3.48">let me make it a little bit more</text><text start="3715.98" dur="5.339">tangible first and talk about</text><text start="3718.26" dur="5.88">sub AGI within this context this</text><text start="3721.319" dur="6.601">molokian metacrisis context what are the</text><text start="3724.14" dur="5.04">actual risks of AI look like</text><text start="3727.92" dur="4.139">I&apos;ll give a few different</text><text start="3729.18" dur="6.179">categorizations and I want to say I am</text><text start="3732.059" dur="5.401">not an AI alignment expert or a uh AI</text><text start="3735.359" dur="5.581">risk expert there are a lot of experts</text><text start="3737.46" dur="5.099">at places like Mary and Redwood and</text><text start="3740.94" dur="3.24">um other places that I think people</text><text start="3742.559" dur="3.54">should listen to pay a lot of attention</text><text start="3744.18" dur="3.24">to I&apos;m familiar enough of those</text><text start="3746.099" dur="3.781">arguments and then very specifically</text><text start="3747.42" dur="3.84">with the meta crisis argument to see how</text><text start="3749.88" dur="3.06">it relates and that&apos;s what I&apos;m speaking</text><text start="3751.26" dur="3.799">to here</text><text start="3752.94" dur="2.119">um</text><text start="3757.319" dur="3.841">there&apos;s something very unique about AI</text><text start="3759.599" dur="3.361">relative to all other forms of</text><text start="3761.16" dur="4.199">technology I&apos;ll speak to the deeper part</text><text start="3762.96" dur="4.8">in a moment but to begin with</text><text start="3765.359" dur="5.22">synthetic biology is very powerful like</text><text start="3767.76" dur="6.059">obviously it&apos;s very very powerful uh</text><text start="3770.579" dur="6.02">there are awesome applications there are</text><text start="3773.819" dur="5.821">awful applications</text><text start="3776.599" dur="4.96">but synthetic bio does not automatically</text><text start="3779.64" dur="4.8">give us the ability to make better</text><text start="3781.559" dur="4.681">drones</text><text start="3784.44" dur="3.78">doesn&apos;t give us the ability to make</text><text start="3786.24" dur="4.26">better high-speed trading doesn&apos;t give</text><text start="3788.22" dur="4.32">us the ability to make better nukes</text><text start="3790.5" dur="4.38">nukes don&apos;t automatically give us better</text><text start="3792.54" dur="3.12">bio weapons AI gives us better all of</text><text start="3794.88" dur="3.54">them</text><text start="3795.66" dur="5.04">right that&apos;s an important thing is that</text><text start="3798.42" dur="4.139">AI has the capacity to do optimization</text><text start="3800.7" dur="3.54">across all the things which means the</text><text start="3802.559" dur="4.921">good things which is what we all want we</text><text start="3804.24" dur="5.4">want to have ai work on protein folding</text><text start="3807.48" dur="3.3">for immuno oncology to cure cancer and</text><text start="3809.64" dur="3.36">on</text><text start="3810.78" dur="4.62">um receptor sites for new drug Discovery</text><text start="3813.0" dur="5.099">and to make Supply chains more efficient</text><text start="3815.4" dur="6.24">and things like that but everything that</text><text start="3818.099" dur="5.76">AI can optimize it can also break</text><text start="3821.64" dur="5.52">you run it in reverse right to the the</text><text start="3823.859" dur="5.041">AI that was doing drug Discovery I think</text><text start="3827.16" dur="4.86">it was Oak Ridge National Laboratories</text><text start="3828.9" dur="5.0">was ran in reverse and came up with a</text><text start="3832.02" dur="4.86">bunch of chemical weapons very rapidly</text><text start="3833.9" dur="5.74">and uh minus sign in front of it</text><text start="3836.88" dur="4.979">essentially and an AI that can optimize</text><text start="3839.64" dur="4.199">Supply chains can also optimize exactly</text><text start="3841.859" dur="3.72">how to break them right can optimize</text><text start="3843.839" dur="4.561">terrorist attacks on them even just</text><text start="3845.579" dur="4.681">through cyber and things like that a AI</text><text start="3848.4" dur="3.48">that can do protein folding for immuno</text><text start="3850.26" dur="2.579">oncology can also make up bio</text><text start="3851.88" dur="5.04">weapons</text><text start="3852.839" dur="6.361">and so the first principle is that as</text><text start="3856.92" dur="5.22">you it&apos;s very</text><text start="3859.2" dur="6.3">hard to advance AIA right it takes</text><text start="3862.14" dur="4.86">massive GPU Farms it takes only a few</text><text start="3865.5" dur="3.359">companies in the world that can even do</text><text start="3867.0" dur="3.359">the chip manufacturing to do that kind</text><text start="3868.859" dur="3.901">of thing</text><text start="3870.359" dur="5.061">um it takes a lot of computer science</text><text start="3872.76" dur="4.64">Talent massive amounts of data and Etc</text><text start="3875.42" dur="4.659">but</text><text start="3877.4" dur="4.48">once it&apos;s developed and then it is</text><text start="3880.079" dur="3.181">connected to the internet like a large</text><text start="3881.88" dur="3.78">language model</text><text start="3883.26" dur="3.96">can run on a lot less compute than it&apos;s</text><text start="3885.66" dur="2.88">trained on it takes a lot to train it</text><text start="3887.22" dur="4.68">doesn&apos;t take that much to run it right</text><text start="3888.54" dur="5.1">that&apos;s a big deal and it takes a lot to</text><text start="3891.9" dur="3.719">figure it out once it&apos;s been figured out</text><text start="3893.64" dur="4.5">and you publish the paper</text><text start="3895.619" dur="5.101">is and this is one of the key things is</text><text start="3898.14" dur="3.719">by building software you know it&apos;s very</text><text start="3900.72" dur="2.339">hard to build you know you need</text><text start="3901.859" dur="2.521">programming knowledge and so on to build</text><text start="3903.059" dur="2.641">a software but once you&apos;ve built it and</text><text start="3904.38" dur="3.78">you&apos;ve built the sort of user interface</text><text start="3905.7" dur="4.619">any any old schmuck can use it and get</text><text start="3908.16" dur="4.08">and like reap the benefits from it so if</text><text start="3910.319" dur="3.721">you have a company like a Google or an</text><text start="3912.24" dur="4.02">open AI or whatever that says hey we&apos;re</text><text start="3914.04" dur="3.84">going to put safety parameters on this</text><text start="3916.26" dur="3.539">I think there&apos;s a bunch of arguments</text><text start="3917.88" dur="4.32">against why that is</text><text start="3919.799" dur="5.76">even if they could do it won&apos;t be</text><text start="3922.2" dur="5.46">adequate but they can&apos;t because</text><text start="3925.559" dur="6.121">um you know you had the I think it was</text><text start="3927.66" dur="6.3">llama or alpaca the Stanford meta one</text><text start="3931.68" dur="5.54">that ended up getting leaked through</text><text start="3933.96" dur="6.599">some GitHub leak and then somebody died</text><text start="3937.22" dur="5.98">downloaded it onto a computer started</text><text start="3940.559" dur="4.141">sharing it and that means the full power</text><text start="3943.2" dur="4.28">of something that probably cost tens of</text><text start="3944.7" dur="4.8">millions of dollars to train is now</text><text start="3947.48" dur="3.4">unlocked and will be available for</text><text start="3949.5" dur="4.5">anybody to use for all the purposes so</text><text start="3950.88" dur="5.04">the safeties not there there are some</text><text start="3954.0" dur="4.619">projects working on decentralized AI</text><text start="3955.92" dur="5.399">that are about to make at least GPT 3.5</text><text start="3958.619" dur="5.781">level kind of unlocked widely available</text><text start="3961.319" dur="6.421">that thing is impossible to avoid</text><text start="3964.4" dur="5.14">and so the thing to understand is that</text><text start="3967.74" dur="4.26">it takes a lot of work to develop the</text><text start="3969.54" dur="5.34">new capacity once it&apos;s developed</text><text start="3972.0" dur="5.339">the barrier of Entry to be able to do</text><text start="3974.88" dur="3.84">the things that it allows to be done has</text><text start="3977.339" dur="3.301">been radically lowered for everybody</text><text start="3978.72" dur="3.48">that all the good things come from that</text><text start="3980.64" dur="3.479">we can all do more creative stuff that&apos;s</text><text start="3982.2" dur="4.919">exciting all the bad things come from</text><text start="3984.119" dur="5.041">that and so there&apos;s this principle that</text><text start="3987.119" dur="3.361">we could say all technology is dual use</text><text start="3989.16" dur="2.699">right you&apos;re developing it for some</text><text start="3990.48" dur="3.24">positive purpose but it has a military</text><text start="3991.859" dur="4.641">application but it&apos;s not just dual every</text><text start="3993.72" dur="7.92">technology is kind of omni use meaning</text><text start="3996.5" dur="7.539">it will get used for all the uses that</text><text start="4001.64" dur="4.56">people have incentives to use it for who</text><text start="4004.039" dur="4.201">are capable of using it so however much</text><text start="4006.2" dur="4.26">you lower the barrier of Entry anyone</text><text start="4008.24" dur="3.299">who can go into that barrier of Entry</text><text start="4010.46" dur="5.22">now we&apos;ll use it for the things they</text><text start="4011.539" dur="6.481">have incentives for so you develop it</text><text start="4015.68" dur="5.7">for we&apos;re going to cure cancer but now</text><text start="4018.02" dur="5.94">you&apos;ve got that ability for</text><text start="4021.38" dur="5.28">biological engineering really easily</text><text start="4023.96" dur="3.359">available widely right</text><text start="4026.66" dur="3.959">um</text><text start="4027.319" dur="5.401">and so I think the</text><text start="4030.619" dur="4.021">the couple things the first thing to</text><text start="4032.72" dur="3.54">understand about AI is AI can make</text><text start="4034.64" dur="4.08">better cyber weapons better nuclear</text><text start="4036.26" dur="4.68">weapons better drone weapons better all</text><text start="4038.72" dur="4.859">of those things better info weapons</text><text start="4040.94" dur="7.32">better population Centric weapons and so</text><text start="4043.579" dur="6.601">it increases the capacity to increase</text><text start="4048.26" dur="3.299">all the other risks in a way no other</text><text start="4050.18" dur="3.3">thing does</text><text start="4051.559" dur="5.28">you could kind of say well</text><text start="4053.48" dur="5.94">does that right energy every industry</text><text start="4056.839" dur="3.96">needs energy yeah but it&apos;s not doing the</text><text start="4059.42" dur="3.119">novelty search part of figuring out new</text><text start="4060.799" dur="3.3">better ways to make those domains it&apos;s</text><text start="4062.539" dur="4.08">simply just allowing them to do more of</text><text start="4064.099" dur="4.321">it right the AI both allows you to scale</text><text start="4066.619" dur="3.72">the stuff but also allows the innovation</text><text start="4068.42" dur="4.5">of Way new better stuff</text><text start="4070.339" dur="4.381">so that&apos;s novel that&apos;s a very novel</text><text start="4072.92" dur="4.26">thing about it so the first principle is</text><text start="4074.72" dur="4.2">anything AI can optimize it can break</text><text start="4077.18" dur="3.359">you develop it for one purpose it&apos;ll get</text><text start="4078.92" dur="3.72">used for all the purposes if anybody can</text><text start="4080.539" dur="3.481">figure out how to use it for you try to</text><text start="4082.64" dur="3.0">make safeties and whatever but you</text><text start="4084.02" dur="5.4">create an incentive now for a bunch of</text><text start="4085.64" dur="5.82">cat and mouse type Dynamics on how to</text><text start="4089.42" dur="4.86">utilize that</text><text start="4091.46" dur="5.359">and obviously we can think about</text><text start="4094.28" dur="6.539">the ones it just involve synthetic media</text><text start="4096.819" dur="4.0">and increasing</text><text start="4101.299" dur="4.92">hypernormal stimuli and ubiquitous deep</text><text start="4104.06" dur="4.139">fakes and really Dreadful things like</text><text start="4106.219" dur="4.02">that right like there&apos;s in terms of</text><text start="4108.199" dur="3.54">population Centric Warfare and breakdown</text><text start="4110.239" dur="3.0">of government and public trust and</text><text start="4111.739" dur="3.661">there&apos;s a lot that are very very near</text><text start="4113.239" dur="4.08">term</text><text start="4115.4" dur="3.899">so this is one set of risks anything</text><text start="4117.319" dur="4.681">that the AI can optimize it can also</text><text start="4119.299" dur="4.741">break that&apos;s kind of the bad actor case</text><text start="4122.0" dur="5.16">but the other case the more molokian</text><text start="4124.04" dur="5.159">case is just accelerating the thing that</text><text start="4127.16" dur="4.199">is already happening</text><text start="4129.199" dur="3.721">the externalities that aren&apos;t included</text><text start="4131.359" dur="3.121">in the optimization function is</text><text start="4132.92" dur="3.299">accelerating the externalities when</text><text start="4134.48" dur="3.66">we&apos;re already hitting the Tipping points</text><text start="4136.219" dur="4.741">on the externalities</text><text start="4138.14" dur="5.82">and so you could say yeah but AI is</text><text start="4140.96" dur="4.98">going to make it to where we can produce</text><text start="4143.96" dur="5.42">things so much more efficiently that</text><text start="4145.94" dur="3.44">it&apos;ll actually save the environment</text><text start="4149.779" dur="3.9">um</text><text start="4151.4" dur="4.379">not that there isn&apos;t a way to do that</text><text start="4153.679" dur="4.321">and those are the things that I want us</text><text start="4155.779" dur="3.721">to pursue but we&apos;re not on track for</text><text start="4158.0" dur="3.719">that you&apos;ve got this kind of jevin&apos;s</text><text start="4159.5" dur="4.679">paradox that when you increase the</text><text start="4161.719" dur="4.201">efficiency of energy you don&apos;t actually</text><text start="4164.179" dur="3.66">use less energy use more energy because</text><text start="4165.92" dur="3.12">now energy is cheaper which opens up a</text><text start="4167.839" dur="2.52">whole bunch of new markets that weren&apos;t</text><text start="4169.04" dur="2.699">open before the same as truth compute</text><text start="4170.359" dur="5.46">you make compute cheaper you use more</text><text start="4171.739" dur="5.58">compute not less compute and so AI make</text><text start="4175.819" dur="2.88">some stuff more efficient more efficient</text><text start="4177.319" dur="3.54">just means there are more things to</text><text start="4178.699" dur="3.54">which I can apply energy to get more</text><text start="4180.859" dur="4.5">energy as long as there&apos;s positive</text><text start="4182.239" dur="5.1">return we will go for it now to not go</text><text start="4185.359" dur="3.42">for all those things you can&apos;t do it</text><text start="4187.339" dur="3.541">purely incentive</text><text start="4188.779" dur="3.48">you have to do with deterrent with</text><text start="4190.88" dur="3.6">agreement with law with some other thing</text><text start="4192.259" dur="5.221">which is not adequate and in speed to</text><text start="4194.48" dur="4.64">the overall situation right now</text><text start="4197.48" dur="4.92">so</text><text start="4199.12" dur="5.2">one risk of AI is it anything you can</text><text start="4202.4" dur="3.9">use to optimize you can also use it to</text><text start="4204.32" dur="3.6">break it increases all of the other</text><text start="4206.3" dur="3.3">risks in a way nothing else increases</text><text start="4207.92" dur="4.68">all the other risks it increases the</text><text start="4209.6" dur="5.28">total complexity of the risk landscape</text><text start="4212.6" dur="3.599">Etc the other one is that even when</text><text start="4214.88" dur="3.12">you&apos;re using it for the positive</text><text start="4216.199" dur="3.301">purposes and you&apos;re succeeding at</text><text start="4218.0" dur="3.54">whatever your positive purposes are</text><text start="4219.5" dur="3.3">you&apos;re also speeding up externality</text><text start="4221.54" dur="2.22">right as we&apos;re hitting the Tipping</text><text start="4222.8" dur="3.84">points</text><text start="4223.76" dur="5.28">the next problem is you&apos;re increasing</text><text start="4226.64" dur="5.28">the info complexity I think yukowski</text><text start="4229.04" dur="4.44">calls it um inscrutable matrices of</text><text start="4231.92" dur="2.94">floating Point numbers talking about the</text><text start="4233.48" dur="2.64">large language models and like nobody</text><text start="4234.86" dur="2.58">actually knows what the is going on</text><text start="4236.12" dur="3.9">inside of them right so these kind of</text><text start="4237.44" dur="4.799">black boxes so the only thing that could</text><text start="4240.02" dur="3.84">figure out is your AI actually doing the</text><text start="4242.239" dur="4.681">thing it&apos;s supposed to do or not how do</text><text start="4243.86" dur="5.879">you if we wanted to if we wanted a</text><text start="4246.92" dur="5.22">law in some way to be able to</text><text start="4249.739" dur="3.721">regulate it or adjudicate what&apos;s</text><text start="4252.14" dur="4.74">happening it would take another AI</text><text start="4253.46" dur="7.14">That&apos;s more powerful to do it and so now</text><text start="4256.88" dur="5.64">you end up getting an increase toward</text><text start="4260.6" dur="5.82">you of the race towards the info</text><text start="4262.52" dur="5.88">Singularity where people can&apos;t actually</text><text start="4266.42" dur="3.66">make sense or adjudicate any of what&apos;s</text><text start="4268.4" dur="2.94">happening right the total complexity of</text><text start="4270.08" dur="2.099">everything is beyond our ability to</text><text start="4271.34" dur="3.68">process</text><text start="4272.179" dur="6.0">and that just means the</text><text start="4275.02" dur="4.36">unsolvability of everything increases</text><text start="4278.179" dur="3.0">so</text><text start="4279.38" dur="3.96">what I&apos;m saying is that if you add</text><text start="4281.179" dur="5.701">something as fastly recursive and</text><text start="4283.34" dur="5.899">Powerful as the increasingly generalized</text><text start="4286.88" dur="6.9">AIS that we have to an already</text><text start="4289.239" dur="7.241">misaligned super intelligence near the</text><text start="4293.78" dur="4.32">boundary points of breakdown</text><text start="4296.48" dur="3.42">that&apos;s a problem</text><text start="4298.1" dur="5.94">and that we should figure out alignment</text><text start="4299.9" dur="5.88">first so what I&apos;m suggesting is in the</text><text start="4304.04" dur="2.94">way that the AI risk Community is saying</text><text start="4305.78" dur="3.6">we should really try to figure out</text><text start="4306.98" dur="4.14">alignment before we race forward on</text><text start="4309.38" dur="2.88">developing more powerful AIS I&apos;m saying</text><text start="4311.12" dur="2.22">yes we really need to figure out</text><text start="4312.26" dur="2.88">alignment but it doesn&apos;t just mean</text><text start="4313.34" dur="3.66">alignment of the AIS it means alignment</text><text start="4315.14" dur="4.74">of the existing</text><text start="4317.0" dur="5.28">General intelligences these cyber</text><text start="4319.88" dur="4.38">General the capitalist model essentially</text><text start="4322.28" dur="4.74">so it&apos;s not fair to call it capitalism</text><text start="4324.26" dur="4.2">it&apos;s more like Game Theory capitalism</text><text start="4327.02" dur="4.5">just makes it easy to think about</text><text start="4328.46" dur="5.52">because there is actually a metric and</text><text start="4331.52" dur="4.38">it it turns out that that you know the</text><text start="4333.98" dur="4.02">capital can be used for</text><text start="4335.9" dur="3.96">population-centric Warfare or supply</text><text start="4338.0" dur="4.199">chain or most any of those things it</text><text start="4339.86" dur="5.64">ends up being kind of a unit of power</text><text start="4342.199" dur="4.741">pretty widely but yes that system that</text><text start="4345.5" dur="3.719">and it&apos;s hard to even call it a system</text><text start="4346.94" dur="4.739">right that set of</text><text start="4349.219" dur="4.621">um perverse incentives and the</text><text start="4351.679" dur="3.601">coordinations that arise from it is</text><text start="4353.84" dur="4.92">misaligned</text><text start="4355.28" dur="6.3">reaching criticality alignment in that</text><text start="4358.76" dur="4.26">thing has to be figured out because a</text><text start="4361.58" dur="6.06">misaligned</text><text start="4363.02" dur="6.3">context cannot develop aligned AI it</text><text start="4367.64" dur="3.12">can&apos;t emerge from it</text><text start="4369.32" dur="3.78">because that&apos;s I think some people&apos;s</text><text start="4370.76" dur="5.34">hope is that just give it enough</text><text start="4373.1" dur="5.34">capability and some emergent magic will</text><text start="4376.1" dur="4.44">essentially come this is I think why the</text><text start="4378.44" dur="4.98">orthogonality thesis is important which</text><text start="4380.54" dur="6.36">is to say it is possible to get very</text><text start="4383.42" dur="5.88">good at optimizing and not get a good at</text><text start="4386.9" dur="3.96">picking good goals right those are two</text><text start="4389.3" dur="3.14">separate things and we already see that</text><text start="4390.86" dur="4.44">in the world we&apos;re already</text><text start="4392.44" dur="4.54">much more good at creating Tech than we</text><text start="4395.3" dur="3.12">are at creating a world that everybody</text><text start="4396.98" dur="4.86">thinks is a world that really makes</text><text start="4398.42" dur="5.58">sense right so this is the exponential</text><text start="4401.84" dur="3.839">Tech gives us the power of gods we have</text><text start="4404.0" dur="3.0">not yet seen to demonstrate the love and</text><text start="4405.679" dur="2.461">wisdom of God&apos;s needed to bind it we</text><text start="4407.0" dur="4.08">have to figure that thing out or this</text><text start="4408.14" dur="4.32">thing kind of caps out same with the AI</text><text start="4411.08" dur="3.0">right and we&apos;ve handed it over to the</text><text start="4412.46" dur="7.08">shitty God as well</text><text start="4414.08" dur="5.46">the worst one so AI</text><text start="4421.52" dur="5.159">has fast enough feedback loops and</text><text start="4424.52" dur="4.159">enough power that it is one of the only</text><text start="4426.679" dur="5.581">things that could help</text><text start="4428.679" dur="5.801">change the other thing in time change</text><text start="4432.26" dur="4.439">the paper clip maximizing nature of the</text><text start="4434.48" dur="5.88">global system right of the Molex system</text><text start="4436.699" dur="5.881">in time but only if it was developed in</text><text start="4440.36" dur="4.319">association with the cybernetic systems</text><text start="4442.58" dur="3.42">that were actually aligned and aligned</text><text start="4444.679" dur="3.661">here doesn&apos;t just mean with our intent</text><text start="4446.0" dur="3.78">aligned means actually with our our</text><text start="4448.34" dur="3.66">long-term well-being and this is one of</text><text start="4449.78" dur="5.64">the critical issues is when we talk</text><text start="4452.0" dur="4.92">about alignment aligning AIS with human</text><text start="4455.42" dur="3.9">intent would not be great because human</text><text start="4456.92" dur="5.22">intent is not awesome so far right like</text><text start="4459.32" dur="5.879">that&apos;s kind of the point is that</text><text start="4462.14" dur="5.579">um whether we are looking at the</text><text start="4465.199" dur="4.321">overfishing of the oceans or proxy wars</text><text start="4467.719" dur="3.96">or whatever it is we&apos;re like</text><text start="4469.52" dur="5.06">we want to give exponentially more power</text><text start="4471.679" dur="2.901">to this species</text><text start="4474.92" dur="4.98">with its intent cotton multi-polar traps</text><text start="4477.32" dur="4.56">the way that it is it is not a good</text><text start="4479.9" dur="3.96">Steward of power in the Bronze Age it</text><text start="4481.88" dur="4.52">wasn&apos;t in Iron Age it wasn&apos;t it still</text><text start="4483.86" dur="4.68">isn&apos;t but with exponentially more power</text><text start="4486.4" dur="4.12">exponential externalities and</text><text start="4488.54" dur="3.6">exponential conflict both</text><text start="4490.52" dur="2.76">eventually break the finite playing</text><text start="4492.14" dur="3.599">field</text><text start="4493.28" dur="4.98">and so how do we</text><text start="4495.739" dur="4.44">and you know solving those coordination</text><text start="4498.26" dur="4.86">failure multi-polar trap type Dynamics</text><text start="4500.179" dur="5.761">are necessary for the wisdom to be able</text><text start="4503.12" dur="6.96">to Prevail adequately</text><text start="4505.94" dur="6.719">um so I am very hopeful of the very</text><text start="4510.08" dur="5.46">of the uniquely positive things to</text><text start="4512.659" dur="4.681">support coordination that</text><text start="4515.54" dur="3.24">computational capabilities and</text><text start="4517.34" dur="3.359">artificial intelligence in particular</text><text start="4518.78" dur="5.22">can help with they are not being</text><text start="4520.699" dur="5.46">developed for those purposes and in</text><text start="4524.0" dur="5.1">contexts that have the right Frameworks</text><text start="4526.159" dur="4.981">and the right incentives currently it&apos;s</text><text start="4529.1" dur="3.66">very much the opposite thing so rather</text><text start="4531.14" dur="4.98">than build them</text><text start="4532.76" dur="6.66">to be able to change moloch they are</text><text start="4536.12" dur="5.64">being built by moloch in its service</text><text start="4539.42" dur="4.98">even though no one building them would</text><text start="4541.76" dur="5.16">say that but the nature of the capital</text><text start="4544.4" dur="6.12">that they are building it with has that</text><text start="4546.92" dur="6.06">built right into it inherently yeah</text><text start="4550.52" dur="4.8">so</text><text start="4552.98" dur="4.92">the sort of</text><text start="4555.32" dur="4.08">attic of the thing that to me is the</text><text start="4557.9" dur="2.94">anti-molecular but I don&apos;t even like to</text><text start="4559.4" dur="3.0">call it the anti-molex because it is</text><text start="4560.84" dur="3.24">something that operates that by calling</text><text start="4562.4" dur="2.88">it The anti-molex it says it&apos;s on the</text><text start="4564.08" dur="2.76">same plane as it&apos;s the same</text><text start="4565.28" dur="3.36">dimensionality and it&apos;s something higher</text><text start="4566.84" dur="3.18">than that uh is this thing you know if</text><text start="4568.64" dur="3.66">Malik is the god of lose lose games</text><text start="4570.02" dur="5.1">negative some games what&apos;s the god of</text><text start="4572.3" dur="4.14">positive sum games I call it win-win</text><text start="4575.12" dur="3.02">everyone has different names for it you</text><text start="4576.44" dur="7.14">call it Omnia</text><text start="4578.14" dur="8.26">it is that the direction of the type of</text><text start="4583.58" dur="6.599">artificial intelligence we need to build</text><text start="4586.4" dur="5.4">and if so how would we go about</text><text start="4590.179" dur="3.361">doing that</text><text start="4591.8" dur="5.34">could we use</text><text start="4593.54" dur="5.04">a whole bunch of info Technologies</text><text start="4597.14" dur="3.539">including artificial intelligence</text><text start="4598.58" dur="3.78">including we can already see that all</text><text start="4600.679" dur="4.801">the problems that we saw in the social</text><text start="4602.36" dur="5.819">dilemma that you show in your second</text><text start="4605.48" dur="4.259">molok video and some in Beauty words the</text><text start="4608.179" dur="4.201">social media related ones</text><text start="4609.739" dur="4.621">well that&apos;s it&apos;s that is actually</text><text start="4612.38" dur="4.68">already a certain kind of AI right it&apos;s</text><text start="4614.36" dur="4.379">AI that is curating the news feed</text><text start="4617.06" dur="3.96">aligned with an objective function the</text><text start="4618.739" dur="3.901">objective function is either time on</text><text start="4621.02" dur="4.02">site or engagement or something like</text><text start="4622.64" dur="4.019">that some combo of metrics and so of</text><text start="4625.04" dur="4.139">course if the objective function is to</text><text start="4626.659" dur="5.161">maximize your engagement things that are</text><text start="4629.179" dur="6.421">addictive will do things that as you</text><text start="4631.82" dur="6.3">often tribalize will do and so it gets</text><text start="4635.6" dur="5.4">to take now what&apos;s interesting about the</text><text start="4638.12" dur="4.14">moloch part is so far Facebook or Tick</text><text start="4641.0" dur="3.239">Tock or whatever has not been creating</text><text start="4642.26" dur="3.66">its own content but it has been</text><text start="4644.239" dur="3.541">incentivizing all people to create</text><text start="4645.92" dur="3.36">content that will Rank and as the people</text><text start="4647.78" dur="3.18">pay attention to what ranks or doesn&apos;t</text><text start="4649.28" dur="3.84">as you saw Beauty filters were one thing</text><text start="4650.96" dur="4.38">and there are a lot of other things even</text><text start="4653.12" dur="5.22">so far as that all of the Legacy Media</text><text start="4655.34" dur="4.14">now does stuff that will make it rank on</text><text start="4658.34" dur="2.64">Facebook and Twitter or whatever because</text><text start="4659.48" dur="3.6">that&apos;s increasingly where the eyeballs</text><text start="4660.98" dur="3.66">are coming from so even that which would</text><text start="4663.08" dur="3.72">seem like</text><text start="4664.64" dur="5.28">an alternative source is still actually</text><text start="4666.8" dur="5.22">influenced by that thing so the</text><text start="4669.92" dur="4.2">to be able because it&apos;s so powerful to</text><text start="4672.02" dur="3.54">direct all human attention that&apos;s the</text><text start="4674.12" dur="2.7">thing right so whatever the algorithm is</text><text start="4675.56" dur="2.46">it&apos;s going to direct all human attention</text><text start="4676.82" dur="2.76">it&apos;s also going to direct all the</text><text start="4678.02" dur="3.0">innovation in the direction of what wins</text><text start="4679.58" dur="4.5">that algorithm</text><text start="4681.02" dur="5.28">and so and then because it&apos;s customizing</text><text start="4684.08" dur="4.079">the news feed for every person it&apos;s</text><text start="4686.3" dur="3.66">split testing what do the people click</text><text start="4688.159" dur="3.901">on and engage with and share and Etc so</text><text start="4689.96" dur="4.1">it&apos;s basically just objectively</text><text start="4692.06" dur="4.679">maximizing for personal engagement</text><text start="4694.06" dur="4.72">without paying attention to if it is</text><text start="4696.739" dur="3.301">positive or negative reward circuits and</text><text start="4698.78" dur="2.399">it happens to be that negative reward</text><text start="4700.04" dur="3.24">circuits are easier to hack than</text><text start="4701.179" dur="3.361">positive ones most of the time and</text><text start="4703.28" dur="2.52">because the positive ones you want to</text><text start="4704.54" dur="3.42">get the off the computer and go do</text><text start="4705.8" dur="3.6">other stuff and then Doom scroll but the</text><text start="4707.96" dur="3.18">Doom scroll Mass share thing the</text><text start="4709.4" dur="4.62">negative reward circuits feed better on</text><text start="4711.14" dur="6.66">so that&apos;s already an example</text><text start="4714.02" dur="5.639">of a bad objective function and an AI</text><text start="4717.8" dur="4.919">that has made it to where there are</text><text start="4719.659" dur="4.141">almost no adolescent girls with a good</text><text start="4722.719" dur="3.241">body image</text><text start="4723.8" dur="4.02">right dysmorphia is kind of ubiquitous</text><text start="4725.96" dur="3.42">it has made it to where polarization is</text><text start="4727.82" dur="3.72">as Extreme as it is and on and on like</text><text start="4729.38" dur="5.04">the externalities are massive what it&apos;s</text><text start="4731.54" dur="4.04">done to attention span and</text><text start="4734.42" dur="3.96">um</text><text start="4735.58" dur="4.84">now with synthetic media we&apos;re talking</text><text start="4738.38" dur="3.299">about not just an AI that can curate but</text><text start="4740.42" dur="2.94">can create</text><text start="4741.679" dur="4.621">now you can imagine a feedback between</text><text start="4743.36" dur="4.74">those when one is creating things it</text><text start="4746.3" dur="3.72">will be maximally sticky to you based on</text><text start="4748.1" dur="5.099">personal Dynamics split testing multiple</text><text start="4750.02" dur="5.76">created ones and the other one is</text><text start="4753.199" dur="4.441">curating but now not just getting all</text><text start="4755.78" dur="4.8">the humans to do decentralized create</text><text start="4757.64" dur="5.76">creation but also the as you can see</text><text start="4760.58" dur="4.44">that kind of feedback but of course if</text><text start="4763.4" dur="2.819">we had if we change the objective</text><text start="4765.02" dur="4.32">function</text><text start="4766.219" dur="5.401">of social media&apos;s AI right and rather</text><text start="4769.34" dur="4.2">and you did things like</text><text start="4771.62" dur="3.78">uh</text><text start="4773.54" dur="3.24">it&apos;s not just how much engagement it</text><text start="4775.4" dur="3.96">gets which can be fighting that is</text><text start="4776.78" dur="4.98">polarizing but something that say gets</text><text start="4779.36" dur="3.96">positive engagement across political</text><text start="4781.76" dur="3.06">divides and ideologic divides and</text><text start="4783.32" dur="3.24">memetic clusters which is stuff that we</text><text start="4784.82" dur="4.44">could do right we could do we have the</text><text start="4786.56" dur="4.82">info science to do that which means that</text><text start="4789.26" dur="4.919">it&apos;s identifying places where there is</text><text start="4791.38" dur="4.42">shared agreement or shared perception</text><text start="4794.179" dur="3.421">and upregulating that rather than the</text><text start="4795.8" dur="3.419">most divisive stuff you&apos;d have a totally</text><text start="4797.6" dur="3.18">different world and the technology could</text><text start="4799.219" dur="3.841">totally do that</text><text start="4800.78" dur="3.78">um it would not be as good for ad sales</text><text start="4803.06" dur="3.72">right now because it actually wouldn&apos;t</text><text start="4804.56" dur="4.08">keep people online quite as long and so</text><text start="4806.78" dur="3.24">this is where the fiscal model up</text><text start="4808.64" dur="3.36">the application of the tech because</text><text start="4810.02" dur="3.659">we&apos;re not just saying all social media</text><text start="4812.0" dur="4.199">is bad we&apos;re saying</text><text start="4813.679" dur="6.721">the incentives make us develop the</text><text start="4816.199" dur="6.96"> up versions of it right and so we</text><text start="4820.4" dur="4.98">could make social media that was</text><text start="4823.159" dur="3.901">exposing people to different ideas</text><text start="4825.38" dur="3.72">rather than reinforcing their existing</text><text start="4827.06" dur="3.599">ones and growing their Network to</text><text start="4829.1" dur="4.619">include people of very different types</text><text start="4830.659" dur="6.181">and like so similarly it rewards them to</text><text start="4833.719" dur="4.98">go out and touch cross essentially and</text><text start="4836.84" dur="3.72">you know almost rewards you for time</text><text start="4838.699" dur="4.5">spent off the app or something like that</text><text start="4840.56" dur="4.56">there&apos;s probably two they&apos;re a lot I</text><text start="4843.199" dur="3.121">mean now one of the challenges is of</text><text start="4845.12" dur="2.94">course the AI coming you can say we&apos;re</text><text start="4846.32" dur="3.96">just giving people what they want right</text><text start="4848.06" dur="3.48">but it&apos;s manufacturing Demand on bad</text><text start="4850.28" dur="2.52">reward circuits in the same way</text><text start="4851.54" dur="2.46">McDonald&apos;s can say we&apos;re giving people</text><text start="4852.8" dur="4.8">what we want but you have a much more</text><text start="4854.0" dur="4.92">obligation yeah exactly right and so on</text><text start="4857.6" dur="4.28">the other side you&apos;re like well it</text><text start="4858.92" dur="5.52">sounds kind of um</text><text start="4861.88" dur="4.359">paternalistic that we should pick what</text><text start="4864.44" dur="4.08">are the good reward circuits and but</text><text start="4866.239" dur="4.741">it&apos;s like no if you&apos;re influence if if</text><text start="4868.52" dur="4.74">you have that much asymmetric influence</text><text start="4870.98" dur="4.199">over humans to not take some</text><text start="4873.26" dur="3.78">responsibility for what the statistical</text><text start="4875.179" dur="4.02">changes in their life are is actually</text><text start="4877.04" dur="4.5">silly right now do we want the</text><text start="4879.199" dur="3.721">corporations to do that themselves do</text><text start="4881.54" dur="3.0">you want the government to do it we</text><text start="4882.92" dur="3.18">don&apos;t trust any authorities adequately</text><text start="4884.54" dur="4.08">right now for good reason so there&apos;s</text><text start="4886.1" dur="3.78">some very deep conversation around how</text><text start="4888.62" dur="3.36">do we</text><text start="4889.88" dur="4.02">ensure that the power of that technology</text><text start="4891.98" dur="4.02">is optimizing for things that actually</text><text start="4893.9" dur="3.839">increase quality of life in meaningful</text><text start="4896.0" dur="4.739">ways</text><text start="4897.739" dur="4.801">um but the same with all new types of AI</text><text start="4900.739" dur="3.361">could we use it to radically improve</text><text start="4902.54" dur="2.94">governance where we could actually be</text><text start="4904.1" dur="3.42">able to have the large language model</text><text start="4905.48" dur="4.62">see what people&apos;s beliefs and sentiments</text><text start="4907.52" dur="3.9">across the entire space are and find the</text><text start="4910.1" dur="2.7">topics that actually a lot of people</text><text start="4911.42" dur="3.0">agree on that super majorities would</text><text start="4912.8" dur="3.6">agree on and start there</text><text start="4914.42" dur="3.06">and be able to make platforms for</text><text start="4916.4" dur="3.96">candidates to actually be able to</text><text start="4917.48" dur="4.62">represent the the wills of the people</text><text start="4920.36" dur="4.62">better because we actually can see what</text><text start="4922.1" dur="4.74">they feel and believe at scale and</text><text start="4924.98" dur="3.3">um would it be possible for it to work</text><text start="4926.84" dur="2.64">on identifying things that a lot of</text><text start="4928.28" dur="3.48">people would</text><text start="4929.48" dur="5.28">uh believe or at least for it to give</text><text start="4931.76" dur="4.86">information about the stack ranking of</text><text start="4934.76" dur="3.78">the distribution of values to a</text><text start="4936.62" dur="4.14">proposition crafting process so that it</text><text start="4938.54" dur="3.3">could craft better propositions right so</text><text start="4940.76" dur="2.82">we&apos;re not saying that there are not</text><text start="4941.84" dur="5.64">awesome applications including maybe</text><text start="4943.58" dur="6.54">maybe the AI applications that are</text><text start="4947.48" dur="3.84">critical to be able to fix these moloch</text><text start="4950.12" dur="2.52">Dynamics because they can actually help</text><text start="4951.32" dur="3.12">coordination</text><text start="4952.64" dur="3.24">right but if you are not trying to</text><text start="4954.44" dur="3.6">actually understand the coordination</text><text start="4955.88" dur="4.14">failures deeply enough and say what</text><text start="4958.04" dur="3.659">should we really be trying to solve in</text><text start="4960.02" dur="3.54">terms of fixing coordination that makes</text><text start="4961.699" dur="3.48">this whole technosphere compatible with</text><text start="4963.56" dur="3.06">the biosphere compatible with human</text><text start="4965.179" dur="4.141">nature compatible with meaningful</text><text start="4966.62" dur="4.68">definitions of human flourishing it if</text><text start="4969.32" dur="3.78">it is as powerful as it is serving</text><text start="4971.3" dur="4.26">anything other than that</text><text start="4973.1" dur="4.8">the externalities in those areas will</text><text start="4975.56" dur="5.159">become increasingly catastrophic or</text><text start="4977.9" dur="6.0">dystopic or both are there any like</text><text start="4980.719" dur="6.721">promising projects that you think are</text><text start="4983.9" dur="4.44">trying to harness AI in a in this sort</text><text start="4987.44" dur="2.759">of</text><text start="4988.34" dur="4.379">again I don&apos;t like to say antimonic in</text><text start="4990.199" dur="6.421">this win-win way or you know</text><text start="4992.719" dur="5.761">using info Technologies in a in a way</text><text start="4996.62" dur="3.24">that is more aligned with what is</text><text start="4998.48" dur="3.0">actually good for human nature in the</text><text start="4999.86" dur="4.44">biosphere</text><text start="5001.48" dur="4.679">I think we can see like Audrey Tang&apos;s</text><text start="5004.3" dur="3.72">work in Taiwan with creating digital</text><text start="5006.159" dur="3.601">democracy where they look for unlikely</text><text start="5008.02" dur="3.6">consensus and they&apos;re starting to apply</text><text start="5009.76" dur="5.399">large language models</text><text start="5011.62" dur="4.559">um and things like that I I believe and</text><text start="5015.159" dur="3.901">um</text><text start="5016.179" dur="5.881">and you know a lot of the work that say</text><text start="5019.06" dur="4.02">the ethereum and some of the web 3</text><text start="5022.06" dur="2.88">communities tried to do with public</text><text start="5023.08" dur="4.86">goods is obviously thinking about some</text><text start="5024.94" dur="7.259">elements of that and um yeah it&apos;s not</text><text start="5027.94" dur="7.32">that nobody is but nobody with the giant</text><text start="5032.199" dur="6.421">capital in GPU farms and Etc</text><text start="5035.26" dur="7.5">has that as their primary objective</text><text start="5038.62" dur="6.059">um and the primary objectives of all of</text><text start="5042.76" dur="3.54">those ones have Milwaukee and Dynamics</text><text start="5044.679" dur="3.721">involved even if they also have some</text><text start="5046.3" dur="3.419">good Dynamics involved and that&apos;s a</text><text start="5048.4" dur="4.319">problem and that&apos;s what I would most</text><text start="5049.719" dur="6.061">hope to start to shift is in recognition</text><text start="5052.719" dur="5.221">of the total power that is there the</text><text start="5055.78" dur="4.08">total number of things that will be</text><text start="5057.94" dur="4.14">affected by it the downstream end order</text><text start="5059.86" dur="4.56">effects of it the speed the nature the</text><text start="5062.08" dur="4.8">irreversibility and there is a</text><text start="5064.42" dur="3.96">precautionary principle thing that</text><text start="5066.88" dur="4.799">uh</text><text start="5068.38" dur="5.58">who&apos;s right utkowski or Cristiano on a</text><text start="5071.679" dur="4.081">alignment or Whatever It Is Well given</text><text start="5073.96" dur="4.8">that most of the people who have studied</text><text start="5075.76" dur="4.68">AI alignment very deeply</text><text start="5078.76" dur="3.12">are concerned with the pace and</text><text start="5080.44" dur="3.6">Direction with which we&apos;re moving that&apos;s</text><text start="5081.88" dur="4.5">a pretty good sign that we should pay</text><text start="5084.04" dur="5.94">attention and where there is</text><text start="5086.38" dur="6.06">disagreements between experts</text><text start="5089.98" dur="5.16">but where there so there&apos;s where there&apos;s</text><text start="5092.44" dur="6.0">radical uncertainty but also maximum</text><text start="5095.14" dur="5.579">consequentiality and irreversibility</text><text start="5098.44" dur="4.62">go as fast as possible is not the right</text><text start="5100.719" dur="3.44">answer what if there was a way to just</text><text start="5103.06" dur="4.98">remove</text><text start="5104.159" dur="5.741">the competition side of it entirely like</text><text start="5108.04" dur="3.9">I know it sounds very pie in the sky but</text><text start="5109.9" dur="4.74">like if all the major</text><text start="5111.94" dur="4.98">companies you know which yes let&apos;s say</text><text start="5114.64" dur="4.32">the companies themselves are as</text><text start="5116.92" dur="4.08">by definition misaligned or like aligned</text><text start="5118.96" dur="3.12">with mollock but if the companies would</text><text start="5121.0" dur="3.719">essentially hang up their competition</text><text start="5122.08" dur="5.28">hat not compete and work together as a</text><text start="5124.719" dur="5.341">sort of single entity almost like a</text><text start="5127.36" dur="4.44">you know uh</text><text start="5130.06" dur="4.22">a collaborative science project</text><text start="5131.8" dur="4.56">essentially would that solve the problem</text><text start="5134.28" dur="3.399">fascinatingly when we talk about the way</text><text start="5136.36" dur="4.319">that</text><text start="5137.679" dur="4.741">laws get built that end up supposedly</text><text start="5140.679" dur="3.961">having good purposes and they do but</text><text start="5142.42" dur="5.22">they also end up being part of moloch</text><text start="5144.64" dur="6.059">like the fiduciary responsibility the</text><text start="5147.64" dur="4.74">director or whatever uh currently I</text><text start="5150.699" dur="4.141">think Anti-Trust laws would try to</text><text start="5152.38" dur="4.74">prevent that thing from happening</text><text start="5154.84" dur="3.359">um that you&apos;re mentioning and so the</text><text start="5157.12" dur="4.619">government would actually have to get</text><text start="5158.199" dur="4.621">involved but uh would I like to see the</text><text start="5161.739" dur="4.761">major</text><text start="5162.82" dur="3.68">uh AI labs</text><text start="5167.44" dur="3.98">together with the major academic</text><text start="5169.36" dur="4.859">researchers together with The Regulators</text><text start="5171.42" dur="6.4">take very seriously</text><text start="5174.219" dur="5.161">uh short medium long-term futures of AI</text><text start="5177.82" dur="4.08">that factor all these types of</text><text start="5179.38" dur="6.0">Assessments that factor the molokian</text><text start="5181.9" dur="6.6">Dynamics The Meta crisis the way that</text><text start="5185.38" dur="4.799">the AI that is the safeties won&apos;t</text><text start="5188.5" dur="4.199">totally last right they will get</text><text start="5190.179" dur="4.321">decentralized the safeties will be taken</text><text start="5192.699" dur="4.101">off it will be used for all purposes the</text><text start="5194.5" dur="5.58">kind of omni-use nature of it</text><text start="5196.8" dur="5.62">and to look at what does responsible</text><text start="5200.08" dur="4.02">movement forward in light of that look</text><text start="5202.42" dur="4.2">like I</text><text start="5204.1" dur="4.02">um I don&apos;t see any good answers that</text><text start="5206.62" dur="3.059">don&apos;t involve that</text><text start="5208.12" dur="3.9">yeah because that&apos;s the thing but the</text><text start="5209.679" dur="5.281">one thing mollock requires</text><text start="5212.02" dur="6.96">you know one thing Game Theory requires</text><text start="5214.96" dur="6.719">is competition of some kind some kind of</text><text start="5218.98" dur="4.679">either fabricated or real scarcity and</text><text start="5221.679" dur="4.381">in this instance there isn&apos;t really you</text><text start="5223.659" dur="4.801">know there are no intellectual</text><text start="5226.06" dur="3.659">powerhouses trying to compete to be the</text><text start="5228.46" dur="2.699">first to do a thing everyone is working</text><text start="5229.719" dur="3.361">on the same project together it&apos;s the</text><text start="5231.159" dur="3.181">ultimate cooperation thing</text><text start="5233.08" dur="3.78">um</text><text start="5234.34" dur="4.2">but yes it seems like a very almost</text><text start="5236.86" dur="4.319">intractable</text><text start="5238.54" dur="3.54">salute you know method because it would</text><text start="5241.179" dur="3.361">actually need to be an international</text><text start="5242.08" dur="4.74">collaboration as well it&apos;s not just you</text><text start="5244.54" dur="3.48">know I mean we do have the one advantage</text><text start="5246.82" dur="2.94">that</text><text start="5248.02" dur="3.48">it seems like the majority of the major</text><text start="5249.76" dur="4.14">players at least are all in Western</text><text start="5251.5" dur="5.4">Hemisphere and they all speak the same</text><text start="5253.9" dur="4.5">language and to degree share similar</text><text start="5256.9" dur="3.239">values</text><text start="5258.4" dur="4.86">um you know we&apos;re not well maybe not so</text><text start="5260.139" dur="5.281">much that but that is one starting point</text><text start="5263.26" dur="3.72">um there are some very Advanced AI labs</text><text start="5265.42" dur="3.239">in China</text><text start="5266.98" dur="4.98">um</text><text start="5268.659" dur="5.281">and the government-run ones versus the</text><text start="5271.96" dur="3.96">corporate ones are slightly different</text><text start="5273.94" dur="3.36">um there&apos;s obviously AI that is not just</text><text start="5275.92" dur="3.299">large language models for public</text><text start="5277.3" dur="4.26">deployment that is still also risky for</text><text start="5279.219" dur="4.201">many other purposes I think the the</text><text start="5281.56" dur="3.78">large language models for public</text><text start="5283.42" dur="4.38">deployment has a unique case because</text><text start="5285.34" dur="6.6">when the whole world starts</text><text start="5287.8" dur="6.839">uh utilizing them for the many many</text><text start="5291.94" dur="4.68">purposes they will it&apos;ll become nearly</text><text start="5294.639" dur="3.54">impossible to roll it back once we see</text><text start="5296.62" dur="3.059">the risks that are associated because we</text><text start="5298.179" dur="3.901">will have created so much economic and</text><text start="5299.679" dur="3.781">cultural dependence on it</text><text start="5302.08" dur="3.0">um</text><text start="5303.46" dur="4.98">so I think even if all we&apos;re talking</text><text start="5305.08" dur="6.54">about is as a start all the major GPU</text><text start="5308.44" dur="5.4">farms and large language models being in</text><text start="5311.62" dur="4.579">cooperation that would be a thing but I</text><text start="5313.84" dur="5.339">do think it does need to be</text><text start="5316.199" dur="6.281">wider than that and not just corporate</text><text start="5319.179" dur="6.201">and international uh</text><text start="5322.48" dur="2.9">and</text><text start="5325.86" dur="4.24">I think the</text><text start="5330.58" dur="5.28">I don&apos;t think there are any existential</text><text start="5333.46" dur="4.5">risks near term if we don&apos;t</text><text start="5335.86" dur="3.66">I don&apos;t think large language models</text><text start="5337.96" dur="4.259">destroy the world</text><text start="5339.52" dur="4.199">I think there will be problems and then</text><text start="5342.219" dur="2.46">we&apos;ll create solutions to those and</text><text start="5343.719" dur="3.181">whatever</text><text start="5344.679" dur="4.98">but I think it absolutely accelerates</text><text start="5346.9" dur="4.86">the overall meta crisis across many many</text><text start="5349.659" dur="4.381">vectors which is why one has to actually</text><text start="5351.76" dur="3.899">take that seriously to then really try</text><text start="5354.04" dur="3.96">to take responsibility for this thing</text><text start="5355.659" dur="5.401">for which there is so much incentive to</text><text start="5358.0" dur="4.98">not take responsibility absolutely</text><text start="5361.06" dur="3.9">is there any like specific call to</text><text start="5362.98" dur="4.679">action you want to put out there if</text><text start="5364.96" dur="4.92">there happened to be any you know</text><text start="5367.659" dur="3.421">major technology technologists listening</text><text start="5369.88" dur="4.68">to this</text><text start="5371.08" dur="6.96">I mean to to begin with it&apos;s really</text><text start="5374.56" dur="6.96">to engage with the wider risk arguments</text><text start="5378.04" dur="5.54">not just in super intelligent AGI taking</text><text start="5381.52" dur="4.38">off but the</text><text start="5383.58" dur="4.659">acceleration of all of the risks that</text><text start="5385.9" dur="3.72">the release of these things cause to</text><text start="5388.239" dur="4.861">really engage with those arguments</text><text start="5389.62" dur="4.92">seriously and to really think about the</text><text start="5393.1" dur="3.48">world that&apos;s creating and be like all</text><text start="5394.54" dur="4.5">right</text><text start="5396.58" dur="4.559">despite the humongous incentive for me</text><text start="5399.04" dur="4.38">to rush ahead with this</text><text start="5401.139" dur="4.861">uh</text><text start="5403.42" dur="5.7">is the</text><text start="5406.0" dur="4.98">risk calculus high enough the speed and</text><text start="5409.12" dur="4.019">the irreversibility high enough that I</text><text start="5410.98" dur="2.84">am actually</text><text start="5413.139" dur="4.261">um</text><text start="5413.82" dur="5.379">inclined to figure out something to to</text><text start="5417.4" dur="5.819">figure out better coordination around</text><text start="5419.199" dur="5.401">this and to Simply start engaging in</text><text start="5423.219" dur="3.121">um</text><text start="5424.6" dur="4.2">more percentage of their total energy</text><text start="5426.34" dur="4.5">going into risk analysis than</text><text start="5428.8" dur="4.32">opportunity advancement</text><text start="5430.84" dur="4.379">to have that more attached to the actual</text><text start="5433.12" dur="4.44">governance and choice making</text><text start="5435.219" dur="3.96">I think all companies working on AGI</text><text start="5437.56" dur="4.92">should kill the fiduciary agreement to</text><text start="5439.179" dur="4.681">maximize shareholder profit obviously</text><text start="5442.48" dur="2.84">um some have talked about that I think</text><text start="5443.86" dur="5.04">that should be killed</text><text start="5445.32" dur="6.58">and I think the earnest real engagement</text><text start="5448.9" dur="5.6">between the AI Labs the people in AI</text><text start="5451.9" dur="5.16">Safety Research and</text><text start="5454.5" dur="4.84">Regulators should be a very actively</text><text start="5457.06" dur="4.56">happening thing</text><text start="5459.34" dur="5.28">don&apos;t be more lucky</text><text start="5461.62" dur="3.0">I</text><text start="5464.679" dur="3.921">can print me one of those bumper</text><text start="5466.239" dur="2.361">stickers</text></transcript>