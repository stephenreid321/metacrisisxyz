<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.17" dur="6.57">As of today, we are in a war that has moved
the atomic clock closer to midnight than it</text><text start="6.74" dur="1.0">has ever been.</text><text start="7.74" dur="2.35">We&apos;re dealing with nukes and AI and things
like that.</text><text start="10.09" dur="5.509">We could easily have the last chapter in that
book if we are not more careful about confident,</text><text start="15.599" dur="3.531">wrong ideas.</text><text start="19.13" dur="2.79">This is a different sort of podcast.</text><text start="21.92" dur="4.43">Not only because it&apos;s Daniel Schmadenberger,
one of the most requested guests, who by the</text><text start="26.35" dur="1.95">way, I&apos;ll give an introduction to shortly,</text><text start="28.3" dur="4.95">but also because today marks season 3 of the
Theories of Everything podcast.</text><text start="33.25" dur="5.23">Each episode will be far more in-depth, more
challenging, more engaging, have more energy,</text><text start="38.48" dur="5.5">more effort, and more thought placed into
it than any single one of the previous episodes.</text><text start="43.98" dur="6.12">Welcome to the season premiere of season 3
of the Theories of Everything podcast with</text><text start="50.1" dur="11.79">myself, Curt Jaimungal.</text><text start="61.89" dur="7.04">This will be a journey of a podcast with several
moments of pause, of tutelage, of reflection,</text><text start="68.93" dur="3.74">of surprise appearances, even personal confessions.</text><text start="72.67" dur="4.559">This is meant for you to be able to watch
and re-watch or listen and re-listen.</text><text start="77.229" dur="1.271">As with every TOE!</text><text start="78.5" dur="3.82">podcast, there are timestamps in the description
and you can just scroll through to see the</text><text start="82.32" dur="1.78">different headings, the chapter marks.</text><text start="84.1" dur="4.09">I say this phrase frequently in the Theories
of Everything podcast, this phrase, just get</text><text start="88.19" dur="1.98">wet, which comes from Wheeler,</text><text start="90.17" dur="3.61">and it&apos;s about how there are these abstruse
concepts in mathematics and you&apos;re mainly</text><text start="93.78" dur="1.799">supposed to get used to them,</text><text start="95.579" dur="3.951">rather than attempt to bang your head against
the wall to understand it the first time through.</text><text start="99.53" dur="4.79">It&apos;s generally in the re-watching that much
of the lessons are acquired and absorbed and</text><text start="104.32" dur="1.0">understood.</text><text start="105.32" dur="3.6">While you may be listening to this, so either
you&apos;re walking around and it&apos;s on YouTube</text><text start="108.92" dur="2.62">or you&apos;re listening on Spotify or iTunes,</text><text start="111.54" dur="3.7">by the way, if you&apos;re watching on YouTube,
this is on Spotify and iTunes, links in the</text><text start="115.24" dur="1.0">description,</text><text start="116.24" dur="4.12">I recommend that you at least watch it once
on YouTube, so you periodically check in,</text><text start="120.36" dur="3.42">because occasionally there are equations being
referenced, visuals.</text><text start="123.78" dur="4.839">I don&apos;t know about you, but much or most,
in fact, of the podcasts that I watch, I walk</text><text start="128.619" dur="2.241">away with this feeling like I&apos;ve learned something,</text><text start="130.86" dur="3.92">but I actually haven&apos;t, and the next day if
you ask me to recall, I wouldn&apos;t be able to</text><text start="134.78" dur="1.0">recall much of it.</text><text start="135.78" dur="4.951">That means that they&apos;re great for being entertaining
and feeling like I&apos;m learning something, that</text><text start="140.731" dur="1.989">is the feelings of productivity,</text><text start="142.72" dur="4.41">but if I actually want to deep dive into a
subject matter, it seems to fail at that,</text><text start="147.13" dur="1.0">at least for myself.</text><text start="148.13" dur="4.32">Therefore, I&apos;m attempting to solve that by
working with the interviewee, for instance,</text><text start="152.45" dur="1.22">we worked with Daniel,</text><text start="153.67" dur="4.41">to making this episode and any episode that
comes out from season 3 onward, from this</text><text start="158.08" dur="1.0">point onward,</text><text start="159.08" dur="6.03">to make it not only a fantastic podcast, but
perhaps in this small, humble way to evolve</text><text start="165.11" dur="1.59">what a podcast could be.</text><text start="166.7" dur="4.22">You may not know this, but in addition to
math and physics, my background is in filmmaking,</text><text start="170.92" dur="3.8">so I know how powerful certain techniques
can be with regards to elucidation,</text><text start="174.72" dur="3.89">how the difference between making a cut here
or making a cut here can be the difference</text><text start="178.61" dur="2.86">between you absorbing a lesson or it being
forgotten.</text><text start="181.47" dur="5.06">By the way, my name is Curt Jaimungal, and
this is a podcast called Theories of Everything,</text><text start="186.53" dur="4.97">dedicated to investigating the versicolored
terrain of theories of everything, primarily</text><text start="191.5" dur="2.28">from a theoretical physics perspective,</text><text start="193.78" dur="4.97">but also venturing beyond that to hopefully
understand what the heck is fundamental reality,</text><text start="198.75" dur="3.23">get closer to it, can you do so, is there
a fundamental reality,</text><text start="201.98" dur="3.68">is it fundamental, because even the word fundamental
has certain presumptions in it.</text><text start="205.66" dur="4.24">I&apos;m going to use almost everything from my
filmmaking background and my mathematical</text><text start="209.9" dur="4.399">background to make TOE the deepest dive, not
only with the guest,</text><text start="214.299" dur="3.671">but we&apos;d like it to be the deepest dive on
the subject matter that the guest is speaking</text><text start="217.97" dur="1.0">about.</text><text start="218.97" dur="3.71">It&apos;s so supplementary that it&apos;s best to call
it complementary, as the aim is to achieve</text><text start="222.68" dur="2.47">so much that there&apos;s no fat, there&apos;s just
meat.</text><text start="225.15" dur="1.85">It&apos;s all substantive, that&apos;s the goal.</text><text start="227.0" dur="4.34">Now, there&apos;s some necessary infrastructure
of concepts to be explicated prior in order</text><text start="231.34" dur="2.739">to gain the most from this conversation with
Daniel,</text><text start="234.079" dur="2.091">so I&apos;ll attempt to outline when needed.</text><text start="236.17" dur="4.17">Again, timestamps are in the description,
so you can go at your own pace, you can revisit</text><text start="240.34" dur="1.0">sections.</text><text start="241.34" dur="3.25">There will also be announcements throughout
and especially at the end of this video, so</text><text start="244.59" dur="1.0">stay tuned.</text><text start="245.59" dur="4.399">Now, Daniel Schmattenberger is a systems thinker,
which is different than reductionism, primarily</text><text start="249.989" dur="1.261">in its focus.</text><text start="251.25" dur="5.22">So systems thinkers think about the interactions,
the end-to-or-greater interactions, the second</text><text start="256.47" dur="1.13">order or third order.</text><text start="257.6" dur="4.349">And Daniel, in this conversation, is constantly
referring to the interconnectivity of systems</text><text start="261.949" dur="2.761">and the potential for unintended consequences.</text><text start="264.71" dur="2.929">We also talk about the risks associated with
AI.</text><text start="267.639" dur="3.101">We also talk about their boons, because that&apos;s
often overlooked.</text><text start="270.74" dur="3.04">Plenty of alarmist talk is on this subject.</text><text start="273.78" dur="3.759">When talking about the risks, we&apos;re mainly
talking about its alignment or misalignment</text><text start="277.539" dur="1.0">with human values.</text><text start="278.539" dur="4.561">We also talk about why each route, even if
it&apos;s aligned, isn&apos;t exactly salutary.</text><text start="283.1" dur="4.409">About a third of the way through, Daniel begins
to advocate for a cooperative orientation</text><text start="287.509" dur="1.401">in AI development,</text><text start="288.91" dur="4.259">where the focus is on ensuring that AI systems
are designed to benefit and that there are</text><text start="293.169" dur="1.0">safeguards placed in,</text><text start="294.169" dur="1.461">much like any other technology.</text><text start="295.63" dur="3.7">You can think about this in terms of a tweet,
a recent tweet by Rob Miles, which says,</text><text start="299.33" dur="3.78">It&apos;s not that hard to go to the moon, but
in worlds that manage it, saying that these</text><text start="303.11" dur="4.25">astronauts will probably die is responded
with a detailed technical plan showing all</text><text start="307.36" dur="3.54">the fail-safes, testings, and procedures that
are in place.</text><text start="310.9" dur="3.62">They&apos;re not met with, hey, wow, what an extraordinarily
speculative claim.</text><text start="314.52" dur="5.02">Now, this cooperative orientation resonates
with the concept of Nash equilibrium.</text><text start="319.54" dur="6.37">A Nash equilibrium occurs when all players
choose their optimal strategy given their</text><text start="325.91" dur="2.95">beliefs about other people&apos;s strategies,</text><text start="328.86" dur="4.92">such that no one player can benefit from altering
their strategy.</text><text start="333.78" dur="3.15">Now, that was fairly abstract, so let me give
an instance.</text><text start="336.93" dur="3.739">There&apos;s rock, paper, scissors, and you may
think, hey, how the heck can you choose an</text><text start="340.669" dur="2.241">optimal strategy in this random game?</text><text start="342.91" dur="1.12">Well, that&apos;s the answer.</text><text start="344.03" dur="4.94">It&apos;s actually to be random, so a one-third
chance of being rock or paper or scissors.</text><text start="348.97" dur="3.0">And you can see this because if you were to
choose, let&apos;s say, one-half chance of being</text><text start="351.97" dur="1.0">rock,</text><text start="352.97" dur="2.729">well, then a player can beat you one-half
of the time by choosing their strategy to</text><text start="355.699" dur="1.0">be paper,</text><text start="356.699" dur="4.131">and then that means that you can improve your
strategy by choosing something else.</text><text start="360.83" dur="4.74">In game theory, a move is something that you
do at a particular point in the game or it&apos;s</text><text start="365.57" dur="1.34">a decision that you make.</text><text start="366.91" dur="4.659">For instance, in this game, you can reveal
a card, you can draw a card, you can relocate</text><text start="371.569" dur="2.13">a chip from one place to another.</text><text start="373.699" dur="4.821">Moves are the building blocks of games, and
each player makes a move individually in response</text><text start="378.52" dur="1.41">to what you do or what you don&apos;t do</text><text start="379.93" dur="3.95">or in response to something that they&apos;re thinking,
a strategy, for instance.</text><text start="383.88" dur="5.289">A strategy is a complete plan of action that
you employ throughout the game.</text><text start="389.169" dur="6.041">A strategy is your response to all possible
situations, all situations that can be thrown</text><text start="395.21" dur="1.0">your way.</text><text start="396.21" dur="3.46">And by the way, that&apos;s what this upside-down,
funny-looking symbol is.</text><text start="399.67" dur="2.589">This means for all in math and in logic.</text><text start="402.259" dur="4.521">It&apos;s a comprehensive guide that dictates the
actions you take in response to the players</text><text start="406.78" dur="4.609">you cooperate with and also the players that
you don&apos;t.</text><text start="411.389" dur="4.75">A common misconception about Nash Equilibria
is that they result in the best possible outcome</text><text start="416.139" dur="1.191">for all players.</text><text start="417.33" dur="3.48">Actually, most often, they&apos;re suboptimal for
each player.</text><text start="420.81" dur="1.75">They also have social inefficiencies.</text><text start="422.56" dur="2.27">For instance, the infamous prisoner&apos;s dilemma.</text><text start="424.83" dur="4.799">Now, this relates to AI systems, and as Daniel
talks about, this has significant implications</text><text start="429.629" dur="1.0">for AI risk.</text><text start="430.629" dur="3.97">Do we know if AI systems will adopt cooperative
or uncooperative strategies?</text><text start="434.599" dur="3.301">How desirable or undesirable will those outcomes
be?</text><text start="437.9" dur="1.92">What about the nation states that possess
them?</text><text start="439.82" dur="4.2">Will it be ordered and positive, or will it
be chaotic and ataxic, like the intersection</text><text start="444.02" dur="1.0">behind me?</text><text start="445.02" dur="2.56">Although it&apos;s fairly ordered right now, it&apos;s
usually not like this.</text><text start="447.58" dur="5.7">The stability of a Nash Equilibrium refers
to its robustness in face of small changes,</text><text start="453.28" dur="2.21">perturbations, and payoffs or strategies.</text><text start="455.49" dur="4.519">An unstable Nash Equilibrium can collapse
under slight perturbations, leading to shifts</text><text start="460.009" dur="4.051">in player strategies, and then consequently
a new Nash Equilibrium.</text><text start="464.06" dur="6.65">In the case of AI risk, an unstable Nash Equilibrium
could result in rapid and extreme harmful</text><text start="470.71" dur="3.579">oscillations in AI behavior as they compete
for dominance.</text><text start="474.289" dur="4.31">And by the way, this isn&apos;t including that
an AI itself may be fractionated in the way</text><text start="478.599" dur="1.621">that we are as people,</text><text start="480.22" dur="7.42">with several selves inside us vying for control
in a Jungian manner.</text><text start="487.64" dur="3.5">Generalizations also have a huge role in understanding
complex systems.</text><text start="491.14" dur="3.16">So what occurs is you take some concept, and
then you list out some conditions, and then</text><text start="494.3" dur="2.26">you relax some of those conditions.</text><text start="496.56" dur="1.34">You abstract away.</text><text start="497.9" dur="4.35">Through the recognition of certain recurring
patterns, we can construct frameworks, we</text><text start="502.25" dur="1.34">can hypothesize,</text><text start="503.59" dur="4.549">such that hopefully it captures not only this
phenomenon, but a diverse array of phenomenon.</text><text start="508.139" dur="3.671">The themes of theories of everything of this
channel is what is fundamental reality,</text><text start="511.81" dur="3.69">and like I mentioned, we generally explore
that from a theoretical physics perspective,</text><text start="515.5" dur="3.2">but we also abstract out and think, well,
what is consciousness?</text><text start="518.7" dur="1.1">Does that arise from material?</text><text start="519.8" dur="3.02">Does it have a relationship to what&apos;s fundamental
reality?</text><text start="522.82" dur="1.0">What about philosophy?</text><text start="523.82" dur="1.66">What does that have to say in metaphysics?</text><text start="525.48" dur="5.08">So that is, generalizations empower prognostication,
the discerning of patterns,</text><text start="530.56" dur="3.72">and they streamline our examination of the
environment that we seem to be embedded in.</text><text start="534.28" dur="5.19">Now, in the realm of quantum mechanics, generalizations
take on a specific significance.</text><text start="539.47" dur="3.72">Now, given that we talk about probability
and uncertainty,</text><text start="543.19" dur="3.61">both in these videos, which you&apos;re seeing
on screen now, and in this conversation with</text><text start="546.8" dur="1.0">Daniel,</text><text start="547.8" dur="4.82">thus it&apos;s fruitful to explore one powerful
generalization of probabilities</text><text start="552.62" dur="9.52">that bridges classical mechanics with quantum
theory called quasi-probability distributions.</text><text start="562.14" dur="7.36">Born in the early days of quantum mechanics,
a quasi-probability distribution,</text><text start="569.5" dur="4.39">also known as a QPD, bridges between classical
and quantum theories.</text><text start="573.89" dur="3.05">There&apos;s this guy named Eugene Wigner, who
around 1932,</text><text start="576.94" dur="5.07">published his paper on the quantum corrections
of thermodynamic equilibriums,</text><text start="582.01" dur="2.5">which introduces the Wigner function.</text><text start="584.51" dur="5.41">What&apos;s notable here is that both position
and momentum appear in this analog to the</text><text start="589.92" dur="1.0">wave function,</text><text start="590.92" dur="4.58">when ordinarily you choose to work in the
so-called momentum space, or position space,</text><text start="595.5" dur="1.0">but not both.</text><text start="596.5" dur="3.88">To better grasp the concept, think of quasi-probability
distributions</text><text start="600.38" dur="5.37">as maps that encode quantum features into
classical-like probability distributions.</text><text start="605.75" dur="4.09">Whenever you hear the suffix &quot;-like&quot;, you
should immediately be skeptical,</text><text start="609.84" dur="3.84">as space-like isn&apos;t space, and time-like isn&apos;t
the same thing as time.</text><text start="613.68" dur="3.75">In this instance, classical-like isn&apos;t classical.</text><text start="617.43" dur="3.45">There&apos;s something called the Kalmogorov axioms
of probability,</text><text start="620.88" dur="3.209">and some of them are relaxed in these quasi-probability
distributions.</text><text start="624.089" dur="2.74">For instance, you&apos;re allowed negative probabilities.</text><text start="626.829" dur="4.421">They also don&apos;t have to sum up to one, and
doing so with the Wigner function</text><text start="631.25" dur="4.19">reveals some of the more peculiar aspects
of quantum theory, like superposition and</text><text start="635.44" dur="1.19">entanglement.</text><text start="636.63" dur="5.68">The development of QPDs expanded with the
Glauber-Sedartian p-representation,</text><text start="642.31" dur="6.16">introduced by Sedartian in 1963, and refined
by Glauber and Houssoumis q-representation</text><text start="648.47" dur="1.13">in 1940.</text><text start="649.6" dur="3.5">QPDs play a crucial role in quantum tomography,</text><text start="653.1" dur="4.08">which allow us to reconstruct and characterize
unknown quantum states.</text><text start="657.18" dur="3.56">They also maintain their invariance under
symplectic transformations,</text><text start="660.74" dur="2.38">preserving the structure of phase-space dynamics.</text><text start="663.12" dur="4.959">You can think of this as preserving the areas
of parallelograms formed by vectors in phase-space.</text><text start="668.079" dur="4.421">Nowadays, QPDs have ventured beyond the quantum
realm,</text><text start="672.5" dur="3.6">inspiring advancements in machine learning
and artificial intelligence.</text><text start="676.1" dur="3.729">This is called quantum machine learning, and
while it&apos;s in its infancy,</text><text start="679.829" dur="3.25">it may be that the next breakthrough in lowering
compute</text><text start="683.079" dur="3.431">lies with these kernel methods and quantum
variational encoders.</text><text start="686.51" dur="3.43">By leveraging QPDs in place of density matrices,</text><text start="689.94" dur="4.76">researchers gain the ability to study quantum
processes with reduced computational complexity.</text><text start="694.7" dur="5.51">For instance, QPDs have been employed to create
quantum-inspired optimization algorithms,</text><text start="700.21" dur="3.31">like the quantum-inspired genetic algorithm,
QGA,</text><text start="703.52" dur="5.64">which incorporates quantum superposition to
enhance search and optimization processes.</text><text start="709.16" dur="2.869">Quantum variational autoencoders can be used
for tasks such as</text><text start="712.029" dur="3.151">quantum states compression and quantum generative
models,</text><text start="715.18" dur="1.81">also quantum error mitigation.</text><text start="716.99" dur="4.96">The whole point of this is that there are
new techniques being developed daily,</text><text start="721.95" dur="4.02">and unlike the incremental change of the past,
there&apos;s a probability, a low one but it&apos;s</text><text start="725.97" dur="1.0">non-zero,</text><text start="726.97" dur="6.05">that one of these will remarkably and irrevocably
change the landscape of technology.</text><text start="733.02" dur="2.11">So, generalizations are important.</text><text start="735.13" dur="3.149">For instance, spin and GR, so general relativity,</text><text start="738.279" dur="3.741">is known to be the only theory that&apos;s consistent
with being Lorentz invariant,</text><text start="742.02" dur="2.8">having an interaction, and being spin-2, something
called spin-2.</text><text start="744.82" dur="4.509">This means if you have a field and it&apos;s spin-2
and it&apos;s not free, so there&apos;s interactions,</text><text start="749.329" dur="4.44">and it&apos;s Lorentz invariant, then general relativity
pops out, meaning you get it as a result.</text><text start="753.769" dur="2.971">Now, this interacting aspect is important,
because if you have a scalar,</text><text start="756.74" dur="5.899">so if you have a spin-0 field, then what happens
is it couples to the trace of the energy momentum</text><text start="762.639" dur="1.0">tensor,</text><text start="763.639" dur="1.0">because there&apos;s nothing else for it to couple
to,</text><text start="764.639" dur="3.111">and it turns out that does reproduce Newton&apos;s
law of gravity.</text><text start="767.75" dur="3.55">However, as soon as you add an interacting
relativistic matter,</text><text start="771.3" dur="1.47">then you don&apos;t get that light bends.</text><text start="772.77" dur="4.379">So then you think, well, let&apos;s generalize
it to spin-1, and then there are some problems</text><text start="777.149" dur="1.0">there,</text><text start="778.149" dur="1.331">and you think, well, let&apos;s generalize it to
spin-3 and above,</text><text start="779.48" dur="2.17">and there&apos;s some no-go theorems by Weinberg
there.</text><text start="781.65" dur="2.29">By the way, the problem with spin-1 is that
masses will repel,</text><text start="783.94" dur="4.079">for the same reason that in electromagnetism,
that if you have same charges, they repel.</text><text start="788.019" dur="4.551">Okay, other than just a handful of papers,
it seems like we&apos;ve covered all the necessary</text><text start="792.57" dur="1.0">ground,</text><text start="793.57" dur="3.18">and when there&apos;s more room to be covered,
I&apos;ll cover it spasmodically throughout the</text><text start="796.75" dur="1.0">podcast.</text><text start="797.75" dur="2.709">There&apos;ll be links to the papers and to the
other concepts that are explored in the description.</text><text start="800.459" dur="3.201">Most of the prep work for this conversation
seems to be out of the way,</text><text start="803.66" dur="3.71">so now, let&apos;s introduce Daniel Schmattenberger.</text><text start="807.37" dur="3.279">Welcome, valued listeners and watchers.</text><text start="810.649" dur="3.701">Today, we&apos;re honored to introduce this remarkable
guest,</text><text start="814.35" dur="4.75">an extraordinary, extraordinary thinker, who
transcends conventional boundaries,</text><text start="819.1" dur="1.0">Daniel Schmattenberger.</text><text start="820.1" dur="4.85">So, what are the underlying causes that everything
from nuclear war to environmental degradation,</text><text start="824.95" dur="2.949">to animal rights issues, to class issues,
what do these things have in common?</text><text start="827.899" dur="5.821">As a multidisciplinary aficionado, Daniel&apos;s
expertise spans complex systems theory,</text><text start="833.72" dur="5.679">evolutionary dynamics, and existential risk,
topics that challenge the forefront of academic</text><text start="839.399" dur="1.0">exploration.</text><text start="840.399" dur="4.201">Seamlessly melding different fields such as
philosophy, neuroscience, and sustainability,</text><text start="844.6" dur="5.62">he offers a comprehensive understanding of
our world&apos;s most pressing challenges.</text><text start="850.22" dur="3.77">Really, the thing we have to shift is the
economy,</text><text start="853.99" dur="2.43">because perverse economic incentive is under
the whole thing.</text><text start="856.42" dur="4.289">There&apos;s no way that as long as you have a
for-profit military-industrial complex</text><text start="860.709" dur="3.481">as the largest block of the global economy,
that you could ever have peace.</text><text start="864.19" dur="4.39">There&apos;s an anti-incentive on it as long as
there&apos;s so much money to be made with mining,</text><text start="868.58" dur="1.0">etc.</text><text start="869.58" dur="2.17">Like, we have to fix the nature of economic
incentives.</text><text start="871.75" dur="3.519">In 2018, Daniel co-founded the Consilience
Project,</text><text start="875.269" dur="4.671">a groundbreaking initiative that aims to foster
societal-wide transformation</text><text start="879.94" dur="5.3">via the synthesis of disparate domains promoting
collaboration, innovation,</text><text start="885.24" dur="2.13">as well as something we used to call wisdom.</text><text start="887.37" dur="4.55">Today&apos;s conversation delves into AI, consciousness,
and morality,</text><text start="891.92" dur="2.159">aligning with the themes of the TOE podcast.</text><text start="894.079" dur="1.57">It may challenge your beliefs.</text><text start="895.649" dur="3.38">It&apos;ll present alternative perspectives to
the AI risk scenarios,</text><text start="899.029" dur="3.851">by also outlining the positive cases which
are often overlooked.</text><text start="902.88" dur="3.78">Ultimately, Daniel offers a fresh outlook
on the interconnectedness of reality.</text><text start="906.66" dur="3.169">Say, let&apos;s get the decentralized collective
intelligence of the world</text><text start="909.829" dur="4.57">having the best frameworks for understanding
the most fundamental problems</text><text start="914.399" dur="3.371">as the center of the innovative focus of the
creativity of the world.</text><text start="917.77" dur="3.48">So, you TOE Watchers, you, my name is Curt
Jeymungel.</text><text start="921.25" dur="5.389">Prepare for a captivating journey as we explore
the peerless, enthralling world of Daniel</text><text start="926.639" dur="9.161">Schmattenberger.</text><text start="935.8" dur="9.159">Enjoy.</text><text start="944.959" dur="3.411">I do not know with what weapons World War
III will be fought,</text><text start="948.37" dur="3.48">but World War IV will be fought with sticks
and stones.</text><text start="951.85" dur="8.789">All right, Daniel, what have you been up to
in the past few years?</text><text start="960.639" dur="4.001">Past few years?</text><text start="964.64" dur="11.49">Trying to understand the unfolding global
situation</text><text start="976.13" dur="7.91">and the trajectories towards existential and
global catastrophic risk in particular,</text><text start="984.04" dur="6.24">the solutions to those that involve control
mechanisms that create trajectories towards</text><text start="990.28" dur="2.739">dystopias,</text><text start="993.019" dur="6.531">and the consideration of what a world that
is neither in the attractor basin of catastrophe</text><text start="999.55" dur="1.0">or dystopia</text><text start="1000.55" dur="3.5">looks like, a kind of third attractor.</text><text start="1004.05" dur="5.64">What would it take to have a civilization
that could steward the power of exponential</text><text start="1009.69" dur="1.37">technology</text><text start="1011.06" dur="3.14">much better than we have stewarded all of
our previous technological power?</text><text start="1014.2" dur="5.24">What would that mean in terms of culture and
in terms of political economies and governance</text><text start="1019.44" dur="1.43">and things like that?</text><text start="1020.87" dur="9.829">So, thinking about those things and acting
on specific cases of near-term catastrophic</text><text start="1030.699" dur="1.0">risks</text><text start="1031.699" dur="5.061">that we were hoping to ameliorate and helping
with various projects on how to transition</text><text start="1036.76" dur="3.25">institutions to be more intelligent and things
like that.</text><text start="1040.01" dur="4.4">What are some of these near-term catastrophic
risks?</text><text start="1044.41" dur="12.97">Well, as of today, we are in a war that has
moved the atomic clock closer to midnight</text><text start="1057.38" dur="2.919">than it has ever been.</text><text start="1060.299" dur="4.001">And that&apos;s a pretty obvious one.</text><text start="1064.3" dur="12.16">And if we were to write a book about the folly
of the history of human hubris,</text><text start="1076.46" dur="4.62">we would get very concerned about where we
are confident, about where we&apos;re right, where</text><text start="1081.08" dur="2.81">we might actually be wrong, and the consequences
of it.</text><text start="1083.89" dur="4.149">And as we&apos;re dealing with nukes and AI and
things like that, we could easily have the</text><text start="1088.039" dur="1.411">last chapter in that book.</text><text start="1089.45" dur="6.5">If we are not more careful about confident,
wrong ideas.</text><text start="1095.95" dur="7.37">So, what are all the assumptions in the way
we&apos;re navigating that particular conflict</text><text start="1103.32" dur="2.42">that might not be right?</text><text start="1105.74" dur="5.02">What are the ways we are modeling the various
sides?</text><text start="1110.76" dur="4.02">And what would an end state that is viable
for the world and that just at minimum doesn&apos;t</text><text start="1114.78" dur="2.05">go to a global catastrophic risk?</text><text start="1116.83" dur="1.8">That&apos;s an example.</text><text start="1118.63" dur="7.919">If we look at the domain of synthetic biology
as a different kind of advanced technology,</text><text start="1126.549" dur="2.271">exponential tech,</text><text start="1128.82" dur="8.5">and we look at that the cost of things like
gene sequencing and then the ability to synthesize</text><text start="1137.32" dur="5.87">genomes, gene printing, are dropping faster
than Moore&apos;s law in cost.</text><text start="1143.19" dur="9.82">Well, open science means that the most virulent
viruses possible studied in context that have</text><text start="1153.01" dur="2.919">ethical review boards getting open published,</text><text start="1155.929" dur="6.031">then that&apos;s a situation where that knowledge
combined with near-term decentralized gene</text><text start="1161.96" dur="6.2">printers is decentralized catastrophe weapons
on purpose or even accidentally.</text><text start="1168.16" dur="5.7">There are heaps of examples in the environmental
space if we look at our planetary boundaries.</text><text start="1173.86" dur="3.6">Climate change is the one people have the
most awareness of publicly.</text><text start="1177.46" dur="5.76">But if you look at the other planetary boundaries
like mining pollution or chemical pollution</text><text start="1183.22" dur="7.27">or nitrogen dead zones in oceans or biodiversity
loss or species extinction,</text><text start="1190.49" dur="2.39">we&apos;ve already passed certain tipping points.</text><text start="1192.88" dur="5.46">The question is how runaway are those effects?</text><text start="1198.34" dur="5.77">There was an article published a few months
ago on PFOS and PFAS, the fluorinated surfactants,</text><text start="1204.11" dur="2.39">forever chemicals as they&apos;re popularly called,</text><text start="1206.5" dur="5.86">that found higher than EPA allowable standards
of them in rainwater all around the world,</text><text start="1212.36" dur="5.02">including in snowfall in Antarctica because
they actually evaporate.</text><text start="1217.38" dur="5.56">And we&apos;re not slowing down on the production
of those in their endocrine disruptors and</text><text start="1222.94" dur="1.0">carcinogens.</text><text start="1223.94" dur="4.7">And that doesn&apos;t just affect humans, but affects
things like the entirety of ecology and soil</text><text start="1228.64" dur="1.0">microorganisms.</text><text start="1229.64" dur="2.13">It&apos;s kind of a humongous effect.</text><text start="1231.77" dur="2.02">So those are all examples.</text><text start="1233.79" dur="5.09">And I would say right now I know the topic
of our conversation today is AI.</text><text start="1238.88" dur="9.96">AI is both a novel example of a possible catastrophic
risk through certain types of utilization.</text><text start="1248.84" dur="4.36">It is also an accelerant to every category
of catastrophic risk potentially.</text><text start="1253.2" dur="3.19">So that one has a lot of attention at the
moment.</text><text start="1256.39" dur="3.88">So that makes AI different than the rest that
you&apos;ve mentioned?</text><text start="1260.27" dur="1.19">Definitely.</text><text start="1261.46" dur="5.27">And are you focused primarily on avoiding
disaster or moving towards something that&apos;s</text><text start="1266.73" dur="6.03">much more heavenly or positive, like a Shangri-La?</text><text start="1272.76" dur="4.63">So we have an assessment called the metacrisis.</text><text start="1277.39" dur="3.12">There&apos;s a more popular term out there right
now, the polycrisis.</text><text start="1280.51" dur="4.3">We&apos;ve been calling this the metacrisis since
before coming across that term.</text><text start="1284.81" dur="5.249">Polycrisis is the idea that the global catastrophic
risk that we all need to focus on and coordinate</text><text start="1290.059" dur="6.12">on is not just climate change and is not just
wealth inequality and is not just kind of</text><text start="1296.179" dur="5.271">the breakdown of Pax Americana and the possibility
of war or these species extinction issues,</text><text start="1301.45" dur="1.0">but it&apos;s lots of things.</text><text start="1302.45" dur="3.24">There&apos;s lots of different global catastrophic
risks.</text><text start="1305.69" dur="3.06">And that they interact with each other and
they&apos;re complicated and there can even be</text><text start="1308.75" dur="2.11">cascades between them, right?</text><text start="1310.86" dur="6.55">We don&apos;t have to have climate change produce
total Venusification of the earth to produce</text><text start="1317.41" dur="1.389">a global catastrophic risk.</text><text start="1318.799" dur="4.591">It just has to increase the likelihood of
extreme weather events in an area.</text><text start="1323.39" dur="1.89">And we&apos;ve already seen that happening.</text><text start="1325.28" dur="2.05">Statistics on that seem quite clear.</text><text start="1327.33" dur="5.94">And it&apos;s not just total climate change, deforestation
affecting local transpiration and heat in</text><text start="1333.27" dur="4.94">an area can have an effect on and total amount
of pavement laid and whatever can have an</text><text start="1338.21" dur="1.79">effect on extreme weather events.</text><text start="1340.0" dur="4.77">But extreme weather events, I mean, we saw
what happened to Australia a couple years</text><text start="1344.77" dur="4.81">ago when a significant percentage of a whole
continent burned in a way that we don&apos;t have</text><text start="1349.58" dur="2.75">near-term historical precedent for.</text><text start="1352.33" dur="2.64">We saw the way that droughts affected.</text><text start="1354.97" dur="6.1">The migration that led to the whole Syrian
conflict that got very close to a much larger-scale</text><text start="1361.07" dur="1.18">conflict.</text><text start="1362.25" dur="5.48">The Australia situation happened to hit a
low population density area, but there are</text><text start="1367.73" dur="6.01">plenty of high population density areas that
are getting very near the temperatures that</text><text start="1373.74" dur="4.96">create total crop failures, whether we&apos;re
talking about India, Pakistan, Bangladesh,</text><text start="1378.7" dur="1.79">Nigeria, Iran.</text><text start="1380.49" dur="4.36">And so if you have massive human migration,
the UN currently predicts hundreds of millions</text><text start="1384.85" dur="6.7">of climate-mediated migrants in the next decade
and a half, then it&apos;s pretty easy under those</text><text start="1391.55" dur="2.53">situations to have resource wars.</text><text start="1394.08" dur="4.14">And those can hit existing political fault
lines and then technological amplification.</text><text start="1398.22" dur="4.38">And so in the past, we obviously had a lot
less people.</text><text start="1402.6" dur="3.199">We only had half a billion people for the
entirety of the history of the world until</text><text start="1405.799" dur="2.321">the Industrial Revolution.</text><text start="1408.12" dur="6.13">And then with the Green Revolution and nitrogen
fertilizer and oil and like that, we went</text><text start="1414.25" dur="7.84">from half a billion people to 8 billion people
overnight in historical timelines.</text><text start="1422.09" dur="4.81">And we went from those people mostly living
on local subsistence to almost all living</text><text start="1426.9" dur="6.48">on dependent upon very complicated supply
chains now that are six-continent-mediated</text><text start="1433.38" dur="1.0">supply chains.</text><text start="1434.38" dur="5.58">So that means that there&apos;s radically more
fragility in the life support systems so that</text><text start="1439.96" dur="3.922">local catastrophes can turn to breakdowns
of supply chains, economic effects, et cetera,</text><text start="1443.882" dur="1.998">that affect people very widely.</text><text start="1445.88" dur="5.89">So polycrisis kind of looking at all that,
metacrisis adds looking at the underlying</text><text start="1451.77" dur="1.71">drivers of all of them.</text><text start="1453.48" dur="3.92">Why do we have all of these issues?</text><text start="1457.4" dur="4.11">And what would it take to solve them, not
just on a point-by-point basis, but to solve</text><text start="1461.51" dur="1.01">the underlying basis?</text><text start="1462.52" dur="4.36">So we can see that all of these have to do
with coordination failures.</text><text start="1466.88" dur="4.63">We can see that underneath all of them, there
are things like perverse economic interests.</text><text start="1471.51" dur="4.59">If the cost of the environmental pollution
to clean it up was something where in the</text><text start="1476.1" dur="5.49">process of the corporation selling the PFAS
as a surfactant for waterproofing clothes</text><text start="1481.59" dur="4.151">or whatever, it also had to pay for the cost
to clean up its effect in the environment</text><text start="1485.741" dur="4.329">or the oil cost had to clean up the effect
on the environment.</text><text start="1490.07" dur="4.45">So you didn&apos;t have the perverse incentive
to externalize costs onto nature&apos;s balance</text><text start="1494.52" dur="1.529">sheet, which nobody enforces.</text><text start="1496.049" dur="3.641">Obviously, we&apos;d have none of those environmental
issues, right?</text><text start="1499.69" dur="1.89">That would be a totally different situation.</text><text start="1501.58" dur="5.219">So can we address perverse incentive writ
large that would require fundamental changes</text><text start="1506.799" dur="3.651">in what we think of as economy and how we
enact that, so political economy?</text><text start="1510.45" dur="2.579">So we think about those things.</text><text start="1513.029" dur="5.831">So I would say with the metacrisis assessment,
we would say that we&apos;re in a very novel position</text><text start="1518.86" dur="7.011">with regard to catastrophic risk, global catastrophic
risk, because until World War II, there was</text><text start="1525.871" dur="5.058">no technology big enough to cause a global
catastrophic risk as a result of dumb human</text><text start="1530.929" dur="2.021">choices or human failure quickly.</text><text start="1532.95" dur="1.0">And then with the bomb, there was.</text><text start="1533.95" dur="1.23">It was the beginning.</text><text start="1535.18" dur="6.42">And that&apos;s a moment ago in evolutionary time,
right?</text><text start="1541.6" dur="4.96">And if we reverse back a little bit before
the bomb, until the Industrial Revolution,</text><text start="1546.56" dur="5.22">we didn&apos;t have any technology that could have
caused global catastrophic risk even cumulatively.</text><text start="1551.78" dur="4.29">The industrial technology, extracting stuff
from nature and turning it into human stuff</text><text start="1556.07" dur="3.66">for a little while before turning it into
pollution and trash, where we&apos;re extracting</text><text start="1559.73" dur="3.92">stuff from nature in ways that destroy the
environment faster than nature can replenish</text><text start="1563.65" dur="3.6">it and turning it into trash and pollution
faster than it can be processed and doing</text><text start="1567.25" dur="4.46">exponentially more of that because it&apos;s coupled
to an economy that requires exponential growth</text><text start="1571.71" dur="2.719">to keep up with interest.</text><text start="1574.429" dur="2.511">That creates an existential risk.</text><text start="1576.94" dur="4.609">It creates a catastrophic risk within about
a few centuries of cumulative effects.</text><text start="1581.549" dur="3.38">And we&apos;re basically at that few-century point.</text><text start="1584.929" dur="1.861">And so that&apos;s very new.</text><text start="1586.79" dur="4.78">All of our historical systems for that, you
know, our historical systems for thinking</text><text start="1591.57" dur="2.66">about governance in the world didn&apos;t have
to deal with those effects.</text><text start="1594.23" dur="5.04">We could just kind of think about the world
as inexhaustible.</text><text start="1599.27" dur="3.43">And then, of course, when we got the bomb,
we&apos;re like, all right, this is the first</text><text start="1602.7" dur="4.15">technology that rather than racing to implement,
we have to ensure that no one ever uses.</text><text start="1606.85" dur="2.83">In all previous technologies, there was a
race to implement it.</text><text start="1609.68" dur="2.099">It was a very different situation.</text><text start="1611.779" dur="6.251">But since that time, a lot more catastrophic
technologies have emerged, catastrophic technologies</text><text start="1618.03" dur="6.11">in terms of applications of AI and synthetic
biology and cyber and various things that</text><text start="1624.14" dur="2.57">are way easier to build the nukes and way
harder to control.</text><text start="1626.71" dur="5.13">When you have many actors that have access
to many different types of catastrophic technology</text><text start="1631.84" dur="3.339">that can&apos;t be monitored, you don&apos;t get mutually
assured destruction and those types</text><text start="1635.179" dur="1.0">of safeties.</text><text start="1636.179" dur="5.701">So we&apos;d say that we&apos;re in a situation where
the catastrophic risk landscape is novel.</text><text start="1641.88" dur="2.98">Nothing in history has been anything like
it.</text><text start="1644.86" dur="3.37">And the current trajectory doesn&apos;t look awesome
for making it through.</text><text start="1648.23" dur="6.75">What it would take to make it through actually
requires change to those underlying coordination</text><text start="1654.98" dur="3.1">structures of humanity very deeply.</text><text start="1658.08" dur="3.439">So I don&apos;t see a model where we do make it
through those.</text><text start="1661.519" dur="3.621">It doesn&apos;t also become a whole lot more awesome.</text><text start="1665.14" dur="5.16">And that&apos;s why we say the only other example
is to control for catastrophes, you can try</text><text start="1670.3" dur="2.05">to put very strong control provisions.</text><text start="1672.35" dur="6.1">Okay, so now, unlike in the past, people could
or pretty soon have gene drives where they</text><text start="1678.45" dur="4.479">could build pandemic weapons in their basement
or drone weapons where they could take out</text><text start="1682.929" dur="3.941">infrastructure targets or now AI weapons even
easier.</text><text start="1686.87" dur="1.02">We can&apos;t let that happen.</text><text start="1687.89" dur="2.86">So we need ubiquitous surveillance to know
what everybody&apos;s doing in their basement,</text><text start="1690.75" dur="3.559">because if we don&apos;t, then the world is unacceptably
fragile.</text><text start="1694.309" dur="2.391">So we can see catastrophes or dystopias, right?</text><text start="1696.7" dur="4.09">Because most versions of ubiquitous surveillance
are pretty terrible.</text><text start="1700.79" dur="6.37">And so if you can control decentralized action,
if you don&apos;t control decentralized action,</text><text start="1707.16" dur="3.85">the current decentralized action is moving
towards planetary boundaries and conflict</text><text start="1711.01" dur="1.47">and etc.</text><text start="1712.48" dur="5.42">If you control it, then who, what are the
checks and balances on that control?</text><text start="1717.9" dur="3.56">Sorry, what do you mean control decentralized
actions?</text><text start="1721.46" dur="5.449">So when we look at what is what causes catastrophe,
so when we&apos;re talking about environmental</text><text start="1726.909" dur="4.041">issues, there&apos;s not one group that is taking
all the fish out of the ocean, or causing</text><text start="1730.95" dur="6.05">species extinction or doing all the pollution,
there&apos;s a decentralized incentive that lots</text><text start="1737.0" dur="3.25">of companies share to do those things.</text><text start="1740.25" dur="3.909">So nobody&apos;s intentionally trying to remove
all the fish from the ocean, they&apos;re trying</text><text start="1744.159" dur="5.9">to meet an economic incentive that they have
that&apos;s associated with fishing, but the cumulative</text><text start="1750.059" dur="3.221">effect of that is overfishing the ocean, right?</text><text start="1753.28" dur="4.66">So if you try to – if there&apos;s a decentralized
set of activity where the lack of coordination</text><text start="1757.94" dur="4.81">of everybody doing that, everybody pursuing
their own near-term optimum creates a shitty</text><text start="1762.75" dur="3.14">term global minimum for everybody, right?</text><text start="1765.89" dur="2.8">A long-term bad outcome for everybody.</text><text start="1768.69" dur="4.619">If you try to create some centralized control
against that, that&apos;s a lot of centralized</text><text start="1773.309" dur="1.941">power.</text><text start="1775.25" dur="2.63">And where are the checks and balances on that
power?</text><text start="1777.88" dur="4.34">Otherwise, how do you create decentralized
coordination?</text><text start="1782.22" dur="6.67">And similarly, if you&apos;re looking at things
like in an age where terrorism can get exponential</text><text start="1788.89" dur="6.75">technologies and you don&apos;t want exponentially
empowered terrorism with catastrophe weapons</text><text start="1795.64" dur="6.43">for everyone, to be able to see what&apos;s being
developed ahead of time, does that look like</text><text start="1802.07" dur="3.92">a degree of surveillance that nobody wants
to be able to control those things not happening,</text><text start="1805.99" dur="1.0">right?</text><text start="1806.99" dur="1.0">Do you know what I mean?</text><text start="1807.99" dur="3.49">So if you – how to prevent the catastrophes
if the catastrophes are currently the result</text><text start="1811.48" dur="6.97">of the human motivational landscape in a decentralized
way, if the solution is a centralized method</text><text start="1818.45" dur="4.16">powerful enough to do it, where are the checks
and balances on that power?</text><text start="1822.61" dur="7.71">So a future that is neither cascading catastrophes
nor controlled dystopias is the one that we&apos;re</text><text start="1830.32" dur="1.0">interested in.</text><text start="1831.32" dur="5.56">And so, yes, I would say the whole focus is
that this is now AI comes back into the topic</text><text start="1836.88" dur="4.09">because a lot of people see possibilities
for a very pro-topian future with AI where</text><text start="1840.97" dur="4.96">it can help solve coordination issues and
solve lots of resource allocation issues.</text><text start="1845.93" dur="4.15">It also – and it can – it can also make
lots of things.</text><text start="1850.08" dur="1.839">The catastrophe is worse and dystopia is worse.</text><text start="1851.919" dur="5.321">It&apos;s actually kind of unique in being able
to make both of those things more powerful.</text><text start="1857.24" dur="3.74">Can you explain what you mean when you say
that the negative externalities are coupled</text><text start="1860.98" dur="3.99">to an economy that depends on exponential
growth?</text><text start="1864.97" dur="2.25">Yeah.</text><text start="1867.22" dur="9.26">So it&apos;s – if you think about it just in
a first principle way, the idea is supposed</text><text start="1876.48" dur="5.419">to be something like there are real goods
and services that people want that improve</text><text start="1881.899" dur="2.551">their life that we care about.</text><text start="1884.45" dur="5.81">And so the services might not be physical
goods directly.</text><text start="1890.26" dur="3.5">They might be things humans are doing, but
they still depend upon lots of goods, right?</text><text start="1893.76" dur="5.34">If you are going to provide a consultation
over a Zoom meeting, you have to have laptops</text><text start="1899.1" dur="6.1">and satellites and power lines and mining
and all those things.</text><text start="1905.2" dur="3.15">So you can&apos;t separate the service industry
from the goods industry.</text><text start="1908.35" dur="3.49">So there&apos;s physical stuff that we want.</text><text start="1911.84" dur="6.17">And to mediate the access to that and the
exchange of it, we think about it through</text><text start="1918.01" dur="1.0">a currency.</text><text start="1919.01" dur="2.759">So it&apos;s supposed to be that there&apos;s this physical
stuff and the currency is a way of</text><text start="1921.769" dur="3.681">being able to mediate the incentives and exchange
of it.</text><text start="1925.45" dur="3.77">But the currency starts to gain its own physics,
right?</text><text start="1929.22" dur="5.37">So we make a currency that has no intrinsic
value, that is just representative of any</text><text start="1934.59" dur="1.76">kind of value we could want.</text><text start="1936.35" dur="5.4">But the moment we do something like interest,
where we&apos;re now exponentiating the monetary</text><text start="1941.75" dur="8.09">supply independent of an actual automatic
growth of goods or services to not debase</text><text start="1949.84" dur="5.1">the value of the currency, you have to also
exponentiate the total amount of goods and</text><text start="1954.94" dur="1.839">services.</text><text start="1956.779" dur="3.051">And everybody&apos;s seen how compounding interest
works, right?</text><text start="1959.83" dur="3.02">Because you have a particular amount of interest
and then you have interest on that amount</text><text start="1962.85" dur="2.929">of interest, so you do get an exponential
curve.</text><text start="1965.779" dur="1.4">Obviously that&apos;s just the beginning.</text><text start="1967.179" dur="5.191">Financial services as a whole and all of the
dynamics where you have money making on money</text><text start="1972.37" dur="7.649">mean that you expand the monetary supply on
an exponential curve, which was based on the</text><text start="1980.019" dur="6.101">idea that there is a natural exponential curve
of population anyways and there is a natural</text><text start="1986.12" dur="1.83">growth of goods and services correlated.</text><text start="1987.95" dur="3.729">But that was true at an early part of the
curve that was supposed to be an S curve,</text><text start="1991.679" dur="1.0">right?</text><text start="1992.679" dur="4.371">So we have an exponential curve that inflects,
goes into an X curve, but we don&apos;t have the</text><text start="1997.05" dur="2.84">S curve part of the financial system planned.</text><text start="1999.89" dur="5.12">The financial system has to keep doing exponential
growth or it breaks.</text><text start="2005.01" dur="3.62">And not only is that key to the financial
system, because what does it mean to have</text><text start="2008.63" dur="1.94">a financial system without interest?</text><text start="2010.57" dur="4.05">Say it&apos;s a very deeply different system.</text><text start="2014.62" dur="5.73">That formalizing that was also key to our
solution to not have World War III, right?</text><text start="2020.35" dur="6.63">The history of the world in terms of war does
not look great, that the major empires and</text><text start="2026.98" dur="6.11">major nations don&apos;t stay out of violent conflict
with each other very long.</text><text start="2033.09" dur="4.77">And World War I was supposed to be the war
that ended all wars, but it wasn&apos;t.</text><text start="2037.86" dur="1.6">We had World War II.</text><text start="2039.46" dur="3.791">Now this one really has to be the war that
ends all major superpower wars because of</text><text start="2043.251" dur="1.0">the bomb.</text><text start="2044.251" dur="1.658">We can&apos;t do that again.</text><text start="2045.909" dur="5.76">And the primary basis of wars, one of the
primary bases had been resources, which was</text><text start="2051.669" dur="3.281">a particular empire wanted to grow and get
more stuff.</text><text start="2054.95" dur="3.0">And that meant taking it from somebody else.</text><text start="2057.95" dur="6.649">And so the idea of if we could exponentially
grow global GDP, everybody could have more</text><text start="2064.599" dur="1.601">without taking each other&apos;s stuff.</text><text start="2066.2" dur="4.129">It&apos;s so highly positive sum that we don&apos;t
have to go zero sum on war.</text><text start="2070.329" dur="4.55">So the whole post-World War II banking system,
the Bretton Woods monetary system, et cetera,</text><text start="2074.879" dur="5.131">was part of the how do we not have world war
along with mutual assured destruction, the</text><text start="2080.01" dur="4.23">UN and other international intergovernmental
organizations.</text><text start="2084.24" dur="8.34">But that let&apos;s exponentially grow the monetary
system also meant that if you have a whole</text><text start="2092.58" dur="3.569">bunch more dollars and you don&apos;t have more
goods and services, the dollars become worth</text><text start="2096.149" dur="3.111">less and it&apos;s just inflation and debasing
the currency.</text><text start="2099.26" dur="5.5">So now you have an artificial incentive to
keep growing the physical economy, which also</text><text start="2104.76" dur="6.23">means that the materials economy has to have
an exponential amount of nature getting turned</text><text start="2110.99" dur="4.589">from nature into stuff, into trash and pollution
in a linear materials economy.</text><text start="2115.579" dur="4.421">And you don&apos;t get to exponentially do that
on the finite biosphere forever.</text><text start="2120.0" dur="4.13">So the economy is tied to interest and that&apos;s
at the root of this of what you just explained,</text><text start="2124.13" dur="4.51">not at the root of every catastrophe.</text><text start="2128.64" dur="6.51">Interest is the beginning of what all of the
financial services do, but there&apos;s an embedded</text><text start="2135.15" dur="3.76">growth obligation of which interest is the
first thing you can see on the economic system.</text><text start="2138.91" dur="5.14">The embedded growth obligation that creates
exponentiation of it tied to the physical</text><text start="2144.05" dur="5.03">world where exponential curves don&apos;t get to
run forever is one of the problems.</text><text start="2149.08" dur="1.0">There are a handful.</text><text start="2150.08" dur="1.0">This is where we&apos;re thinking metacrisis.</text><text start="2151.08" dur="1.16">What are the underlying issues?</text><text start="2152.24" dur="1.24">This is one of the underlying issues.</text><text start="2153.48" dur="3.76">There&apos;s quite a few other ones that we can
look at to say if we really want to address</text><text start="2157.24" dur="2.9">the issues, we have to address it at this
level.</text><text start="2160.14" dur="3.7">What&apos;s the issue with transitioning from something
that&apos;s exponential to sub-exponential when</text><text start="2163.84" dur="4.96">it comes to the economy?</text><text start="2168.8" dur="1.74">What&apos;s the issue with it?</text><text start="2170.54" dur="11.59">Well I mean, there&apos;s a bunch of ways we could
go.</text><text start="2182.13" dur="6.27">There is an old refrain from the hippie days
that seems very obvious I think as soon as</text><text start="2188.4" dur="4.23">anyone thinks about it, which is that you
can&apos;t run an exponential growth system on</text><text start="2192.63" dur="1.959">a finite planet forever.</text><text start="2194.589" dur="3.24">That seems kind of obvious and intuitive.</text><text start="2197.829" dur="3.601">Because it&apos;s so obvious and intuitive, there&apos;s
a lot of counters to it.</text><text start="2201.43" dur="2.82">One counter is we&apos;re not going to run it on
a finite planet forever.</text><text start="2204.25" dur="4.26">We&apos;re going to become an interplanetary species,
mine asteroids, ship our ways to the sun,</text><text start="2208.51" dur="2.94">blah, blah, blah.</text><text start="2211.45" dur="6.02">I don&apos;t think that we are anywhere near close,
independent of the ethical or aesthetic argument</text><text start="2217.47" dur="5.42">on if us obliterating our planet&apos;s carrying
capacity and then exporting that to the rest</text><text start="2222.89" dur="5.06">of the universe is a good or lovely idea or
not, independent of that argument.</text><text start="2227.95" dur="7.139">The timelines by which that could actually
meet the humanity superorganism growing needs</text><text start="2235.089" dur="4.601">relative to the timelines where this thing
starts failing don&apos;t work.</text><text start="2239.69" dur="2.829">So that&apos;s not an answer.</text><text start="2242.519" dur="4.951">That said, the attempt to even try to get
there quicker is a utilization of resources</text><text start="2247.47" dur="7.13">here that is speeding up the breakdown here
faster than it is providing alternatives.</text><text start="2254.6" dur="3.99">The other answer people have to why there
could be exponential growth forever is because</text><text start="2258.59" dur="3.16">digital, right?</text><text start="2261.75" dur="8.869">That more and more money is a result of software
being created, a result of digital entertainment</text><text start="2270.619" dur="5.211">being created, and that there&apos;s a lot less
physical impact of that.</text><text start="2275.83" dur="7.15">And so we can keep growing digital goods because
it doesn&apos;t affect the physical plan and physical</text><text start="2282.98" dur="1.119">supply chain.</text><text start="2284.099" dur="2.161">So we can keep the exponential growth up forever.</text><text start="2286.26" dur="5.22">That&apos;s very much the kind of Silicon Valley
take on it.</text><text start="2291.48" dur="2.26">Of course, that has an effect.</text><text start="2293.74" dur="3.02">It does not solve the problem.</text><text start="2296.76" dur="8.411">And it&apos;s pretty straightforward to see why,
which is for – let&apos;s go ahead and say</text><text start="2305.171" dur="2.089">software in particular.</text><text start="2307.26" dur="8.38">Does software have to run on hardware where
the computer systems and server banks and</text><text start="2315.64" dur="6.38">satellites and et cetera require massive mining,
which also requires a financial system and</text><text start="2322.02" dur="4.36">police and courts to maintain the entire cybernetic
system that runs all that?</text><text start="2326.38" dur="2.58">Yes, it does.</text><text start="2328.96" dur="5.74">Does a lot more compute require more of that,
more atoms, adjacent services, energy?</text><text start="2334.7" dur="1.0">Yes.</text><text start="2335.7" dur="9.47">But also, for us to consider software valuable,
it&apos;s either because we&apos;re engaging with</text><text start="2345.17" dur="2.86">what&apos;s – we&apos;re engaging with what it&apos;s doing
directly.</text><text start="2348.03" dur="2.76">So that&apos;s the case in entertainment or education
or something.</text><text start="2350.79" dur="5.809">But then it is interfacing with the finite
resource called human attention, of which</text><text start="2356.599" dur="2.261">there is a finite amount.</text><text start="2358.86" dur="4.87">Or because we&apos;re not necessarily being entertained
or educated or engaging with it, but it&apos;s</text><text start="2363.73" dur="3.79">doing something for us to – again, to consider
valuable.</text><text start="2367.52" dur="3.19">It is doing something to the physical world.</text><text start="2370.71" dur="9.48">So the software is engaging, say, supply chain
optimization or new modeling for how to make</text><text start="2380.19" dur="2.72">better transistors or something like that.</text><text start="2382.91" dur="5.72">But then it&apos;s still moving atoms around using
energy and physical space, which is</text><text start="2388.63" dur="1.05">a finite resource.</text><text start="2389.68" dur="4.54">If it is not either affecting the physical
world or affecting our attention, why would</text><text start="2394.22" dur="1.04">we value it?</text><text start="2395.26" dur="1.17">We don&apos;t.</text><text start="2396.43" dur="4.399">So it still bottoms out on finite resources.</text><text start="2400.829" dur="3.691">So I can&apos;t just keep producing an infinite
amount of software where you get more and</text><text start="2404.52" dur="5.97">more content that nobody has time to watch
and more and more designs for physical things</text><text start="2410.49" dur="1.89">that we don&apos;t have physical atoms for or energy
for.</text><text start="2412.38" dur="3.33">You get a diminishing return on the value
of it if it&apos;s not coupled to things that</text><text start="2415.71" dur="1.76">are finite.</text><text start="2417.47" dur="3.84">The value of it is in modulating things that
are also finite.</text><text start="2421.31" dur="3.04">So there&apos;s a coupling coefficient there.</text><text start="2424.35" dur="2.04">You still don&apos;t get an exponential curve.</text><text start="2426.39" dur="4.85">So what we just did is say the old hippie
refrain, you can&apos;t run an exponential economy</text><text start="2431.24" dur="1.4">on a finite planet forever.</text><text start="2432.64" dur="3.4">The alt, the counters to it don&apos;t hold.</text><text start="2436.04" dur="5.14">What about mind uploading or some computer
brain interface to allow us to have more attention</text><text start="2441.18" dur="3.939">exponentially?</text><text start="2445.119" dur="9.851">Yeah, so it&apos;s another, it&apos;s kind of, that&apos;s
almost like the hybrid of the other two, right?</text><text start="2454.97" dur="5.01">Which is get beyond this planet and do it
more digitally.</text><text start="2459.98" dur="10.96">So get beyond this brain and become digital
gods in the singularity universe.</text><text start="2470.94" dur="10.71">I again, I think there are pretty interesting
arguments we can have ethically, aesthetically,</text><text start="2481.65" dur="10.57">and epistemically about why that is neither
possible nor desirable.</text><text start="2492.22" dur="6.04">But independent of those, I don&apos;t think it&apos;s
anywhere close.</text><text start="2498.26" dur="4.62">And same like the multi-planetary species,
it is nowhere near close enough to address</text><text start="2502.88" dur="4.61">any of the timelines we have by which economy
has to change because the growth imperative</text><text start="2507.49" dur="4.44">on the economy as is, is moving us towards
catastrophic tipping points.</text><text start="2511.93" dur="5.36">So if it were close, would that change your
assessment or you still have other issues?</text><text start="2517.29" dur="10.69">If it were close, then we would have to say,
first, that is implying that we have a good</text><text start="2527.98" dur="3.52">reason to think that it&apos;s possible, right?</text><text start="2531.5" dur="6.72">That it&apos;s possible to, and that means all
the axioms that consciousness is substrate</text><text start="2538.22" dur="7.06">independent, that the consciousness is purely
a function of compute, strong computationalism</text><text start="2545.28" dur="7.319">holds, that we could map the states of the
brain and or if we believe in embodied cognition,</text><text start="2552.599" dur="5.441">the physiology adequately to represent that
informational system on some other substrate,</text><text start="2558.04" dur="8.44">that that could operate with an amount of
energy that is and substrate that&apos;s possible,</text><text start="2566.48" dur="1.26">blah, blah, blah.</text><text start="2567.74" dur="2.45">So first we have to believe that&apos;s possible.</text><text start="2570.19" dur="4.24">I would question literally every one of the
axioms or assumptions I just said.</text><text start="2574.43" dur="1.66">We&apos;re going to get to that.</text><text start="2576.09" dur="5.66">We would say, is it desirable and how do we
know?</text><text start="2581.75" dur="1.93">How ahead of time?</text><text start="2583.68" dur="7.0">And now you get something very much like how
do I know that the AI is sentient, which for</text><text start="2590.68" dur="5.56">the most part on all AI risk topics, whether
it&apos;s sentient or not is irrelevant.</text><text start="2596.24" dur="2.54">Whether it does stuff is all that matters.</text><text start="2598.78" dur="6.66">But how do you tell if it&apos;s sentient and all
of the Chalmers, P-zombie questions or whatever</text><text start="2605.44" dur="1.77">are actually really hard?</text><text start="2607.21" dur="5.619">Because what we&apos;re asking is how can we use
third-person observation to infer something</text><text start="2612.829" dur="4.721">about the nature of first-person given the
ontological difference between them?</text><text start="2617.55" dur="3.35">So how would we know that that feature is
desirable?</text><text start="2620.9" dur="4.75">Are there safe to fail tests and what would
we have to test to know it to start making</text><text start="2625.65" dur="2.71">that conversion?</text><text start="2628.36" dur="8.489">But I don&apos;t think we have to answer any of
those questions because I don&apos;t think that</text><text start="2636.849" dur="5.341">anybody that is working on whole brain emulation
thinks that we are close enough that it would</text><text start="2642.19" dur="5.159">address the timeline of the economy issues
that you&apos;re addressing.</text><text start="2647.349" dur="3.641">Let&apos;s attempt to address one of the questions
about substrate independence.</text><text start="2650.99" dur="1.52">What are your views?</text><text start="2652.51" dur="4.67">Is consciousness something that our biological
brains do or that requires a development from</text><text start="2657.18" dur="1.29">an embryonic stage?</text><text start="2658.47" dur="5.139">Whatever it is that produced us, there&apos;s something
special about us or animals or is it something</text><text start="2663.609" dur="5.901">that can be transferred or started up, booted
up from scratch into what&apos;s not us, like decidedly</text><text start="2669.51" dur="2.83">not us, a computer?</text><text start="2672.34" dur="1.0">Okay.</text><text start="2673.34" dur="7.12">So this is now much more a proper theory of
everything conversation than the topic that</text><text start="2680.46" dur="2.79">we intended for the day, which is about AI
risk.</text><text start="2683.25" dur="6.18">So what I will do is say briefly the conclusion
of my thoughts on this without actually going</text><text start="2689.43" dur="8.23">into it in depth, but I would be happy to
explore that at some point.</text><text start="2697.66" dur="17.429">I think that how I come to my position on
it to try to do a kind of proper construction</text><text start="2715.089" dur="1.24">takes a while.</text><text start="2716.329" dur="3.051">So briefly I&apos;ll say...</text><text start="2719.38" dur="8.15">I&apos;m not a strong computationalist, meaning
don&apos;t believe that mind, universe, sentience,</text><text start="2727.53" dur="3.579">qualia is purely a function of computation.</text><text start="2731.109" dur="6.691">I am not an emergent physicalist that believes
that consciousness is an epiphenomenon of</text><text start="2737.8" dur="6.92">non-conscious physics that in the same way
that we have weak emergence, more of a particular</text><text start="2744.72" dur="5.369">property through certain kind of combinatorics
or strong emergence, new</text><text start="2750.089" dur="4.502">properties emerging out of some type of interaction
where that hadn&apos;t occurred before, like a</text><text start="2754.591" dur="3.379">cell respirating or none of the molecules
that make it up respirate.</text><text start="2757.97" dur="1.67">I believe in weak emergence.</text><text start="2759.64" dur="1.15">That happens all the time.</text><text start="2760.79" dur="1.68">You get more of certain qualities.</text><text start="2762.47" dur="4.91">It happens in metallurgy when you combine
metals where the combined tensile strength</text><text start="2767.38" dur="5.4">or shearing strength or whatever is more than
you would expect as a result of the nature</text><text start="2772.78" dur="1.46">of how the molecular lattices form.</text><text start="2774.24" dur="1.94">You get more of a thing of the same type.</text><text start="2776.18" dur="4.37">I believe in strong emergence, which is you
get new types of things you didn&apos;t have before,</text><text start="2780.55" dur="5.2">like respiration and replication out of parts,
none of which do that.</text><text start="2785.75" dur="4.63">But those are all still in the domain of third
person accessible things.</text><text start="2790.38" dur="4.5">The idea of radical emergence, that you get
the emergence of first person out of third</text><text start="2794.88" dur="5.239">person or third person out of first person,
which is idealism on one side and physicalism</text><text start="2800.119" dur="3.44">on the other, I don&apos;t buy either of.</text><text start="2803.559" dur="5.831">I think that idealism and physicalism are
similar types of reductionism where they both</text><text start="2809.39" dur="7.959">take certain ontological assumptions to bootload
their epistemology and then get self-referential</text><text start="2817.349" dur="1.531">dynamics.</text><text start="2818.88" dur="5.2">So I don&apos;t think that if a computational system
gets advanced enough, automatically consciousness</text><text start="2824.08" dur="1.42">pops out of it.</text><text start="2825.5" dur="1.0">That&apos;s one.</text><text start="2826.5" dur="8.58">Two, I do think that the process of a system
self-organizing is fundamentally connected</text><text start="2835.08" dur="2.999">to the nature of experience of selfness.</text><text start="2838.079" dur="9.061">And things that are being designed and are
not self-organizing, where the boundary between</text><text start="2847.14" dur="5.41">the system and its environment that exchanges
energy and information and matter across the</text><text start="2852.55" dur="4.1">boundary is a auto-poetic process.</text><text start="2856.65" dur="8.169">I do believe that&apos;s fundamental to the nature
of things that have self-other recognition.</text><text start="2864.819" dur="6.661">And on substrate independence, I do believe
that carbon and silicon are different in pretty</text><text start="2871.48" dur="10.879">fundamental ways that don&apos;t orient to the
same types of possibilities.</text><text start="2882.359" dur="9.011">And I think that that&apos;s actually pretty important
to the AI risk argument.</text><text start="2891.37" dur="3.33">But – so I&apos;ll just go ahead and say those
things.</text><text start="2894.7" dur="5.889">I also don&apos;t think – I believe that embodied
cognition in the Damasio sense is important</text><text start="2900.589" dur="3.431">and that a scan of purely brain states is
insufficient.</text><text start="2904.02" dur="8.0">I also don&apos;t think that a scan of brain states
is possible even in theory.</text><text start="2912.02" dur="1.0">And –</text><text start="2913.02" dur="1.0">Sorry to interrupt.</text><text start="2914.02" dur="1.339">I know you said you don&apos;t believe it&apos;s possible.</text><text start="2915.359" dur="1.0">What if it is?</text><text start="2916.359" dur="2.5">And you&apos;re able to scan your brain state and
body state.</text><text start="2918.859" dur="3.47">So we take into account the embodied cognition.</text><text start="2922.329" dur="1.52">Sure.</text><text start="2923.849" dur="11.871">So
I think that – okay.</text><text start="2935.72" dur="3.63">It&apos;s not simply a matter of scanning the brain
state.</text><text start="2939.35" dur="2.42">We need to scan the rest of the central nervous
system.</text><text start="2941.77" dur="1.99">No, we also have to get the peripheral nervous
system.</text><text start="2943.76" dur="2.14">No, we have to get the endocrine system.</text><text start="2945.9" dur="7.0">No, all of the cells have the production of
and reception of neuroendocrine-type things.</text><text start="2952.9" dur="2.79">We have to scan the whole thing.</text><text start="2955.69" dur="6.85">Does that then extend to the microbiome, virome,
et cetera?</text><text start="2962.54" dur="1.84">I would argue yes.</text><text start="2964.38" dur="2.4">Does it then extend to the environment?</text><text start="2966.78" dur="5.579">I would argue yes.</text><text start="2972.359" dur="3.591">Where does that stop its extension is actually
a very important question.</text><text start="2975.95" dur="5.28">So I would take the embodied cognition a step
further.</text><text start="2981.23" dur="7.4">The other thing is Stuart Kaufman&apos;s arguments
about quantum amplification to the mesoscopic</text><text start="2988.63" dur="6.689">level that quantum mechanical events don&apos;t
just fully cancel themselves out.</text><text start="2995.319" dur="9.361">At the subatomic level and at the level of
brains, everything that is happening is straightforwardly</text><text start="3004.68" dur="9.149">classical, but that there is quantum mechanical,
i.e., some fundamental kind of indeterminism</text><text start="3013.829" dur="1.651">built in phenomena.</text><text start="3015.48" dur="4.94">They end up affecting what happens at the
level of molecules.</text><text start="3020.42" dur="3.37">Now then, one can say, well, does that just
mean we have to add a certain amount of a</text><text start="3023.79" dur="3.769">random function in, or is there something
else?</text><text start="3027.559" dur="6.361">This is a big rabbit hole, I would say, for
another time, because then you get into quantum</text><text start="3033.92" dur="6.25">entanglement and coherence, so you get something
that is neither perfectly random, meaning</text><text start="3040.17" dur="1.0">without pattern.</text><text start="3041.17" dur="4.13">You get a born distribution even on a single
one, but it&apos;s also not deterministic or with</text><text start="3045.3" dur="2.049">hidden variables.</text><text start="3047.349" dur="9.341">So do I think that what&apos;s happening in the
brain-body system is not purely deterministic,</text><text start="3056.69" dur="4.07">and also as a result of that means you could
not measure or scan it even in principle,</text><text start="3060.76" dur="1.7">in a kind of Heisenberg sense?</text><text start="3062.46" dur="2.33">Yes, I think that.</text><text start="3064.79" dur="4.299">Have you heard of David Wolpert and his &quot;Limits
On Inference Machines&quot;?</text><text start="3069.089" dur="2.551">I have not studied his work.</text><text start="3071.64" dur="1.0">Okay.</text><text start="3072.64" dur="4.11">Well, anyway, he echoes something similar,
which says that you can&apos;t have Laplace&apos;s demon</text><text start="3076.75" dur="1.54">even in a classical world.</text><text start="3078.29" dur="1.14">You can&apos;t have Laplace&apos;s demon.</text><text start="3079.43" dur="2.13">So let me talk about the economy.</text><text start="3081.56" dur="4.17">Which only on your podcast would happen.</text><text start="3085.73" dur="6.369">Why is it that if somehow this exponential
curve starts to get to where the S is, the</text><text start="3092.099" dur="5.52">top of the S, that the halting or the slowing
down of the economy is something that&apos;s so</text><text start="3097.619" dur="5.051">catastrophic and calamitous, rather than something
that would mutate?</text><text start="3102.67" dur="4.08">And if we need to just, at that point, as
it starts to slow down, we make minor changes</text><text start="3106.75" dur="1.0">here and there?</text><text start="3107.75" dur="1.44">Is this something that&apos;s entirely new?</text><text start="3109.19" dur="2.7">Like, will they all come crashing down?</text><text start="3111.89" dur="1.71">Um, okay.</text><text start="3113.6" dur="1.0">So...</text><text start="3114.6" dur="2.769">Let me make the question clear.</text><text start="3117.369" dur="3.97">It sounds like, look, the economy is tied
to exponential growth.</text><text start="3121.339" dur="2.101">We can&apos;t grow exponentially.</text><text start="3123.44" dur="1.44">Virtually no one believes that.</text><text start="3124.88" dur="3.86">So at some point, and let&apos;s just imagine it&apos;s
three decades, just to give some numbers.</text><text start="3128.74" dur="4.51">So at some point, three decades from now,
this exponential curve for all of the economy</text><text start="3133.25" dur="4.74">will start to show its legs and start to weaken
and we&apos;ll see that it&apos;s nearing the S part.</text><text start="3137.99" dur="1.0">So what?</text><text start="3138.99" dur="3.339">Does that mean that there&apos;s been fire in the
streets, that the buildings don&apos;t work, that</text><text start="3142.329" dur="1.54">the water doesn&apos;t run anymore?</text><text start="3143.869" dur="3.801">Like, what will happen?</text><text start="3147.67" dur="3.03">Okay.</text><text start="3150.7" dur="8.96">So people often make jokes about physicists
in particular starting to look at biology</text><text start="3159.66" dur="5.429">and language and society and modeling in particularly
funny reductionist ways because they try to</text><text start="3165.089" dur="4.211">map the entire economy through the second
law of thermodynamics or something like that.</text><text start="3169.3" dur="7.6">And because what we&apos;re really talking about
is the maximally complex and anterocomplex</text><text start="3176.9" dur="6.07">thing and embedded complexity we can because
we&apos;re talking about all of human motives.</text><text start="3182.97" dur="8.639">And how do humans respond to the idea that
there is fundamentally limits on the growth</text><text start="3191.609" dur="8.581">possible to them or there&apos;s less stuff possible
for them or that there – whether it&apos;s issues</text><text start="3200.19" dur="4.129">that are associated with environmental extraction.</text><text start="3204.319" dur="5.23">So here&apos;s one of the classic challenges is
that the problems, the catastrophic risks,</text><text start="3209.549" dur="5.52">many of them in the environmental category
are the result of cumulative action long-term</text><text start="3215.069" dur="2.871">where the upsides are the result of individual
action short-term.</text><text start="3217.94" dur="2.59">And the asymmetry between those is particularly
problematic.</text><text start="3220.53" dur="8.1">That&apos;s why you get this collective choice-making
challenge, meaning if I cut down a tree for</text><text start="3228.63" dur="7.91">timber, I don&apos;t obviously perceive a change
to the atmosphere or to the climate or to</text><text start="3236.54" dur="1.72">watersheds or to anything.</text><text start="3238.26" dur="4.349">But my bank account goes up through being
able to sell that lumber immediately.</text><text start="3242.609" dur="4.371">And the same is true if I fish or if I do
anything like that.</text><text start="3246.98" dur="5.32">But when you run the Kantian categorical imperative
across it and you have the movement from half</text><text start="3252.3" dur="5.47">a billion people doing it to pre-industrial
revolution to 8 billion and you have something</text><text start="3257.77" dur="6.339">like in the industrial world 100X resource
per capita consumption just calorically measured</text><text start="3264.109" dur="2.931">today than at the beginning of the industrial
revolution.</text><text start="3267.04" dur="4.42">Then you start realizing, okay, well, the
cumulative effects of that don&apos;t work.</text><text start="3271.46" dur="4.63">They break the planet, and they start creating
tipping points that auto-propagate in the</text><text start="3276.09" dur="1.59">wrong direction.</text><text start="3277.68" dur="7.56">But no individual person or even local area
doing the thing recognizes their action as</text><text start="3285.24" dur="3.59">driving that downside, and how do you get
global enforcement of the thing?</text><text start="3288.83" dur="3.19">And if you don&apos;t get global enforcement, why
should anyone let themselves be curtailed</text><text start="3292.02" dur="1.349">when other people aren&apos;t being curtailed?</text><text start="3293.369" dur="3.321">And that&apos;ll give them game theoretic advantage.</text><text start="3296.69" dur="3.96">So this is actually, there&apos;s a handful of
asymmetries that are important to understand</text><text start="3300.65" dur="1.07">with regard to risk.</text><text start="3301.72" dur="1.0">All right.</text><text start="3302.72" dur="2.16">We&apos;ve covered plenty so far.</text><text start="3304.88" dur="2.2">And so it&apos;s fruitful to have a brief summary.</text><text start="3307.08" dur="3.039">We&apos;ve talked about the faulty foundation of
our monetary system.</text><text start="3310.119" dur="4.44">Daniel argues that post-World War II especially,
our economic system has not only encouraged</text><text start="3314.559" dur="5.361">but been dependent on exponential monetary
growth, and this can&apos;t continually occur.</text><text start="3319.92" dur="4.06">We&apos;ve also talked about the digital escape
plan and how this is an illusion, at least</text><text start="3323.98" dur="1.0">in Daniel&apos;s eye.</text><text start="3324.98" dur="4.01">He believes that digital growth has physical
costs because their hardware, their human</text><text start="3328.99" dur="4.65">attention limits their finite resources, linear
resources as he calls them, though I have</text><text start="3333.64" dur="4.09">my issues with the term linear resource because
technically anything is linear when measured</text><text start="3337.73" dur="1.079">against itself.</text><text start="3338.809" dur="4.911">We&apos;ve also talked about how moving to Mars
won&apos;t save us, us being civilization.</text><text start="3343.72" dur="3.849">Daniel believes that the idea of becoming
an interplanetary species to escape resource</text><text start="3347.569" dur="3.741">limitations is unrealistic, perhaps even ethically
questionable.</text><text start="3351.31" dur="3.18">We&apos;ve also talked about how mind uploading
is not what it&apos;s cracked up to be.</text><text start="3354.49" dur="1.0">It may not occur.</text><text start="3355.49" dur="4.07">And even if it does, it&apos;s not the answer because
it&apos;s either unfeasible, but even if it&apos;s feasible,</text><text start="3359.56" dur="2.529">Daniel believes it to be undesirable.</text><text start="3362.089" dur="4.861">Another resource as we expand our digital
footprint is the privacy of our digital resources.</text><text start="3366.95" dur="4.5">You can see this being recognized even by
OpenAI as they recently announced an incognito</text><text start="3371.45" dur="1.0">mode.</text><text start="3372.45" dur="1.0">And this is where our sponsor comes in.</text><text start="3373.45" dur="4.75">Do you ever get the feeling that your internet
provider knows more about you than your own</text><text start="3378.2" dur="1.0">mother?</text><text start="3379.2" dur="1.0">It&apos;s like they&apos;re in your head.</text><text start="3380.2" dur="1.0">They can predict your next move.</text><text start="3381.2" dur="4.3">When I&apos;m researching complicated physics topics
or checking the latest news or just in general</text><text start="3385.5" dur="3.849">what I want privacy on, I don&apos;t want to have
to go and research which VPN is best.</text><text start="3389.349" dur="1.52">I don&apos;t want to be bothered by that.</text><text start="3390.869" dur="4.2">Well, I and you can put those fears to rest
with private internet access.</text><text start="3395.069" dur="5.082">If you have a VPN provider that&apos;s got your
back with over 30 million downloads, they&apos;re</text><text start="3400.151" dur="4.089">the real deal when it comes to keeping your
online activity private.</text><text start="3404.24" dur="2.78">And they&apos;ve got apps for every operating system.</text><text start="3407.02" dur="4.38">You can protect 10 of your devices at once,
even if you&apos;re unfortunate enough like me</text><text start="3411.4" dur="1.34">to love Windows.</text><text start="3412.74" dur="4.26">And if you&apos;re worried about strange items
popping up in your search history, don&apos;t worry.</text><text start="3417.0" dur="1.28">I&apos;m not judging.</text><text start="3418.28" dur="3.55">Private internet access comes in here as they
encrypt your connection.</text><text start="3421.83" dur="5.39">They encrypt your IP address so your ISP doesn&apos;t
have access to those strange items in your</text><text start="3427.22" dur="1.0">history.</text><text start="3428.22" dur="1.66">They make you a ghost online.</text><text start="3429.88" dur="3.06">It&apos;s like Batman&apos;s cave before you&apos;re browsing
history.</text><text start="3432.94" dur="4.159">With private internet access, you can keep
your odd internet searches, let&apos;s say, on</text><text start="3437.099" dur="1.341">the down low.</text><text start="3438.44" dur="3.45">It&apos;s like having your own personal confessional
booth, except you never need to talk to a</text><text start="3441.89" dur="1.0">priest.</text><text start="3442.89" dur="1.0">So why wait?</text><text start="3443.89" dur="8.08">Head over to piavpn.com slash TOE, T-O-E,
and get yourself an 82, an 82% discount.</text><text start="3451.97" dur="2.27">That&apos;s less than the price of a coffee per
month.</text><text start="3454.24" dur="4.02">And let&apos;s face it, your online privacy is
worth way more than a latte.</text><text start="3458.26" dur="5.71">That&apos;s piavpn.com slash T-O-E now and get
the protection you deserve.</text><text start="3463.97" dur="5.51">Brilliance is a place where there are bite-sized
interactive learning experiences for science,</text><text start="3469.48" dur="1.41">engineering, and mathematics.</text><text start="3470.89" dur="3.909">Artificial intelligence in its current form
uses machine learning, which uses neural nets,</text><text start="3474.799" dur="1.0">often at least.</text><text start="3475.799" dur="3.751">And there are several courses on Brilliance&apos;s
website teaching you the concepts underlying</text><text start="3479.55" dur="5.25">neural nets and computation in an extremely
intuitive manner that&apos;s interactive,</text><text start="3484.8" dur="2.79">which is unlike almost any of the tutorials
out there.</text><text start="3487.59" dur="1.03">They quiz you.</text><text start="3488.62" dur="4.179">I personally took the course on random variable
distributions and knowledge and uncertainty</text><text start="3492.799" dur="2.471">because I wanted to learn more about entropy,</text><text start="3495.27" dur="4.059">especially as there may be a video coming
out on entropy, as well as you can learn group</text><text start="3499.329" dur="1.22">theory on their website,</text><text start="3500.549" dur="6.101">which underlies physics, that is SU3 cross
SU2 cross U1 is the standard model gauge group.</text><text start="3506.65" dur="6.55">Visit brilliant.org slash TOE, T-O-E, to get
20% off your annual premium subscription.</text><text start="3513.2" dur="3.22">As usual, I recommend you don&apos;t stop before
four lessons.</text><text start="3516.42" dur="1.879">You have to just get wet.</text><text start="3518.299" dur="1.211">You have to try it out.</text><text start="3519.51" dur="4.12">I think you&apos;ll be greatly surprised at the
ease at which you can now comprehend subjects</text><text start="3523.63" dur="4.42">you previously had a difficult time grokking.</text><text start="3528.05" dur="9.88">The bad is the material from which the good
may learn.</text><text start="3537.93" dur="4.409">So this is actually there&apos;s a handful of asymmetries
that are important to understand with regard</text><text start="3542.339" dur="1.341">to risk.</text><text start="3543.68" dur="4.49">One is this one that I&apos;m saying, which is
you have risks that are the result of long-term</text><text start="3548.17" dur="1.51">cumulative action,</text><text start="3549.68" dur="3.09">but that you actually have to change individual
action because of that.</text><text start="3552.77" dur="6.48">But the upside, the benefit, the individual
making that action realizes the benefit directly.</text><text start="3559.25" dur="3.45">And so this is a classic tragedy of the commons
type issue, right?</text><text start="3562.7" dur="5.71">The tragedy of the commons at a not just local
scales, but at global scales.</text><text start="3568.41" dur="8.159">And some of the other asymmetries are particularly
important is people who focus on the upside,</text><text start="3576.569" dur="1.53">who focus on opportunity,</text><text start="3578.099" dur="3.401">do better game theoretically for the most
part than people who focus on risk when it</text><text start="3581.5" dur="3.64">comes to new technologies and advancement
and progress in general.</text><text start="3585.14" dur="8.62">Because if someone says, hey, we thought Vioxx
or DDT or any number of things were good ideas,</text><text start="3593.76" dur="3.99">they ended up – or leaded gasoline, they
ended up being really bad later.</text><text start="3597.75" dur="4.68">We want to do really good long-term safety
testing regarding first, second, third order</text><text start="3602.43" dur="1.56">effects of this.</text><text start="3603.99" dur="3.379">They&apos;re going to spend a lot of money and
knock it first to market and then probably</text><text start="3607.369" dur="3.321">decide the whole thing wasn&apos;t a good idea
at all.</text><text start="3610.69" dur="4.47">Or if they do decide how to do a safe version,
it takes them a very long time.</text><text start="3615.16" dur="1.73">The person says, no, the risks aren&apos;t that
bad.</text><text start="3616.89" dur="1.0">Let me show you.</text><text start="3617.89" dur="4.14">Does a bullshit job of risk analysis as a
box checking process and then really emphasizes</text><text start="3622.03" dur="3.47">the upsides is going to get first mover advantage,
make all the money.</text><text start="3625.5" dur="2.49">They will privatize the gains, socialize the
losses.</text><text start="3627.99" dur="6.589">Then when the problems get revealed a long
time later and are unfixable, that will have</text><text start="3634.579" dur="1.0">already happened.</text><text start="3635.579" dur="5.631">So these are just examples of some of the
kind of choice-making asymmetries that are</text><text start="3641.21" dur="1.409">significant to understand the situation.</text><text start="3642.619" dur="1.771">I only partly answered your question.</text><text start="3644.39" dur="1.0">Sure.</text><text start="3645.39" dur="2.77">Are you having in mind a particular corporation
currently?</text><text start="3648.16" dur="1.05">Totally.</text><text start="3649.21" dur="8.07">Not a particular corporation, but a particularly
important consideration in the entire topic.</text><text start="3657.28" dur="4.0">One view is that Google is not coming out
with something that&apos;s competitive.</text><text start="3661.28" dur="1.43">Like BART is not competitive.</text><text start="3662.71" dur="2.27">I think even Google would admit that.</text><text start="3664.98" dur="2.599">And so one view is that, well, they&apos;re highly
testing.</text><text start="3667.579" dur="3.451">Another one, I&apos;ve spoken to some people behind
the scenes and they say Google doesn&apos;t have</text><text start="3671.03" dur="1.0">anything.</text><text start="3672.03" dur="1.78">They don&apos;t have anything like chat GPT.</text><text start="3673.81" dur="1.58">It&apos;s BS when they say so.</text><text start="3675.39" dur="4.88">Even OpenAI doesn&apos;t know why chat GPT works,
like GPT-4 works as well as it does.</text><text start="3680.27" dur="3.299">They just threw so much data at it and it
was a surprise to them.</text><text start="3683.569" dur="2.101">And in some ways they got lucky.</text><text start="3685.67" dur="4.47">So do you see what&apos;s happening right now between
Microsoft and Google as Google is actually</text><text start="3690.14" dur="3.83">the more cautious one and Microsoft is the
more brazen one and perhaps should be a bit</text><text start="3693.97" dur="5.18">more circumspect?</text><text start="3699.15" dur="7.55">I have heard a lot of things about the choices
that both companies made to not release stuff</text><text start="3706.7" dur="4.58">and safety studies that they did and then
what influenced the choices to release stuff</text><text start="3711.28" dur="4.42">inside of Microsoft and OpenAI and how Google&apos;s
handling it.</text><text start="3715.7" dur="9.869">I don&apos;t know that these stories are the totality
of information on it that&apos;s relevant.</text><text start="3725.569" dur="9.721">Do I think that economic forcing functions
have played a role in something that affected</text><text start="3735.29" dur="4.569">the safety analysis totally?</text><text start="3739.859" dur="7.67">Do I think that that is an unacceptably dumb
thing on a topic that has this level of safety</text><text start="3747.529" dur="3.491">risk associated totally?</text><text start="3751.02" dur="3.579">So now getting into what is unique about AI
risk, right?</text><text start="3754.599" dur="2.261">What is unique about it relative to all other
risks?</text><text start="3756.86" dur="6.25">Do we – people are saying things like we
need an FDA for AI right now, which I would</text><text start="3763.11" dur="7.419">argue is both true and a profoundly inadequate
analogy because a single new chemical that</text><text start="3770.529" dur="3.211">comes out is not an agent.</text><text start="3773.74" dur="5.99">It is not a dynamic thing that continues to
respond differently to huge numbers of new</text><text start="3779.73" dur="2.05">unpredictable stimuli.</text><text start="3781.78" dur="4.87">So how you do the assessment of the phase
space of possible things is totally different.</text><text start="3786.65" dur="8.23">It would probably be good to dive into what
is the risk space of AI, why is it unique,</text><text start="3794.88" dur="8.89">and how given all of the differences of concern,
how to framework and think about that properly.</text><text start="3803.77" dur="1.349">What else is unique about it?</text><text start="3805.119" dur="3.451">Why can&apos;t we have an FDA or a UN version of
an FDA for AI?</text><text start="3808.57" dur="3.35">And when I say UN, sorry, what I mean is global.</text><text start="3811.92" dur="1.179">Yeah.</text><text start="3813.099" dur="8.99">Well, obviously, you bring up UN and say global
because you have to have global regulation</text><text start="3822.089" dur="2.861">on something like that, right?</text><text start="3824.95" dur="6.06">In the same way that when people talk about
climate regulation, if we were – if any</text><text start="3831.01" dur="8.76">country, if any group of countries was to
try to price carbon properly, meaning what</text><text start="3839.77" dur="9.13">does it take to renewably produce those hydrocarbons
and what does it take to in real time fix</text><text start="3848.9" dur="4.99">all of the effects, both sequester the CO2,
clean up the oil spills, whatever it is.</text><text start="3853.89" dur="6.969">The price of oil would become high enough
with those costs internalized that oil then</text><text start="3860.859" dur="5.51">as an input to industries, literally every
industry would be non-profitable.</text><text start="3866.369" dur="5.16">And so even if any country was to try to make
some steps in the direction of internalizing</text><text start="3871.529" dur="3.84">cost and other ones didn&apos;t, then the other
ones who continue to externalize their costs</text><text start="3875.369" dur="5.381">get so much further ahead in terms of GDP
that can be applied to militaries and surplus</text><text start="3880.75" dur="5.5">of many different kinds and advancing exponential
tech that insofar as those are also competing</text><text start="3886.25" dur="5.85">entities for world resources and control,
it&apos;s not a viable thing.</text><text start="3892.1" dur="5.699">This is true for AI as well, and this then
starts to hit this other issue, which is if</text><text start="3897.799" dur="4.541">you can&apos;t regulate something on a purely national
level because it&apos;s not just how does it affect</text><text start="3902.34" dur="4.8">the people in the nation, but how does it
affect the nation&apos;s capability to interact</text><text start="3907.14" dur="1.909">with other nations.</text><text start="3909.049" dur="5.341">Now you get to the – and so the creation
of the UN was kind of the recognition in the</text><text start="3914.39" dur="5.03">existence in the emergence of World War II
that nation-state governance alone was not</text><text start="3919.42" dur="1.889">adequate to prevent World War.</text><text start="3921.309" dur="3.951">Obviously, that&apos;s why the League of Nations
came after World War I and it was not strong</text><text start="3925.26" dur="2.66">enough to prevent World War II.</text><text start="3927.92" dur="4.129">Now you get to the topic of why so many people
are super concerned about global government</text><text start="3932.049" dur="4.25">and don&apos;t want global government, and they&apos;ll
say things like the risks are being exaggerated</text><text start="3936.299" dur="3.24">and blown out of proportion to be able to
drive control paradigms.</text><text start="3939.539" dur="5.33">And the people who want to have a one-world
government or a powerful nation government</text><text start="3944.869" dur="4.99">exaggerate the risks so that they can drive
control paradigms where they will be the one</text><text start="3949.859" dur="2.521">in the control side.</text><text start="3952.38" dur="7.25">This can be excessive paranoia, but it&apos;s also
a really realistic and founded consideration,</text><text start="3959.63" dur="10.3">which is are there any radical asymmetries
of power where the side that had all the power</text><text start="3969.93" dur="2.419">used it really well historically?</text><text start="3972.349" dur="2.761">It doesn&apos;t look that good, right?</text><text start="3975.11" dur="3.679">And so we see a reason to be concerned about
something like a one-world government that</text><text start="3978.789" dur="2.821">has no possible checks and balances.</text><text start="3981.61" dur="3.71">But there&apos;s also a concern about not having
anything where you get some type of global</text><text start="3985.32" dur="6.25">governance, if not government, meaning some
unified establishment that has monopoly of</text><text start="3991.57" dur="1.0">violence.</text><text start="3992.57" dur="4.769">At least governance, meaning some coordination
where everyone is not left in a multipolar</text><text start="3997.339" dur="5.301">trap saying we can&apos;t bind our behavior because
they won&apos;t, and if they won&apos;t, then we have</text><text start="4002.64" dur="1.38">to race ahead.</text><text start="4004.02" dur="3.029">We can&apos;t stop overfishing because the fish
will all get killed because they&apos;re doing</text><text start="4007.049" dur="1.0">the thing anyway.</text><text start="4008.049" dur="4.5">So not only will we not stop, we will actually
race to do it faster than them so they don&apos;t</text><text start="4012.549" dur="4.151">get more resource relative to us, those types
of issues.</text><text start="4016.7" dur="5.37">So obviously with regard to the environment,
we call it a tragedy of the commons.</text><text start="4022.07" dur="4.63">With regard to the development of possible
military technology, we call it an arms race.</text><text start="4026.7" dur="3.34">Both of them are examples of social traps
or multipolar traps.</text><text start="4030.04" dur="5.1">Briefly, why do you call it a multipolar trap?</text><text start="4035.14" dur="4.55">Social trap is a term used in the social sciences
quite a lot to indicate a coordination failure</text><text start="4039.69" dur="6.49">of this type where each agent pursuing their
own near-term rational interest creates a</text><text start="4046.18" dur="7.35">situation that moves the entire global situation
long-term to a suboptimal equilibrium for</text><text start="4053.53" dur="1.0">everybody.</text><text start="4054.53" dur="4.47">And there&apos;s a lot of work in various fields
of social science on social traps.</text><text start="4059.0" dur="5.67">The first time I&apos;m aware of the term multipolar
trap entering the conversation was in the</text><text start="4064.67" dur="6.169">great article called Meditations on Moloch
by Scott Alexander where he – I believe</text><text start="4070.839" dur="4.062">he&apos;s the one who coined the term multipolar
trap there, and it&apos;s pretty close to a social</text><text start="4074.901" dur="1.0">trap.</text><text start="4075.901" dur="7.468">If I was going to define a distinction, it
might be something like in a classic social</text><text start="4083.369" dur="3.34">trap, it&apos;s a social trap.</text><text start="4086.709" dur="5.421">Tragedy of the commons scenario where everyone
is utilizing a common wealth resource like</text><text start="4092.13" dur="6.93">say fishing or cutting down trees in a forest
or whatever.</text><text start="4099.06" dur="6.0">You&apos;re not necessarily in the situation where
everyone is racing to do it faster than the</text><text start="4105.06" dur="4.36">other person to destroy it, just them simply
not curtailing their own behavior, and yet</text><text start="4109.42" dur="6.22">you have a resource per capita consumption
growth and a total population growth such</text><text start="4115.64" dur="3.9">that the environment can&apos;t deal with it.</text><text start="4119.54" dur="2.33">You still end up getting environmental devastation.</text><text start="4121.87" dur="4.5">But as soon as you kind of move over into,
hey, even if I don&apos;t cut down the trees or</text><text start="4126.37" dur="4.31">I don&apos;t fish, the other side is going to,
so I literally don&apos;t have the ability to protect</text><text start="4130.68" dur="5.67">the forest, but I do have the ability to cut
down some of it, benefit myself or our people</text><text start="4136.35" dur="2.86">or tribe or nation or whatever it is.</text><text start="4139.21" dur="4.06">And if I don&apos;t, the other guys will break
it down anyways, but they&apos;ll also use the</text><text start="4143.27" dur="4.33">economic advantage of that against us in whatever
the next rivalrous conflict is.</text><text start="4147.6" dur="5.73">So not only do I have to keep doing it, but
I have to race to do it faster than they do.</text><text start="4153.33" dur="4.77">I actually have to apply innovation now, and
so this is where you get an accelerating dynamic.</text><text start="4158.1" dur="3.92">And if you don&apos;t just have two actors doing
this, but you have many actors doing this</text><text start="4162.02" dur="4.12">where it&apos;s very hard to be able to bind it
because how do you ensure that all the actors</text><text start="4166.14" dur="1.0">are keeping the agreement?</text><text start="4167.14" dur="4.09">You have to make some nonproliferation agreement.</text><text start="4171.23" dur="3.009">You have to have some way of ensuring that
they&apos;re all keeping it, and you have to have</text><text start="4174.239" dur="3.161">some enforceable deterrent if anyone violates
it.</text><text start="4177.4" dur="2.669">Those happen, but it&apos;s not trivial.</text><text start="4180.069" dur="4.951">It&apos;s not trivial to enact those, and it&apos;s
particularly – so let&apos;s say we&apos;ve achieved</text><text start="4185.02" dur="6.92">that when it comes to nukes in some ways,
though at the beginning of the current post-World</text><text start="4191.94" dur="4.5">War II system, there was only two superpowers
with nukes, and now there&apos;s roughly nine countries</text><text start="4196.44" dur="1.0">with them.</text><text start="4197.44" dur="1.76">There are not 100 countries with them.</text><text start="4199.2" dur="4.44">There aren&apos;t even 30 because we&apos;ve done a
really intense job of ensuring that Iran and</text><text start="4203.64" dur="2.15">many countries that want nukes don&apos;t get them,
right?</text><text start="4205.79" dur="1.58">And why?</text><text start="4207.37" dur="3.47">Because there are not uranium mines everywhere.</text><text start="4210.84" dur="1.46">You can see where they are.</text><text start="4212.3" dur="4.85">Uranium enrichment takes massive capability
that you can literally see from space, right?</text><text start="4217.15" dur="2.54">There&apos;s radioactive activity associated.</text><text start="4219.69" dur="5.0">So it&apos;s somewhat easy to monitor that that&apos;s
happening.</text><text start="4224.69" dur="6.83">This is not true at all with the newer technologies
that provide more catastrophic capability.</text><text start="4231.52" dur="5.23">So obviously with AI right now and the regulation
of it, there are conversations about like</text><text start="4236.75" dur="4.25">we need to monitor all large GPU clusters
or something like that, which to some degree</text><text start="4241.0" dur="1.06">can be done.</text><text start="4242.06" dur="6.59">But in terms of applications, it takes a very
large GPU cluster to develop an LLM.</text><text start="4248.65" dur="3.9">It takes a very small one to run that LLM
afterwards, right?</text><text start="4252.55" dur="6.47">And then can you run it for destructive purposes?</text><text start="4259.02" dur="11.42">And it takes a very large capability to advance
something like a CRISPR or a new type of synthetic</text><text start="4270.44" dur="1.39">bio knowledge.</text><text start="4271.83" dur="4.98">It doesn&apos;t take that much to be able to reverse
engineer it after it&apos;s been developed.</text><text start="4276.81" dur="9.15">So this brings up this very important point
of when the technology is built, there&apos;s this</text><text start="4285.96" dur="3.21">general refrain that all technology is dual
use, right?</text><text start="4289.17" dur="4.62">Meaning that if it wasn&apos;t, sometimes it&apos;s
developed for military purpose first and then</text><text start="4293.79" dur="4.42">becomes used for civilian normal market purposes.</text><text start="4298.21" dur="5.12">But if it&apos;s being developed for some non-military
purpose, there&apos;s probably a militarized application.</text><text start="4303.33" dur="3.03">That&apos;s what&apos;s meant with dual use is military
versus non-military.</text><text start="4306.36" dur="2.34">So it&apos;s not the same as this is a double edged
sword.</text><text start="4308.7" dur="1.19">It&apos;s positive and negative.</text><text start="4309.89" dur="1.0">It&apos;s not the same as that.</text><text start="4310.89" dur="1.0">Yeah, it is.</text><text start="4311.89" dur="3.329">What it&apos;s saying is you&apos;re developing this
for some purpose, but it has other purposes</text><text start="4315.219" dur="1.0">too, right?</text><text start="4316.219" dur="3.98">And it has purposes that can be used for violence
or conflict or destruction or something.</text><text start="4320.199" dur="5.571">And while that is historically mostly used
with the concept of has a military application</text><text start="4325.77" dur="4.969">can be used to advance war and killing and
things like that, whether by a state actor</text><text start="4330.739" dur="3.46">or a non-state actor, so you call it terrorist
activity.</text><text start="4334.199" dur="4.48">Sorry, when I was thinking of military, I
was also thinking in terms of pure defense,</text><text start="4338.679" dur="3.48">not just defense that also can be something
that can attack.</text><text start="4342.159" dur="1.011">Yeah.</text><text start="4343.17" dur="11.64">Yeah, the pure defense only military – it
starts becoming part of most military doctrines</text><text start="4354.81" dur="7.19">that viable defense requires things that look
like escalation.</text><text start="4362.0" dur="6.0">But that&apos;s another topic as well.</text><text start="4368.0" dur="2.92">So it&apos;s not just that all technologies are
dual use.</text><text start="4370.92" dur="3.19">It&apos;s that they have many uses.</text><text start="4374.11" dur="5.589">You develop the technology and I think a good
way to think about – so now this is a little</text><text start="4379.699" dur="1.591">bit of theory of tech.</text><text start="4381.29" dur="2.73">Did we close multipolar trap?</text><text start="4384.02" dur="3.46">Well, you mentioned that it first came up
in Scott Erickson&apos;s or –</text><text start="4387.48" dur="1.02">Alexander.</text><text start="4388.5" dur="1.02">Yeah.</text><text start="4389.52" dur="5.47">And so basically the concept is you have many
different agents who all of them pursuing</text><text start="4394.99" dur="4.8">their own rational interest and maybe they
can&apos;t even avoid it because it would be so</text><text start="4399.79" dur="1.0">irrational.</text><text start="4400.79" dur="5.42">It would be so bad for them game theoretically
that the effect of each of the agents pursuing</text><text start="4406.21" dur="4.23">their own rational interest produces a global
effect that is somewhere between catastrophic</text><text start="4410.44" dur="3.25">or at least far from the global optimum if
they could coordinate better.</text><text start="4413.69" dur="5.279">So this is basically a particular type of
multi-agent coordination failure.</text><text start="4418.969" dur="2.741">And we see this all over in the tragedy of
the commons as an example.</text><text start="4421.71" dur="5.87">A market race to the bottom like happens in
marketing and attention currently is an example,</text><text start="4427.58" dur="1.69">and an arms race is another example.</text><text start="4429.27" dur="4.33">Those would all be examples of a kind of multipolar
trap coordination failure.</text><text start="4433.6" dur="8.52">This is why if you have – say one nation
is advancing bioweapons or advancing AI weapons,</text><text start="4442.12" dur="5.2">either AI applied to cyber or applied to drones
or applied to autonomous weapons of various</text><text start="4447.32" dur="1.0">kinds.</text><text start="4448.32" dur="5.19">If any country is doing that, it is such an
obvious strategic advantage that every other</text><text start="4453.51" dur="3.7">country has to be developing the same types
of things plus the whole suite of counters</text><text start="4457.21" dur="2.66">and defenses to those types of things.</text><text start="4459.87" dur="4.4">And so you could just say, well, the world
in which everybody has autonomous weapons</text><text start="4464.27" dur="5.26">is such a worse world than this world that
we should just all agree not to do it.</text><text start="4469.53" dur="3.68">Except how do I know the other guy is actually
keeping the agreement?</text><text start="4473.21" dur="3.58">Well, with the nukes we can tell because we
can see if they&apos;re mining uranium and enriching</text><text start="4476.79" dur="4.73">it because it takes massive facilities and
they&apos;re radioactive and stuff like that.</text><text start="4481.52" dur="5.46">But if we&apos;re talking about things like working
with AI systems or synthetic biosystems that</text><text start="4486.98" dur="5.08">don&apos;t require a bunch of exotic materials,
exotic mining that don&apos;t produce radioactive</text><text start="4492.06" dur="3.61">tracers, et cetera, and they can be done in
a deep underground military base, how do we</text><text start="4495.67" dur="1.79">know if they&apos;re doing it or not?</text><text start="4497.46" dur="5.71">So if we don&apos;t know if the other side is doing
it or not, then the game theory is you have</text><text start="4503.17" dur="5.7">to assume they are because if you assume they
are, you&apos;re going to develop it as well.</text><text start="4508.87" dur="4.71">And then if they do have it and use it, you
aren&apos;t totally screwed, whereas the risk on</text><text start="4513.58" dur="4.46">the other assumption that they aren&apos;t, if
you were wrong, you&apos;re totally screwed.</text><text start="4518.04" dur="5.19">So under not having full knowledge, the game
theory orients to worst case scenario and</text><text start="4523.23" dur="1.5">being prepared against the worst case.</text><text start="4524.73" dur="2.96">But what that means is all sides assume that
of each other.</text><text start="4527.69" dur="2.53">We don&apos;t know that the other guys are keeping
the agreement.</text><text start="4530.22" dur="2.91">Therefore, we have to race ahead with this
thing.</text><text start="4533.13" dur="6.74">And so this is why you&apos;re saying when it comes
to things like AI, do we need something that</text><text start="4539.87" dur="2.519">is not just an FDA thing but a UN thing?</text><text start="4542.389" dur="2.691">Is this the kind of thing that would require
an international agreement?</text><text start="4545.08" dur="5.52">And obviously when there was the question
of creating a pause on a six-month pause or</text><text start="4550.6" dur="4.52">whatever, one of the first things people brought
up is won&apos;t that let China race ahead?</text><text start="4555.12" dur="4.15">And isn&apos;t this a US-China competitiveness
issue?</text><text start="4559.27" dur="8.12">And we can see with the CHIPS Act and trying
to ban ASML downstream-type GPUs to China,</text><text start="4567.39" dur="6.21">and we can see with the pressures over Taiwan
and TSMC that there is actually a lot of US-China</text><text start="4573.6" dur="7.34">great power play competition related to computation
and AI in specific.</text><text start="4580.94" dur="7.75">And so it&apos;s a classic situation that if you
can&apos;t put certain types of control mechanisms</text><text start="4588.69" dur="5.28">in internationally, you will probably fail
at being able to get them nationally as well.</text><text start="4593.97" dur="4.499">So about this competition where the tragedy
of the commons such that like, well, the competitiveness</text><text start="4598.469" dur="3.291">plus tragedy of the commons accelerates the
tragedy of the commons.</text><text start="4601.76" dur="4.55">Why is it not much more simple, religiously
simple, ethically simple, where we go back</text><text start="4606.31" dur="3.48">and we say, hey, what I&apos;m going to do is outputting
something negative.</text><text start="4609.79" dur="2.73">I don&apos;t care that if you do it, you&apos;re going
to get ahead.</text><text start="4612.52" dur="1.72">I don&apos;t care if you&apos;re going to eliminate
me.</text><text start="4614.24" dur="4.78">I would rather die for your sins rather than
contribute my own sins.</text><text start="4619.02" dur="1.1">So the selflessness.</text><text start="4620.12" dur="1.4">Why isn&apos;t that sort of ethic?</text><text start="4621.52" dur="3.6">Like we say we don&apos;t want to be Luddites,
but why isn&apos;t that a solution?</text><text start="4625.12" dur="6.63">I mean, you&apos;re bringing up a great point,
which is can there be a long range thinking</text><text start="4631.75" dur="5.08">about the kind of world we want to live in
and a recognition of the kind of beings we</text><text start="4636.83" dur="4.81">have to be, the behaviors we would have to
do and not do for that world to come about</text><text start="4641.64" dur="1.56">where we bind ourselves, right?</text><text start="4643.2" dur="4.36">Where we have some kind of whether the ethics
reduces to law, meaning there&apos;s a monopoly</text><text start="4647.56" dur="2.25">of violence that backs up the thing or not.</text><text start="4649.81" dur="3.25">Can we at least self-police in some way towards
it?</text><text start="4653.06" dur="8.579">And the answer is the long term answer must
involve that, I would argue.</text><text start="4661.639" dur="5.091">Past examples have involved it, but let&apos;s
talk about where it&apos;s limited.</text><text start="4666.73" dur="7.99">One could argue that the Sabbath and the punishments
for violating the Sabbath is an example of</text><text start="4674.72" dur="2.68">binding a multipolar trap.</text><text start="4677.4" dur="7.62">So you&apos;re not going to work on the Sabbath,
and if you do, there&apos;s 29 different reasons</text><text start="4685.02" dur="2.13">laid out why you can be killed for working
on the Sabbath.</text><text start="4687.15" dur="10.759">It seems to secular people not thinking about
the Chesterton fence deeply, it seems like</text><text start="4697.909" dur="6.891">a ridiculous, wacky religious idea not grounded
in anything with a ridiculous amount of consequence.</text><text start="4704.8" dur="5.61">Now, your theory of justice is, is it only
a personal or is it a collective theory of</text><text start="4710.41" dur="1.079">justice?</text><text start="4711.489" dur="4.19">Some theories of justice are your punishment
is not based on just what was right for that</text><text start="4715.679" dur="3.79">one person but creating an adequate deterrent
for the entire population because if you don&apos;t,</text><text start="4719.469" dur="1.0">what happens?</text><text start="4720.469" dur="6.0">So a classic example is Singapore&apos;s drug policy
is pretty harsh, right?</text><text start="4726.469" dur="5.571">Life in prison for just possession of drugs.</text><text start="4732.04" dur="6.71">Well, that was following the devastating effect
that the British had on the Chinese with the</text><text start="4738.75" dur="5.33">opium wars and recognizing how as a kind of
population-centric warfare, the British were</text><text start="4744.08" dur="3.85">able to influence, like, catastrophic damage
on China.</text><text start="4747.93" dur="3.33">They&apos;re like, we don&apos;t want that here and
we know that there are external forces that</text><text start="4751.26" dur="5.21">will push to do that kind of thing and it&apos;s
not just personal choice once there are asymmetric</text><text start="4756.47" dur="4.72">forces trying to affect the most vulnerable
people in the most vulnerable ways.</text><text start="4761.19" dur="5.48">So we&apos;re going to make it to where the deterrent
on drug use is so bad nobody will do it.</text><text start="4766.67" dur="4.59">So if you say that, you actually have to lock
somebody up forever for smoking pot, which</text><text start="4771.26" dur="1.689">feels very unfair to them.</text><text start="4772.949" dur="4.031">But you probably only have to do that like
a few times before nobody ever will fucking</text><text start="4776.98" dur="5.1">touch it because the deterrent is so bad and
they believe it will be enforced.</text><text start="4782.08" dur="7.34">And if the net effect on the society as a
whole is that you don&apos;t have black markets</text><text start="4789.42" dur="6.15">associated with drugs and gangs and the violence
that&apos;s associated and you don&apos;t have ODs and</text><text start="4795.57" dur="5.25">you don&apos;t have the susceptibility to population-centric
warfare and whatever, they might argue a utilitarian</text><text start="4800.82" dur="7.93">ethical calculus that the harsh punishment
was radically less harm to the total situation</text><text start="4808.75" dur="1.0">than not having it.</text><text start="4809.75" dur="1.09">So you have a strong deterrent.</text><text start="4810.84" dur="5.33">So that&apos;s just – I&apos;m not saying that I think
that is an adequate theory of justice, but</text><text start="4816.17" dur="2.23">it is a theory of justice, right?</text><text start="4818.4" dur="5.589">So let&apos;s say that the Sabbath was something
like this, and I&apos;m not saying that the rabbis</text><text start="4823.989" dur="4.21">that were creating it at the time thought
this, though many people suggest that that&apos;s</text><text start="4828.199" dur="3.221">probably what they thought.</text><text start="4831.42" dur="7.2">Some very competitive people wanting to get
ahead will work every day.</text><text start="4838.62" dur="6.67">They&apos;ll work seven days a week, and as a result,
they will be able to get a little bit more</text><text start="4845.29" dur="4.38">grain farming, whatever, then other people
get more surplus, start turning that into</text><text start="4849.67" dur="1.97">compounding benefits.</text><text start="4851.64" dur="3.92">And if anyone does, it will create a competitive
pressure where everyone has to.</text><text start="4855.56" dur="2.23">So nobody spends any time with their family.</text><text start="4857.79" dur="3.571">Nobody spends any time connecting to what
binds the culture together, the religious</text><text start="4861.361" dur="1.419">idea, et cetera.</text><text start="4862.78" dur="3.8">So we&apos;re going to make a Sabbath where no
one is even allowed to work, and there&apos;s such</text><text start="4866.58" dur="4.059">a harsh punishment against it that we&apos;re binding
the multipolar trap, right?</text><text start="4870.639" dur="4.341">Because even though it would make sense in
that person&apos;s rational interest to work that</text><text start="4874.98" dur="4.98">extra day a few times to get ahead, the net
effect on the society cumulatively is actually</text><text start="4879.96" dur="1.0">a shittier world.</text><text start="4880.96" dur="3.63">So we&apos;re going to bind it because people having
that time off to be with their family, each</text><text start="4884.59" dur="2.992">other, and studying ethics is a good idea.</text><text start="4887.582" dur="7.617">I would argue that religion has heaps of examples
like this of how do we bind our own behavior</text><text start="4895.199" dur="1.771">to be aligned with some ethic.</text><text start="4896.97" dur="4.31">But I would also argue – because that was
the question you were asking, right?</text><text start="4901.28" dur="2.419">Is there some kind of religious bind to the
multipolar trap?</text><text start="4903.699" dur="4.671">And I think the Sabbath is a good example.</text><text start="4908.37" dur="8.13">I think we can also show how well that didn&apos;t
work for Tibet when China invaded, right?</text><text start="4916.5" dur="5.35">Which is we want to be nonviolently oriented.</text><text start="4921.85" dur="2.66">We have a religion that&apos;s oriented towards
nonviolence.</text><text start="4924.51" dur="5.8">And we can see that there were – if you
think about it, the time of Genghis Khan or</text><text start="4930.31" dur="5.0">Alexander the Great or whatever where you
have a set of worldviews that doesn&apos;t constrain</text><text start="4935.31" dur="2.09">itself in that way.</text><text start="4937.4" dur="3.52">And it&apos;s going to go initiate conflict with
those people who didn&apos;t do anything to initiate</text><text start="4940.92" dur="2.39">it and don&apos;t want it.</text><text start="4943.31" dur="4.15">But the worldview that orients itself that
way also develops military capability and</text><text start="4947.46" dur="3.25">maximum extraction for the surplus to do that
thing.</text><text start="4950.71" dur="2.06">The other worldviews don&apos;t make it through.</text><text start="4952.77" dur="8.369">They get wiped out because – so there are
indigenous cultures and matriarchal cultures</text><text start="4961.139" dur="1.551">and whatever that we just don&apos;t even have
anymore.</text><text start="4962.69" dur="4.239">Don&apos;t even have the ideas around it because
it just got wiped out by warring cultures.</text><text start="4966.929" dur="3.741">And so does that produce the long-term world
we want?</text><text start="4970.67" dur="1.0">No.</text><text start="4971.67" dur="1.88">It doesn&apos;t either.</text><text start="4973.55" dur="7.1">And so there has been this kind of multipolar
trap on that the natural selection if you</text><text start="4980.65" dur="6.6">want to call it that of worldviews that make
it through are selected by their ability to</text><text start="4987.25" dur="4.04">grow their population, have outsized influence
on other population and win wars.</text><text start="4991.29" dur="4.05">And basically things that don&apos;t necessarily
map to a good world long-term.</text><text start="4995.34" dur="4.5">But the things that might map to a good world
long-term might not ever get to the long-term</text><text start="4999.84" dur="1.81">because they get wiped out in the short-term.</text><text start="5001.65" dur="1.87">Yeah, I don&apos;t buy that.</text><text start="5003.52" dur="3.78">So I&apos;m not saying this as someone who&apos;s religious
or from a religious perspective.</text><text start="5007.3" dur="1.87">Well, this is a religious perspective, sorry.</text><text start="5009.17" dur="3.11">But I&apos;m not saying this as someone who&apos;s advocating
for a certain religion.</text><text start="5012.28" dur="2.88">The most dominant religion in the world is
Christianity.</text><text start="5015.16" dur="3.789">And that&apos;s the story of someone who had the
government against him and he said, no, I&apos;m</text><text start="5018.949" dur="1.051">not going to fight back.</text><text start="5020.0" dur="2.39">In fact, if you want to persecute me, go ahead.</text><text start="5022.39" dur="2.09">I will come to you.</text><text start="5024.48" dur="4.659">And one of the most striking stories, literally
striking in the Bible to me is the story of</text><text start="5029.139" dur="3.911">Jesus the captor and Peter, his friend, cut
off the captor&apos;s ear.</text><text start="5033.05" dur="2.97">The guy was going to take Jesus to kill Jesus.</text><text start="5036.02" dur="6.25">And Jesus said, no, no, no, no, don&apos;t do that
and took the ear and healed his captor.</text><text start="5042.27" dur="1.96">So think about this, though.</text><text start="5044.23" dur="4.45">Yes, Jesus is the guy who said, let he who
has no sins cast the first stone and they</text><text start="5048.68" dur="2.59">brought Mary Magdalene and all those things.</text><text start="5051.27" dur="4.68">But we somehow did the Crusades in his name
and the Inquisition in his name and the Dark</text><text start="5055.95" dur="1.46">Ages in his name.</text><text start="5057.41" dur="1.0">Right.</text><text start="5058.41" dur="2.3">That&apos;s some weird ass mental gymnastics.</text><text start="5060.71" dur="3.989">But the scenes, the versions that we&apos;re going
to stay peaceful and not do Crusades, how</text><text start="5064.699" dur="3.911">many do you see around and how much power
did they get?</text><text start="5068.61" dur="4.81">So what happens is you have a bunch of different
interpretations, the interpretations that</text><text start="5073.42" dur="6.96">orient themselves to power and to propagation,
propagate and make it through the interaction</text><text start="5080.38" dur="1.339">between the memes.</text><text start="5081.719" dur="5.901">So memes engage in a kind of competitive selection
like genes do, but not individual memes, meme</text><text start="5087.62" dur="1.0">complexes.</text><text start="5088.62" dur="6.16">So if we have a religion that says, be humble,
be quiet, listen to people and don&apos;t push</text><text start="5094.78" dur="2.23">your ideas on anybody.</text><text start="5097.01" dur="3.209">And then you have another one that says, go
out and proselytize, convert everyone to your</text><text start="5100.219" dur="2.581">religion and kill the infidels.</text><text start="5102.8" dur="2.04">Which one gets more people involved?</text><text start="5104.84" dur="1.0">Right.</text><text start="5105.84" dur="7.64">And so the ones that have propagation and
that have conflict ideas built right in.</text><text start="5113.48" dur="4.47">So, of course, then the meme sets evolve over
time.</text><text start="5117.95" dur="1.0">Right.</text><text start="5118.95" dur="2.66">The religious interpretations don&apos;t stay the
same.</text><text start="5121.61" dur="6.12">And the meme sets that end up winning through
how they reduce themselves to the behaviors</text><text start="5127.73" dur="3.38">that affect war and population growth and
governance, et cetera, are all part of it.</text><text start="5131.11" dur="5.47">So the fact that the dude who said, let he
who has no sins among you cast the first stone</text><text start="5136.58" dur="5.77">got to be the religion that became dominant
through the Crusades and through violent expansionism</text><text start="5142.35" dur="5.619">and then through radical torturous oppression
is fascinating.</text><text start="5147.969" dur="1.0">Right.</text><text start="5148.969" dur="5.531">And it shows you that you have like a real
philosophy and then you have politics of power</text><text start="5154.5" dur="4.3">and you have fusions of those things that
you have to understand both of when you&apos;re</text><text start="5158.8" dur="1.0">studying religion.</text><text start="5159.8" dur="3.189">To me, and I don&apos;t mean to harp on this point,
but it doesn&apos;t have to be a choice between,</text><text start="5162.989" dur="5.101">hey, let me do good and let me not push my
views on anyone and proselytizing slash killing.</text><text start="5168.09" dur="3.98">Because you can also proselytize and say your
ideas and hopefully people will hopefully</text><text start="5172.07" dur="1.78">maybe there is something in us.</text><text start="5173.85" dur="1.36">Maybe there&apos;s something cosmically in us.</text><text start="5175.21" dur="1.0">I don&apos;t know.</text><text start="5176.21" dur="1.11">That says, hey, you know what?</text><text start="5177.32" dur="1.0">I like that.</text><text start="5178.32" dur="1.23">I don&apos;t like that killing.</text><text start="5179.55" dur="1.33">I don&apos;t like where that will lead.</text><text start="5180.88" dur="4.56">It resonates with me that the sins get passed
down or that the violence gets passed down</text><text start="5185.44" dur="1.2">and amplified.</text><text start="5186.64" dur="1.289">But I need to be told that.</text><text start="5187.929" dur="2.94">So I do need to hear that because I can&apos;t
come up with that on my own.</text><text start="5190.869" dur="2.911">So that&apos;s why I&apos;m saying the proselytizing
is a part of it, whether proselytizing is</text><text start="5193.78" dur="3.64">explicit or it&apos;s lived and you just see how
someone lives and then you inquire, hey, what</text><text start="5197.42" dur="1.049">are your views?</text><text start="5198.469" dur="2.381">And why are you so happy when you have nothing?</text><text start="5200.85" dur="1.83">And I&apos;m so miserable and I have everything.</text><text start="5202.68" dur="3.96">I just don&apos;t see it as a choice between you
do good locally and don&apos;t tell anyone about</text><text start="5206.64" dur="4.21">it or you can tell people about your ethical
system, but also oppress them.</text><text start="5210.85" dur="4.65">Well, to make the thought experiment, we picked
both extremes.</text><text start="5215.5" dur="6.02">So we can see that the Mormons proselytize,
but they don&apos;t kill everyone who disagrees</text><text start="5221.52" dur="2.969">in the crusading kind of way.</text><text start="5224.489" dur="1.991">They have not expanded as much as the crusades.</text><text start="5226.48" dur="5.53">They&apos;ve not got as much global dominance or
total population as a result.</text><text start="5232.01" dur="3.01">But they have not got nowhere.</text><text start="5235.02" dur="6.74">We can see that the ones that say, hey, if
someone is interested, we&apos;ll share, but we&apos;re</text><text start="5241.76" dur="4.459">not going to proselytize because we have a
certain humility of how much we don&apos;t know</text><text start="5246.219" dur="4.48">and a respect for everyone&apos;s choice.</text><text start="5250.699" dur="2.571">The mystery schools stay pretty small.</text><text start="5253.27" dur="6.28">And, again, when we were talking about asymmetries,
those who are more focused on the opportunity</text><text start="5259.55" dur="4.01">and downplay the risk move ahead, get the
investment capital, et cetera, and those who</text><text start="5263.56" dur="4.19">are focused on the risk heavily don&apos;t.</text><text start="5267.75" dur="8.35">There&apos;s a similar thing here, which is like
there&apos;s an asymmetry in the ideas that hit</text><text start="5276.1" dur="3.5">evolutionary drivers even if perverse forms.</text><text start="5279.6" dur="6.58">Like in the evolutionary environment, it was
where there was actual food scarcity.</text><text start="5286.18" dur="5.29">We evolved dopaminergic, dopamine opioid responses
to salt, fat, and sugar, which were hard to</text><text start="5291.47" dur="1.689">get and useful.</text><text start="5293.159" dur="3.591">As soon as we got to the point where we could
produce lots and lots of salt, fat, and sugar</text><text start="5296.75" dur="4.869">and there was no scarcity on those things,
our genetics didn&apos;t change.</text><text start="5301.619" dur="4.221">And so the fact that it felt really good when
you ate that and incentivized you to get more</text><text start="5305.84" dur="3.39">of it where that little bit of surplus might
mean you make it through the famine versus</text><text start="5309.23" dur="2.52">not, it was an adaptive response.</text><text start="5311.75" dur="5.929">Then we create a Anthropocene where we have
Hostess and McDonald&apos;s giving amounts of salt,</text><text start="5317.679" dur="5.21">fat, sugar that are – and the combinations
of them with the kind of optimized palatability</text><text start="5322.889" dur="6.111">where it is – not only is it not evolutionarily
useful to get it anymore, it is actually the</text><text start="5329.0" dur="3.8">primary cause of disease in the environments
where that&apos;s available.</text><text start="5332.8" dur="2.58">It doesn&apos;t mean that the dopaminergic signal
changed.</text><text start="5335.38" dur="6.09">So we&apos;re able to kind of take an evolutionary
signal and hijack it.</text><text start="5341.47" dur="3.79">And this is obviously what fast food does
to the evolutionary programs around food.</text><text start="5345.26" dur="3.419">It&apos;s what social media does to the impulses
for social connectivity.</text><text start="5348.679" dur="7.711">It&apos;s what porn does for the impulses to sexual
connection associated with intimacy and procreation</text><text start="5356.39" dur="5.019">and all like that is to extract the hypernormal
stimuli from the rest of what makes it actually</text><text start="5361.409" dur="2.111">evolutionarily fit.</text><text start="5363.52" dur="1.849">Same thing can happen with religion, right?</text><text start="5365.369" dur="5.901">You can offer people an artificial sense of
certainty and offer them an artificial sense</text><text start="5371.27" dur="10.46">of belonging and security and various things
like that and without much actual deep philosophic</text><text start="5381.73" dur="6.29">consideration or necessarily even deep numinous
experience.</text><text start="5388.02" dur="5.969">And that similarly has the ability to scale
more quickly than something where you want</text><text start="5393.989" dur="5.861">people to actually understand deeply, discover
things themselves, have integrated experiences,</text><text start="5399.85" dur="4.21">not just do the right action but for the right
intrinsically emerging reasons.</text><text start="5404.06" dur="13.48">Which is why your podcast doesn&apos;t have one
of the – as many views as the most trending</text><text start="5417.54" dur="6.25">TikTok videos that require less work and are
shorter and more oriented to hypernormal stimuli.</text><text start="5423.79" dur="2.26">So I&apos;m not saying we can&apos;t work with these
things.</text><text start="5426.05" dur="1.97">I&apos;m saying these are the things we have to
work with.</text><text start="5428.02" dur="8.07">So we&apos;re in a situation where the – let&apos;s
say that we&apos;re in group – in groups and</text><text start="5436.09" dur="5.109">out groups would both cooperate and compete
at different times based on what game theory</text><text start="5441.199" dur="2.851">seemed to make most sense.</text><text start="5444.05" dur="4.52">And they would typically cooperate while reserving
the right to compete.</text><text start="5448.57" dur="2.79">And to even fully defect if they need to,
right?</text><text start="5451.36" dur="1.68">Resource scarcity or something.</text><text start="5453.04" dur="6.88">Or just a sociopath coming into leadership,
which totally happens.</text><text start="5459.92" dur="4.25">So the combination of the worldviews – everybody
needs to believe our religion.</text><text start="5464.17" dur="4.89">If they don&apos;t, they are bad and so we&apos;re going
to convert them or whatever, right?</text><text start="5469.06" dur="4.19">Or everyone needs to be – have a democracy
because that&apos;s good and all other forms of</text><text start="5473.25" dur="2.13">governance are bad or whatever it is.</text><text start="5475.38" dur="2.83">There&apos;s ideology that orients itself.</text><text start="5478.21" dur="4.23">There&apos;s a tech stack that is a part of the
capacity to do that.</text><text start="5482.44" dur="2.95">There are coordination mechanisms that are
part of that.</text><text start="5485.39" dur="5.4">So the full stack of the superstructure, the
worldviews, the social structure and the infrastructure</text><text start="5490.79" dur="3.81">are what are engaged in in-group, out-group
competitions and that are upregulating largely</text><text start="5494.6" dur="2.67">shaped by those competitions.</text><text start="5497.27" dur="5.429">It just happens to be that the version that
makes it through that shaping process is also</text><text start="5502.699" dur="4.69">orienting us towards a whole suite of global
catastrophic risks.</text><text start="5507.389" dur="2.79">It is basically self-terminating.</text><text start="5510.179" dur="4.44">And so it has been the case that you have
to win the local arms race because otherwise</text><text start="5514.619" dur="1.471">you lose.</text><text start="5516.09" dur="4.98">But the arms races that are externalizing
harm but on an exponential curve that have</text><text start="5521.07" dur="7.23">cumulative effects, you don&apos;t actually get
to keep externalizing on an exponential curve</text><text start="5528.3" dur="6.189">or running arms races on an exponential curve
in a finite space forever.</text><text start="5534.489" dur="5.801">So we&apos;re at this interesting space where you
can&apos;t try to build an alternate world that</text><text start="5540.29" dur="6.1">just loses but you also can&apos;t keep trying
to win in the same definition of win.</text><text start="5546.39" dur="4.57">This is the interesting point we&apos;re at which
is we have to actually build a version of</text><text start="5550.96" dur="5.41">win that is not for an in-group in relationship
to an out-group but is something that actually</text><text start="5556.37" dur="4.43">allows some kind of omni-win that gets us
out of those multipolar traps.</text><text start="5560.8" dur="4.11">And this was all coming from the topic of
you starting with why you brought up the UN</text><text start="5564.91" dur="8.37">and that you have to deal with these things
with some kind of sense of how are other people</text><text start="5573.28" dur="4.03">dealing with them and how does that affect
the choice-making process.</text><text start="5577.31" dur="4.0">Some people would say, look, we&apos;re group selected
and then we can make our group to be the tribe</text><text start="5581.31" dur="1.25">versus another tribe.</text><text start="5582.56" dur="4.81">And one of the solutions is if there was aliens
and then we could bind together as humans</text><text start="5587.37" dur="1.43">and fight something external.</text><text start="5588.8" dur="1.0">It doesn&apos;t have to be aliens.</text><text start="5589.8" dur="1.51">The point is that there needs to be something
external.</text><text start="5591.31" dur="4.02">So you&apos;re saying there&apos;s another option and
that that option, the bind together in order</text><text start="5595.33" dur="3.73">to fight some other out-group, whether the
group is something physical or it could be</text><text start="5599.06" dur="5.18">more abstract, that that&apos;s not something that
should be pursued and there&apos;s another option.</text><text start="5604.24" dur="8.59">I didn&apos;t say that, but it&apos;s an interesting
conversation.</text><text start="5612.83" dur="7.099">If we are not binding in groups to fight out-groups,
so this is kind of like Machiavelli&apos;s enemy</text><text start="5619.929" dur="6.061">hypothesis that people are kind of evolutionarily
tribal and that to unify a lot of people at</text><text start="5625.99" dur="6.439">a much larger than tribal scale, given that
they naturally will find their own differences</text><text start="5632.429" dur="7.411">and conflicts and reasons to otherize somebody
because they have more influence over their</text><text start="5639.84" dur="1.59">own small group or whatever.</text><text start="5641.43" dur="4.939">To unify them works best if you have a shared
enemy that forces them to unify.</text><text start="5646.369" dur="8.111">And so then you eventually – of course this
makes small tribes unify to deal with a larger</text><text start="5654.48" dur="6.25">tribe and then you get kingdoms and nation
states and global economic trading blocks</text><text start="5660.73" dur="4.78">and eventually you get great superpower conflicts.</text><text start="5665.51" dur="6.51">And that if the only way to unify, that if
groups opposing each other in that way ends</text><text start="5672.02" dur="6.01">up being catastrophic for the world, so we
want to get everybody unified in some way,</text><text start="5678.03" dur="2.18">do we need a shared enemy?</text><text start="5680.21" dur="3.84">Obviously this has been talked about a gazillion
times.</text><text start="5684.05" dur="3.69">Can climate change or environmental harm be
the shared enemy?</text><text start="5687.74" dur="1.68">Not really.</text><text start="5689.42" dur="4.719">Even if everyone believed in it, which they
don&apos;t, it doesn&apos;t hit people&apos;s agency bias</text><text start="5694.139" dur="2.611">in the same way and whatever.</text><text start="5696.75" dur="4.73">Could we stage a false flag alien invasion
that may unify?</text><text start="5701.48" dur="11.82">Of course this has actually been an explored
topic both in sci-fi and reality and it&apos;s</text><text start="5713.3" dur="9.859">how deeply explored is a question but yes,
it&apos;s a very natural topic to explore that</text><text start="5723.159" dur="6.651">something like a attack from the outside would
allow that kind of unification.</text><text start="5729.81" dur="5.1">Because of that, there are people who are
very skeptical and concerned of anything that</text><text start="5734.91" dur="6.34">looks like a presented shared threat that
should create some unified response because</text><text start="5741.25" dur="4.62">then they&apos;re like, well, what is the government
that regulate – that will navigate that</text><text start="5745.87" dur="4.25">shared threat and who has any checks and balances
on that if that thing becomes captured or</text><text start="5750.12" dur="2.22">corrupt?</text><text start="5752.34" dur="3.27">And so this is, again, the catastrophes or
dystopias.</text><text start="5755.61" dur="5.22">If you don&apos;t have some coordination, you get
these problems of coordination failure.</text><text start="5760.83" dur="3.96">If your coordination is imposed, you end up
getting oppression dynamics.</text><text start="5764.79" dur="6.79">So how do you get coordination that is global
but that is emergent, that keeps local power</text><text start="5771.58" dur="6.119">from doing things that drive multipolar traps
but that also ensures that you don&apos;t get centralized</text><text start="5777.699" dur="3.371">power that can be captured or corrupted?</text><text start="5781.07" dur="3.85">A system of coordination has to address both
of those things.</text><text start="5784.92" dur="8.86">And as we move into more people with more
resource consumption per capita and the cumulative</text><text start="5793.78" dur="5.27">tipping points on the biosphere being hit,
but even more than that, exponentially more</text><text start="5799.05" dur="3.99">power available to exponentially more actors.</text><text start="5803.04" dur="5.71">Obviously, if we look at the history of how
humans have used power and you put an exponential</text><text start="5808.75" dur="2.18">curve on that, it doesn&apos;t go well.</text><text start="5810.93" dur="8.19">So, yeah, that&apos;s one way of thinking about
the coordination issue.</text><text start="5819.12" dur="4.65">When we were thinking about the UN or whatever
is this global agency, potentially, the phrase,</text><text start="5823.77" dur="1.65">they have no checks and balances comes up.</text><text start="5825.42" dur="4.88">Is there a way of organizing something that
is global and influential that has its own</text><text start="5830.3" dur="1.55">internal checks and balances?</text><text start="5831.85" dur="2.53">I don&apos;t understand how the US political system
works.</text><text start="5834.38" dur="2.87">It&apos;s my understanding that it&apos;s tripartite
and antagonistic.</text><text start="5837.25" dur="1.54">I don&apos;t understand the details of it.</text><text start="5838.79" dur="1.699">I&apos;m apolitical, at least consciously.</text><text start="5840.489" dur="1.0">I haven&apos;t looked into it.</text><text start="5841.489" dur="2.241">But the point is, I can, that&apos;s interesting.</text><text start="5843.73" dur="1.5">I don&apos;t know how that works.</text><text start="5845.23" dur="4.11">I wonder how much that doesn&apos;t work, how much
that can be accelerated, amplified.</text><text start="5849.34" dur="11.84">Well, one point that we bring up is that any
proposed system of coordination, governance,</text><text start="5861.18" dur="5.34">whatever, is not going to work the same way
after it&apos;s been running for a long time as</text><text start="5866.52" dur="10.329">when it was initially developed because all
of the systems have a certain kind of institutional</text><text start="5876.849" dur="1.781">decay or entropy built in.</text><text start="5878.63" dur="6.85">It has to be considered because every vested
interest that is being bound has a vested</text><text start="5885.48" dur="3.57">interest in figuring out how to break the
control system or capture or corrupt it or</text><text start="5889.05" dur="1.72">something.</text><text start="5890.77" dur="4.69">And so it&apos;s not just how do we build a system
that does that, but it&apos;s also how do we build</text><text start="5895.46" dur="4.63">a system that continues to upregulate itself
to deal with an increasingly complex, different</text><text start="5900.09" dur="4.23">world than the one it was originally designed
for and that continues to deal with the fact</text><text start="5904.32" dur="3.28">that wherever there is an incentive to gain,
the system is going to happen.</text><text start="5907.6" dur="5.039">So you have to not only figure out a system
that makes sense currently, but a system that</text><text start="5912.639" dur="7.131">has an adaptive intelligence that is adequate
for the changing landscape.</text><text start="5919.77" dur="8.8">So when you look at the U.S., because leaving
corrupt monarchy was key to the founding here,</text><text start="5928.57" dur="4.76">and so we were going to try to do this democracy,
non-monarchy thing, it was also the result</text><text start="5933.33" dur="1.349">of a change in tech, right?</text><text start="5934.679" dur="7.381">It was a result of the printing press where
rather than – before a printing press and</text><text start="5942.06" dur="7.099">everyone could not have textbooks and couldn&apos;t
have newspapers and to have access to information,</text><text start="5949.159" dur="4.631">someone had to copy a book by hand, which
meant that there were very few of them who</text><text start="5953.79" dur="3.17">would copy the information by hand so only
the wealthy could have it.</text><text start="5956.96" dur="4.529">The idea of a wealthy nobility class that
got educated enough to make good choices for</text><text start="5961.489" dur="3.88">everyone else where if they were too corrupt,
the people would overthrow them, so there</text><text start="5965.369" dur="3.171">was certain kind of checks and balance that
kind of maybe made sense, right?</text><text start="5968.54" dur="5.77">With the noblesse oblige built in, the obligation
of the nobility class to rule well.</text><text start="5974.31" dur="3.4">I&apos;m not saying it did, but that&apos;s at least
the story.</text><text start="5977.71" dur="3.58">But as soon as the printing press comes and
now everybody could have textbooks and get</text><text start="5981.29" dur="5.39">educated and everybody could have a newspaper
and know what&apos;s going on, it kind of debases</text><text start="5986.68" dur="4.499">the idea that you need a nobility class to
make all the choices because everyone else</text><text start="5991.179" dur="1.131">doesn&apos;t know what&apos;s really going on.</text><text start="5992.31" dur="5.75">And you say, well, maybe we could all get
educated enough to understand how to process</text><text start="5998.06" dur="3.971">information and we could all get news to be
able to understand what&apos;s going on and all</text><text start="6002.031" dur="1.019">have a say.</text><text start="6003.05" dur="5.2">And so obviously democracy emerged following
that change in information tech.</text><text start="6008.25" dur="4.239">I&apos;m saying this because, of course, AI is
a radical change in information tech that</text><text start="6012.489" dur="4.761">will also obliterate our existing political
economies and coordination systems and make</text><text start="6017.25" dur="3.33">new ones and changes to culture as well.</text><text start="6020.58" dur="4.5">The difference in the AI case, just briefly,
is that I don&apos;t see the AI as democratizing</text><text start="6025.08" dur="4.44">more so than exacerbating the inequality in
terms of like, so if you&apos;re extremely bright,</text><text start="6029.52" dur="4.25">the amount of information you can process
is going to be far outpacing someone who either</text><text start="6033.77" dur="5.88">is not so bright or gets access to that AI
three weeks later.</text><text start="6039.65" dur="8.18">So thinking through in the same way that the
printing press had an effect on central religion</text><text start="6047.83" dur="4.869">through everybody can have a Bible and read
it and learn on their own and kind of Lutheran</text><text start="6052.699" dur="7.771">revolution and it had an effect on central
government in the form of feudalism.</text><text start="6060.47" dur="7.09">We can then look at kind of McLuhan&apos;s insights
of how information tech changes the nature</text><text start="6067.56" dur="5.53">of the collective intelligence and motivation
and type of mind that everyone is operating</text><text start="6073.09" dur="1.0">with.</text><text start="6074.09" dur="5.21">And as a result, the emergent type of society,
we can look at the way that the internet and</text><text start="6079.3" dur="2.0">digital have already done that.</text><text start="6081.3" dur="3.45">Looking at the way social media has affected
media, for instance, which affects our democratic</text><text start="6084.75" dur="2.29">systems is a pretty obvious one.</text><text start="6087.04" dur="4.78">But then we can look at AI and not just AI,
but different types of AI, different ways</text><text start="6091.82" dur="1.0">it could develop.</text><text start="6092.82" dur="2.39">LLM is very different than other kinds of
AI.</text><text start="6095.21" dur="1.58">So we&apos;ll come to that in a moment.</text><text start="6096.79" dur="2.58">But let&apos;s come back to the other question
because you were asking the checks and balances</text><text start="6099.37" dur="2.39">one.</text><text start="6101.76" dur="12.34">So the idea in the US system was the British
system following the Magna Carta and the Treaty</text><text start="6114.1" dur="6.61">of Forest and whatever was supposed to be
the most ideal noble thing around and ended</text><text start="6120.71" dur="3.14">up being in their experience a totally corrupt
thing.</text><text start="6123.85" dur="4.61">So the idea that no matter how you develop
a system, it can be corrupted.</text><text start="6128.46" dur="1.0">That was built in.</text><text start="6129.46" dur="3.73">So how do we make sure that no part gets too
much power and that we have checks and balances</text><text start="6133.19" dur="1.85">throughout was kind of key.</text><text start="6135.04" dur="5.25">So before you even get into the three branches
of government, you already have the separation</text><text start="6140.29" dur="8.21">of the state and the church, which was already
a key part, and you have the separation of</text><text start="6148.5" dur="8.659">the market and the state, which is the – you
have a liberal democracy that is proposed.</text><text start="6157.159" dur="3.961">So you don&apos;t have a pure market function,
but you also don&apos;t have that the state is</text><text start="6161.12" dur="3.17">running the entire economy.</text><text start="6164.29" dur="10.04">And so the separation of the market, the state,
the church, there&apos;s a few other ways of thinking</text><text start="6174.33" dur="2.96">about separation was already a part of it.</text><text start="6177.29" dur="4.05">And then with regard to the state&apos;s function,
the separation of the legislative, the judicial,</text><text start="6181.34" dur="2.54">and the executive were critical.</text><text start="6183.88" dur="6.23">And then within each of those, within the
legislative, a bicameral breakdown was really</text><text start="6190.11" dur="1.0">important.</text><text start="6191.11" dur="6.06">And that then the 10th Amendment was to push
as much power, the subsidiary principle, to</text><text start="6197.17" dur="2.84">the states as possible and as little to the
federal.</text><text start="6200.01" dur="5.74">So there were many, many steps of checks and
balances on concentrated power that were built</text><text start="6205.75" dur="1.0">into the system.</text><text start="6206.75" dur="5.54">But of course, everyone who is smart, who
is also agentic, who wants more power, looks</text><text start="6212.29" dur="6.58">for loopholes and or figures out how to write
laws and to get them passed, right, doing</text><text start="6218.87" dur="1.5">legislation and lobbying.</text><text start="6220.37" dur="6.4">And of course, corporations can pay for a
lot more lawyers than an average citizen can</text><text start="6226.77" dur="3.87">or than a nonprofit group that doesn&apos;t have
a revenue stream associated.</text><text start="6230.64" dur="7.209">So the group that is trying to turn commons
into commodities versus one that&apos;s trying</text><text start="6237.849" dur="5.001">to protect the commons will inherently have
a bigger revenue stream to employ media to</text><text start="6242.85" dur="4.36">change everyone&apos;s mind or to employ campaign
budgets or to employ lobbyists or whatever.</text><text start="6247.21" dur="7.27">So you end up seeing that there is a progressive
kind of loophole-finding corruption because</text><text start="6254.48" dur="8.35">the underlying incentive systems, invested
interests, are still there, right?</text><text start="6262.83" dur="4.82">Baudrillard simulation and simulacra that
discusses the steps of the degradation from</text><text start="6267.65" dur="6.38">a new system to how it eventually devolves
into mostly a simulation of what it originally</text><text start="6274.03" dur="4.29">was is a good analysis on this we could discuss.</text><text start="6278.32" dur="5.28">But – so that&apos;s a little bit on kind of
the history of checks and balances on power.</text><text start="6283.6" dur="4.4">But I don&apos;t think anybody looks at our current
U.S. system and says it&apos;s doing a great job</text><text start="6288.0" dur="1.82">of that.</text><text start="6289.82" dur="4.81">And there&apos;s a bunch of reasons in addition
to the one that I said about how there is</text><text start="6294.63" dur="6.38">a natural process of figuring out how to influence
this.</text><text start="6301.01" dur="5.37">Like everyone who – okay, there&apos;s one other
part that&apos;s actually worth saying.</text><text start="6306.38" dur="1.72">So you have a state.</text><text start="6308.1" dur="1.09">You have a market.</text><text start="6309.19" dur="15.39">And you have the people as members of a democratic
government, meaning their function in state,</text><text start="6324.58" dur="2.099">not their function in market.</text><text start="6326.679" dur="1.92">So a government of, for, and by the people.</text><text start="6328.599" dur="5.661">The people might not all be representatives,
but they can all speak to their representative,</text><text start="6334.26" dur="2.32">decide how it votes, those types of things,
right?</text><text start="6336.58" dur="5.539">So there&apos;s supposed to be a check and balance
between these three that the main reason that</text><text start="6342.119" dur="4.501">there is law is to prevent some people or
groups of people from doing things that they</text><text start="6346.62" dur="4.05">have an incentive to do that would suck for
everybody else.</text><text start="6350.67" dur="9.64">Obviously, whether it&apos;s individual stealing
or murder or whatever, or it&apos;s a corporation</text><text start="6360.31" dur="4.0">cutting down the national forest or polluting
the waterways too much, there is – somebody</text><text start="6364.31" dur="2.63">has an incentive to do something.</text><text start="6366.94" dur="5.54">And in a democracy where the idea is supposed
to be that the – we all want and value different</text><text start="6372.48" dur="6.369">things, but the collective will of the people
as determined through some voting process</text><text start="6378.849" dur="6.121">gets instantiated into law where a monopoly
of violence can back that up.</text><text start="6384.97" dur="2.01">That&apos;s kind of core to the idea of a liberal
democracy, right?</text><text start="6386.98" dur="5.9">I&apos;m not arguing that it is a good system,
but I&apos;m arguing for the core logic of it.</text><text start="6392.88" dur="4.941">And it&apos;s because the recognition that if we
just had a pure market system, the reason</text><text start="6397.821" dur="4.798">why there wasn&apos;t just a pure kind of laissez-faire
system even though the people building this</text><text start="6402.619" dur="6.02">understood, at least their expressed reason,
is in a pure type market dynamic, as you were</text><text start="6408.639" dur="3.871">mentioning with AI, some people are way better
at it than other people.</text><text start="6412.51" dur="3.46">And as a result, we&apos;ll just end up getting
a lot more money that they can convert to</text><text start="6415.97" dur="4.81">more land, resources, employees, et cetera,
and you end up getting a power law distribution</text><text start="6420.78" dur="3.74">on wealth, which is a power law distribution
on everything, and these people&apos;s interests</text><text start="6424.52" dur="3.47">end up determining the whole society and these
people&apos;s interests are pretty determined for</text><text start="6427.99" dur="1.249">them.</text><text start="6429.239" dur="4.391">And so if you want to create protections for
these people at all, and that was basically</text><text start="6433.63" dur="4.109">the King George situation and the inspiration
for the Declaration of Independence and leaving,</text><text start="6437.739" dur="3.311">which was there was too much concentrated
power and it was kind of fucked, so how do</text><text start="6441.05" dur="1.45">we make that not happen?</text><text start="6442.5" dur="5.469">Well, since we know that the market is going
to kind of naturally do that, let&apos;s create</text><text start="6447.969" dur="4.02">a state that is more powerful than any market
actor.</text><text start="6451.989" dur="3.771">And let&apos;s make sure that the state reflects
the values of all the people.</text><text start="6455.76" dur="4.65">So the little guys get to unify themselves
through a vote, right?</text><text start="6460.41" dur="3.24">And then you get to have a representative
that represents everybody.</text><text start="6463.65" dur="4.56">It&apos;s the only one given a monopoly of violence
and it gets to make sure that any more powerful</text><text start="6468.21" dur="1.31">actors are checked.</text><text start="6469.52" dur="1.079">That&apos;s kind of the idea.</text><text start="6470.599" dur="1.0">Yeah.</text><text start="6471.599" dur="3.471">So, so far, this is an account of how it&apos;s
been like a history lesson, but you aren&apos;t</text><text start="6475.07" dur="3.44">saying this is how it should continue to be,
nor this is how it&apos;s operating in its ideal</text><text start="6478.51" dur="1.0">sense currently.</text><text start="6479.51" dur="3.379">Are you just saying that this was the reasoning
behind it?</text><text start="6482.889" dur="1.651">One key part of how it broke down.</text><text start="6484.54" dur="3.78">So the idea is that the market, people will
have incentives to do things that are good</text><text start="6488.32" dur="4.089">for them that might suck for the environment
or others.</text><text start="6492.409" dur="5.481">And so others have the ability to agree upon
laws that will bind those actors to not do</text><text start="6497.89" dur="1.61">that thing, right?</text><text start="6499.5" dur="5.449">So the state is supposed to check the market,
let the market do its thing, do resource distribution</text><text start="6504.949" dur="5.511">productivity, let it do that because it&apos;s
good, but check the particularly fucked applications.</text><text start="6510.46" dur="6.699">And in order for the state to check the market,
the people are supposed to check the state</text><text start="6517.159" dur="3.341">and ensure that the state is actually doing
the thing that it&apos;s supposed to do and that</text><text start="6520.5" dur="4.56">the representatives aren&apos;t corrupt and taking
back-end deals and all those kinds of things,</text><text start="6525.06" dur="1.15">right?</text><text start="6526.21" dur="3.29">And then there&apos;s a way in which the market
kind of checks the people, meaning that the</text><text start="6529.5" dur="3.65">people can&apos;t – the accounting checks them.</text><text start="6533.15" dur="4.85">They can&apos;t vote themselves more rights than
they&apos;re willing to take responsibility for.</text><text start="6538.0" dur="2.94">They can&apos;t make the economics of the whole
situation not work, right?</text><text start="6540.94" dur="3.43">They can&apos;t vote themselves a bunch.</text><text start="6544.37" dur="7.32">They can&apos;t – if the people all say, yes,
we should all get no taxes but lots of social</text><text start="6551.69" dur="7.17">services, then the accounting is what actually
checks the people, right?</text><text start="6558.86" dur="3.779">So that&apos;s the idea of how you have this kind
of self-stabilizing thing.</text><text start="6562.639" dur="5.131">But of course the people stopped checking
the market once we were out of kind of the</text><text start="6567.77" dur="2.68">sense of an eminent need for revolution.</text><text start="6570.45" dur="4.19">Then the people have a lot of shit to do other
than really pay attention to government in</text><text start="6574.64" dur="1.0">detail.</text><text start="6575.64" dur="3.13">And there&apos;s a bunch of other reasons beyond
the scope of this conversation why the people</text><text start="6578.77" dur="3.71">stopped checking the government, in which
case the market is continuously trying to</text><text start="6582.48" dur="4.369">influence the government through lobbying
and legislation and campaign finance and all</text><text start="6586.849" dur="1.691">those other things.</text><text start="6588.54" dur="4.5">And so then you end up getting regulatory
capture rather than regulatory effectiveness,</text><text start="6593.04" dur="1.0">right?</text><text start="6594.04" dur="3.78">So when you put those checks and balances,
it&apos;s going to change.</text><text start="6597.82" dur="4.05">When everyone is scared of concentrated power
following a revolution, it&apos;s different than</text><text start="6601.87" dur="5.059">four generations later where nobody actually
feels that fear anymore and is busy doing</text><text start="6606.929" dur="1.42">other shit, right?</text><text start="6608.349" dur="2.35">So it&apos;s not just how you build your system.</text><text start="6610.699" dur="5.501">It&apos;s how do you build a system where the initial
people that went through the difficult thing</text><text start="6616.2" dur="5.05">to build it when they die, you didn&apos;t just
pass on the system but the generator function</text><text start="6621.25" dur="6.41">of the kinds of insights needed to keep updating
and evolving the system under an evolving</text><text start="6627.66" dur="1.45">context.</text><text start="6629.11" dur="5.1">So when you ask the question about could such
a thing be built at an international level</text><text start="6634.21" dur="4.17">where there are checks and balances, the answer
is it&apos;s super hard.</text><text start="6638.38" dur="1.68">But yes.</text><text start="6640.06" dur="3.55">But it&apos;s not just can you design it properly
up front.</text><text start="6643.61" dur="4.921">It&apos;s also can you factor how that system then
even if well intended at first, it&apos;s kind</text><text start="6648.531" dur="1.839">of like all technologies dual use.</text><text start="6650.37" dur="7.059">So you build the gene editing for immuno-oncology
but then it can be used for bioweapons.</text><text start="6657.429" dur="2.901">You have to not just think about what you&apos;re
building it for but all the things that will</text><text start="6660.33" dur="1.67">happen having created that thing.</text><text start="6662.0" dur="1.0">Same thing with government.</text><text start="6663.0" dur="4.88">You have to think about not just who you&apos;re
building it for right now but as the landscape</text><text start="6667.88" dur="2.01">changes, culture changes, can this thing be
corrupted?</text><text start="6669.89" dur="5.84">Can it be captured in future different contexts
and how do you build in immune systems to</text><text start="6675.73" dur="1.16">that?</text><text start="6676.89" dur="3.44">And that sort of thinking seems to be missing
with the development of AI.</text><text start="6680.33" dur="3.79">And it reminds me, I&apos;ve said this several
times, like the development of the bomb where</text><text start="6684.12" dur="4.599">Feynman and Oppenheimer, mainly Feynman and
his peers, said they didn&apos;t think about what</text><text start="6688.719" dur="1.0">they were creating.</text><text start="6689.719" dur="2.321">They were thinking we&apos;re having fun speaking
about these topics.</text><text start="6692.04" dur="2.77">It&apos;s even more fun to do research on these
topics.</text><text start="6694.81" dur="3.69">Einstein said like I&apos;d burn my hands had I
known that this was what was going to be developed.</text><text start="6698.5" dur="1.14">I wasn&apos;t thinking about that.</text><text start="6699.64" dur="3.18">I wasn&apos;t thinking about the consequences and
Feynman said something similar.</text><text start="6702.82" dur="3.91">We&apos;re consumed with the achieving of a goal
and we&apos;re not thinking about what would occur</text><text start="6706.73" dur="1.58">as a consequence once we attain it.</text><text start="6708.31" dur="4.74">And you hear this constantly in the AI scene,
channels like Two Minute Papers that say,</text><text start="6713.05" dur="1.2">what a time to be alive.</text><text start="6714.25" dur="3.869">That&apos;s like his catchphrase, what a time to
be alive, like encouraging and amazed constantly</text><text start="6718.119" dur="1.671">thinking, what is this going to be like?</text><text start="6719.79" dur="4.61">Two Papers down the line said enthusiastically,
I see little caution expressed.</text><text start="6724.4" dur="1.0">Yes.</text><text start="6725.4" dur="2.449">Jeez Louise, like what the heck are we building
and should we?</text><text start="6727.849" dur="1.961">Just because we could, should we?</text><text start="6729.81" dur="5.53">Well, the people who express caution, this
now relates to this asymmetry, said if people</text><text start="6735.34" dur="8.109">are like, hey, this is extremely risky technology,
we need to</text><text start="6743.449" dur="2.041">understand the risk space very deeply first.</text><text start="6745.49" dur="7.01">We need to ensure that the development of
the technology and then its future use by</text><text start="6752.5" dur="6.6">everybody is safe enough to be worth built.</text><text start="6759.1" dur="6.25">Those people end up running non-profits because
there&apos;s no upside to that.</text><text start="6765.35" dur="1.769">There&apos;s no immediate capital upside to that.</text><text start="6767.119" dur="3.611">So they have a hard time getting the capital
to get really good researchers or big enough</text><text start="6770.73" dur="3.71">computers and data sets to try to run stuff
on for trials.</text><text start="6774.44" dur="3.02">And the people that are like, oh, there&apos;s
a market application to this have a much easier</text><text start="6777.46" dur="5.48">time getting a massive GPU cluster and a lot
of talent and a lot of data.</text><text start="6782.94" dur="4.96">And so we can see this if you name the names
that are out there and their views and then</text><text start="6787.9" dur="4.68">map them to the types of organizations they
run and the type of motivational or cognitive</text><text start="6792.58" dur="3.889">bias it somewhat maps.</text><text start="6796.469" dur="2.641">Right.</text><text start="6799.11" dur="2.64">So.</text><text start="6801.75" dur="2.79">What is.</text><text start="6804.54" dur="3.98">This is what we intended to talk about all
of this has been interesting preface.</text><text start="6808.52" dur="1.94">What is the actual risk space with AI?</text><text start="6810.46" dur="1.0">What do we know?</text><text start="6811.46" dur="1.0">What do we.</text><text start="6812.46" dur="1.23">Not know, how should we think about it?</text><text start="6813.69" dur="4.48">How should we proceed, especially given that
AI is a lot of different things?</text><text start="6818.17" dur="3.21">Should we dive in there now?</text><text start="6821.38" dur="1.0">Sure.</text><text start="6822.38" dur="4.43">I just wanted to point out that although this
seems like a disagreement between you and</text><text start="6826.81" dur="1.5">I on the surface, there&apos;s an agreement.</text><text start="6828.31" dur="2.48">So again, I&apos;m not a Mormon.</text><text start="6830.79" dur="5.27">But I don&apos;t see the Mormons failure because
they go and they say, hey, you should whatever</text><text start="6836.06" dur="4.35">they say, act right, be humble, be kind, don&apos;t
overconsume and so on.</text><text start="6840.41" dur="1.18">But then their religion doesn&apos;t grow.</text><text start="6841.59" dur="4.319">I don&apos;t see that as a failure of them because
maybe their religion is larger than just being</text><text start="6845.909" dur="1.0">Mormon.</text><text start="6846.909" dur="3.25">It&apos;s something about the values that are spread
and then they send out these values and they</text><text start="6850.159" dur="3.981">filter through the community in the same way
that these nonprofits, just because they&apos;re</text><text start="6854.14" dur="3.959">not the largest, doesn&apos;t mean that the values
that they send out don&apos;t influence you and</text><text start="6858.099" dur="1.931">I and influence the people who are listening,</text><text start="6860.03" dur="2.45">who then act differently because of these
values.</text><text start="6862.48" dur="5.33">We have no idea how much the pacifism of Tolstoy
has influenced you hugging your father and</text><text start="6867.81" dur="5.69">your brother and the positive sentiment we
have generally speaking in society toward</text><text start="6873.5" dur="1.0">decentralization.</text><text start="6874.5" dur="3.67">And it also reminds me of the Cassandras for
people who don&apos;t know what a Cassandra is.</text><text start="6878.17" dur="1.69">It&apos;s someone who makes a prediction.</text><text start="6879.86" dur="1.0">It&apos;s a doomsayer.</text><text start="6880.86" dur="1.0">It&apos;s akin to a doomsayer.</text><text start="6881.86" dur="2.28">They&apos;re the opposite of a self-fulfilling
belief.</text><text start="6884.14" dur="3.7">So self-fulfilling belief is one where you
state it and you create the conditions such</text><text start="6887.84" dur="1.37">that it becomes true.</text><text start="6889.21" dur="5.67">Whereas in the best case for the people who
say the world is going to end, well, their</text><text start="6894.88" dur="5.83">success depends on them being self-sacrificing,
depends on us then being able to repudiate</text><text start="6900.71" dur="1.0">them.</text><text start="6901.71" dur="1.0">Let the world hasn&apos;t ended.</text><text start="6902.71" dur="4.0">We have no idea how much the doomsayers or
the fear mongerers said something that made</text><text start="6906.71" dur="4.25">us straighten up and act right just enough
that it pushed us off of the brink and influenced</text><text start="6910.96" dur="1.33">us to make society live.</text><text start="6912.29" dur="3.77">And so we then have this archetype of the
cautious and contumacious false cravens of</text><text start="6916.06" dur="1.0">the</text><text start="6917.06" dur="1.0">past.</text><text start="6918.06" dur="3.13">It&apos;s just not clear that because they have
died doesn&apos;t mean that they&apos;re unsuccessful.</text><text start="6921.19" dur="1.12">That&apos;s what I mean.</text><text start="6922.31" dur="1.0">Yeah.</text><text start="6923.31" dur="2.92">So there&apos;s this important point.</text><text start="6926.23" dur="13.639">The idea that even though Greece didn&apos;t continue
its empire relative to Rome, that its memes</text><text start="6939.869" dur="3.841">ended up influencing the whole Roman Empire.</text><text start="6943.71" dur="8.48">And so in some way it won or similar with
Judaic ideas or whatever.</text><text start="6952.19" dur="4.969">One of the greatest examples of that that
people talk about right now is Tibetan Buddhism,</text><text start="6957.159" dur="1.73">which is OK.</text><text start="6958.889" dur="9.231">So from the point of view of Tibet as a nation,
the Tibetan people, the integrity of that</text><text start="6968.12" dur="3.749">wisdom tradition, it was radically destroyed.</text><text start="6971.869" dur="4.421">But in the process of the world seeing that
and having some empathy engendered for it,</text><text start="6976.29" dur="6.21">even though it didn&apos;t protect Tibet, did that
actually disseminate Buddhist ideals to the</text><text start="6982.5" dur="3.4">whole world radically faster?</text><text start="6985.9" dur="5.989">There&apos;s a similar conversation as so many
people become interested in ayahuasca and</text><text start="6991.889" dur="7.04">plant medicines from indigenous cultures that
the economic pressure of that is making what</text><text start="6998.929" dur="4.881">is remnant of those cultures get turned into
tourism and ayahuasca production or shipibo</text><text start="7003.81" dur="1.46">production or whatever it is.</text><text start="7005.27" dur="3.6">That on one hand it actually looks like a
destructive act on those cultures.</text><text start="7008.87" dur="3.65">And the other way it&apos;s – are the memetics
of those now becoming dispersed throughout</text><text start="7012.52" dur="2.35">the dominant systems.</text><text start="7014.87" dur="7.69">That is a part of the consideration set that
has to be considered.</text><text start="7022.56" dur="11.36">And now do we see, for instance, that particularly
nonviolent groups like say the Jains as an</text><text start="7033.92" dur="13.09">example of maximum nonviolence, that those
memes do become decentralized and affect everybody?</text><text start="7047.01" dur="1.8">It&apos;s not a simple yes or no, right?</text><text start="7048.81" dur="2.5">There&apos;s a whole bunch of contextual application.</text><text start="7051.31" dur="4.99">So when we look at are there memes from Buddhism
that have influenced the world?</text><text start="7056.3" dur="1.0">Yes.</text><text start="7057.3" dur="3.939">But are they the ones that are compatible
with the motivation set of the world they</text><text start="7061.239" dur="1.0">influence?</text><text start="7062.239" dur="5.221">So you&apos;ve got this like Buddhist techniques
of mindfulness for capitalists in Silicon</text><text start="7067.46" dur="6.929">Valley to crush it at capitalism is a very
weird version of the subset of the Buddhist</text><text start="7074.389" dur="1.821">stack.</text><text start="7076.21" dur="5.81">And I remember when I first started seeing
kind of the popularization of mindfulness</text><text start="7082.02" dur="5.619">techniques in business so that people could
focus better and crush it at capitalism how</text><text start="7087.639" dur="2.071">fucking hilarious that thing is, right?</text><text start="7089.71" dur="3.12">Because in some ways you are distributing
good ideas.</text><text start="7092.83" dur="4.76">In other ways you&apos;re actually extracting from
a whole cultural set the part that ends up</text><text start="7097.59" dur="4.7">being a service ingredient to another set.</text><text start="7102.29" dur="5.079">So the topic of what makes it through, I mean
it&apos;s a complex topic.</text><text start="7107.369" dur="12.942">What I will say is the idea that a civilization,
which is its superstructure, its worldview</text><text start="7120.311" dur="4.969">values, what is true, good, beautiful, what
it&apos;s oriented to, what&apos;s a good life.</text><text start="7125.28" dur="5.99">Its social structure, meaning its political
economy and institutions, its formal incentives</text><text start="7131.27" dur="4.099">and deterrence and how it organizes collective
agreement and its infrastructure, the physical</text><text start="7135.369" dur="2.961">tech stack that it mediates all this on.</text><text start="7138.33" dur="4.98">Together in a civilizational stack.</text><text start="7143.31" dur="7.02">One of those competing with other ones for
scarce resource and dominance and all of them</text><text start="7150.33" dur="4.84">engaged in that particular competitive thing.</text><text start="7155.17" dur="2.81">No one actually gets to win that.</text><text start="7157.98" dur="6.19">That process of the competition of those relative
to each other does actually create an overall</text><text start="7164.17" dur="5.34">global civilizational topology that self-terminates.</text><text start="7169.51" dur="5.149">But also no one trying to create a good long-term
future can just lose at that game in the short</text><text start="7174.659" dur="1.0">term.</text><text start="7175.659" dur="5.241">So you can neither create the world you want
by just losing at that game nor by trying</text><text start="7180.9" dur="1.0">to win at it.</text><text start="7181.9" dur="1.0">It&apos;s something else.</text><text start="7182.9" dur="4.07">It&apos;s actually having to abandon that game
to try to change the game dynamics themselves.</text><text start="7186.97" dur="1.44">Ah, okay.</text><text start="7188.41" dur="1.92">So I was just watching Arrested Development.</text><text start="7190.33" dur="3.6">This, that you can&apos;t play the game, you can&apos;t
frame it in the same way.</text><text start="7193.93" dur="4.419">So Buster from Arrested Development, I don&apos;t
know if you&apos;ve seen it, he&apos;s this mama&apos;s boy.</text><text start="7198.349" dur="3.06">He wants to go out with this other girl who
happens to have the same name as his mom.</text><text start="7201.409" dur="3.261">But anyway, she&apos;s like, I don&apos;t want to go
out with you because you&apos;re just in love with</text><text start="7204.67" dur="1.0">your mom.</text><text start="7205.67" dur="1.04">Then he&apos;s like, no, and he just left his mom&apos;s
home.</text><text start="7206.71" dur="1.44">He&apos;s like 40 years old or 35.</text><text start="7208.15" dur="1.91">He&apos;s like, no, no, no, it&apos;s the opposite.</text><text start="7210.06" dur="2.28">I&apos;m leaving my mom for you.</text><text start="7212.34" dur="1.54">You&apos;re replacing my mom.</text><text start="7213.88" dur="2.28">And then she&apos;s like, no.</text><text start="7216.16" dur="2.93">And it&apos;s because he&apos;s still framing it in
the same way.</text><text start="7219.09" dur="2.01">Anyway, we have to abandon the frame.</text><text start="7221.1" dur="1.97">So please, what does that look like?</text><text start="7223.07" dur="6.029">And integrate AI into this answer.</text><text start="7229.099" dur="7.181">So we have not just tried to get the frame
on AI risk yet.</text><text start="7236.28" dur="7.61">And to incorporate that in the what is the
long term solution for civilization as a whole</text><text start="7243.89" dur="4.49">look like, let&apos;s actually just kind of do
the AI risk part first and then we can bring</text><text start="7248.38" dur="2.89">it back together.</text><text start="7251.27" dur="11.75">Let&apos;s try to frame how to think about AI risks,
AI opportunities and potentials, including</text><text start="7263.02" dur="6.54">how AI can help solve other risks, which has
to be factored.</text><text start="7269.56" dur="4.52">I will add as preface that I am not an AI
developer.</text><text start="7274.08" dur="3.27">I don&apos;t have background in that.</text><text start="7277.35" dur="3.809">I&apos;m not even an AI risk expert or specialist.</text><text start="7281.159" dur="8.901">I know you&apos;re in conversation might have Yudkowsky
and other people who really are Stuart Russell,</text><text start="7290.06" dur="1.0">Bostrom.</text><text start="7291.06" dur="2.9">Those guys would be great.</text><text start="7293.96" dur="8.88">Because of some maybe novel perspectives about
thinking about risk and governance approaches</text><text start="7302.84" dur="7.01">to risk writ large, metacrisis, that&apos;s the
perspective that I&apos;m taking into the AI topic.</text><text start="7309.85" dur="5.52">So what is unique about AI risk relative to
other risks?</text><text start="7315.37" dur="8.44">We were talking earlier about environmental
risks and risks associated with large scale</text><text start="7323.81" dur="4.74">war and breakdown of human systems and synthetic
bio and other things.</text><text start="7328.55" dur="3.96">If we look at other technologies that are
that have the potential to do some catastrophic</text><text start="7332.51" dur="8.81">things like nuclear, it&apos;s very easy to see
that nuclear weapons don&apos;t make better bio</text><text start="7341.32" dur="1.0">weapons.</text><text start="7342.32" dur="1.54">They don&apos;t make better cyber weapons.</text><text start="7343.86" dur="4.91">They don&apos;t even make better nuclear weapons
directly.</text><text start="7348.77" dur="2.46">And the same is true for biotechnology.</text><text start="7351.23" dur="2.15">It doesn&apos;t automatically make those other
things.</text><text start="7353.38" dur="7.22">AI is pretty unique in that you can use AI
to evolve the state of the art in nuclear</text><text start="7360.6" dur="7.46">technology, in delivery technology, and intelligence
technology, and bio and cyber and literally</text><text start="7368.06" dur="1.0">all of them.</text><text start="7369.06" dur="4.639">So it is unique in its omnipurpose potential
in that way.</text><text start="7373.699" dur="4.171">Because of course all those other technologies
were developed by human intelligence.</text><text start="7377.87" dur="7.63">Human intelligence, agency, creativity, some
unique faculties of human cognitive process.</text><text start="7385.5" dur="5.6">And so where all of the other technologies
are kind of the result of that human process,</text><text start="7391.1" dur="4.4">building a technology that is doing that human
process, possibly much faster and on much</text><text start="7395.5" dur="5.71">more scale, is obviously a unique kind of
case, right?</text><text start="7401.21" dur="7.29">And so there&apos;s thinking about what type of
risk does an AI system create on its own.</text><text start="7408.5" dur="5.469">But then there&apos;s thinking about how do AI
systems affect all other categories of risk,</text><text start="7413.969" dur="1.0">right?</text><text start="7414.969" dur="5.051">We have to think about both of those.</text><text start="7420.02" dur="4.429">And then in addition to the fact that the
nukes don&apos;t automatically make better bioweapons,</text><text start="7424.449" dur="2.801">the nukes don&apos;t even automatically make more
nukes, right?</text><text start="7427.25" dur="2.07">They&apos;re not pattern replicating.</text><text start="7429.32" dur="6.51">But to the degree that we actually get AI
systems that not only can make all the other</text><text start="7435.83" dur="3.56">things better, but they can make better AI
systems and to the degree that there starts</text><text start="7439.39" dur="6.92">to be something like autonomy in that process,
then the self-upgrading and omnipotential</text><text start="7446.31" dur="2.75">of all the other things.</text><text start="7449.06" dur="9.15">It&apos;s also true that there&apos;s an exponential
curve in the development of hardware that</text><text start="7458.21" dur="1.0">AI runs on, right?</text><text start="7459.21" dur="5.23">Better GPUs and all the different kinds of
computational capabilities.</text><text start="7464.44" dur="6.16">There&apos;s an exponential curve in IoT systems
for capturing more data to train them on,</text><text start="7470.6" dur="6.17">exponentially more people and money going
into the field.</text><text start="7476.77" dur="5.36">Because of the way that shared knowledge systems
work, the kind of exponential development</text><text start="7482.13" dur="4.2">in the software and cognitive architectures,
so we&apos;re looking at the intersection of multiple</text><text start="7486.33" dur="3.68">exponential curves, not just a single one.</text><text start="7490.01" dur="3.86">That is also kind of important and unique
to understand about the space.</text><text start="7493.87" dur="8.02">So thinking about the case of AI turning into
AGI, an autonomous artificial intelligence</text><text start="7501.89" dur="5.34">system that we can no longer pull the plug
on that has goals, has objective functions,</text><text start="7507.23" dur="6.36">whatever they happen to be, that is something
that guys like Bostrom and Yudkowsky have</text><text start="7513.59" dur="3.0">done a very good job of describing why that&apos;s
a very risky thing.</text><text start="7516.59" dur="4.75">I think everybody at this point probably has
a decent sense of it, but just make it very</text><text start="7521.34" dur="1.0">quick.</text><text start="7522.34" dur="4.87">When we say a narrow AI system, we mean something
that is trained to be good at a very specific</text><text start="7527.21" dur="5.73">domain, like beating people at chess or beating
them at Go or being able to summarize a large</text><text start="7532.94" dur="2.659">body of text.</text><text start="7535.599" dur="6.861">When we say general intelligence, we mean
something that could maybe do all of those</text><text start="7542.46" dur="5.83">things and can figure out how to be better
than humans at new domains it has not been</text><text start="7548.29" dur="5.01">trained on through some kind of abstraction
or lateral application of what it already</text><text start="7553.3" dur="1.47">knows.</text><text start="7554.77" dur="4.37">So if you put us into an environment where
we have to figure out what is even the adaptive</text><text start="7559.14" dur="2.59">thing to do, we will do it.</text><text start="7561.73" dur="3.06">There&apos;s a certain kind of general intelligence
that we have.</text><text start="7564.79" dur="6.519">So when we talk about a generally intelligent
artificial intelligence, the idea – and</text><text start="7571.309" dur="5.501">then, of course, because we can develop AI
systems, one of the things it could do is</text><text start="7576.81" dur="1.02">develop AI systems.</text><text start="7577.83" dur="4.94">So if it has more cognitive capability than
us in some ways, it can develop a better AI</text><text start="7582.77" dur="2.9">system, and then that one could recursively
develop a better one, and you get this kind</text><text start="7585.67" dur="4.68">of thought about recursive takeoff in the
power of an AI system.</text><text start="7590.35" dur="4.86">And there are conversations about whether
that would be slow or fast.</text><text start="7595.21" dur="4.0">Is there an upper boundary on how intelligent
a system could be, and humans are near the</text><text start="7599.21" dur="1.06">top of that?</text><text start="7600.27" dur="3.579">Or are we barely scratching at the beginning
of it, and we could have something millions</text><text start="7603.849" dur="2.931">of times smarter than us?</text><text start="7606.78" dur="1.939">So that&apos;s all kind of part of that conversation.</text><text start="7608.719" dur="6.551">But the idea that we could create an artificial
intelligence that is – that could basically</text><text start="7615.27" dur="2.32">beat us at all games, right?</text><text start="7617.59" dur="6.92">That could – which it could think about
economy and affecting public opinion and military</text><text start="7624.51" dur="1.589">as games.</text><text start="7626.099" dur="5.35">And it has faster feedback loops, faster OODA
loops to get better than we do.</text><text start="7631.449" dur="5.121">So if we&apos;re trying to deal with it, it&apos;s going
to win at newly defined games.</text><text start="7636.57" dur="4.899">And if that thing we can&apos;t pull the plug on,
and it can&apos;t anticipate our movements and</text><text start="7641.469" dur="6.291">beat us at all games, if it has goals that
are directly antithetical to ours, or not</text><text start="7647.76" dur="5.359">even directly antithetical but the way in
which it fulfills its goals might involve</text><text start="7653.119" dur="1.48">externalizing</text><text start="7654.599" dur="2.901">harm to things that are part of our goal set.</text><text start="7657.5" dur="1.62">That&apos;s bad for us, right?</text><text start="7659.12" dur="5.67">So the idea of don&apos;t let that thing happen
prevents getting to an unaligned AGI.</text><text start="7664.79" dur="3.25">That&apos;s that particular category of risk.</text><text start="7668.04" dur="6.58">And so there are arguments around could an
AGI like that – is it even possible?</text><text start="7674.62" dur="1.369">That&apos;s one question.</text><text start="7675.989" dur="6.301">If it is possible to have such a thing, is
it possible to align it with human interests?</text><text start="7682.29" dur="2.46">What would that take?</text><text start="7684.75" dur="3.829">Is it possible – if it is possible to align
it, is it possible to know ahead of time that</text><text start="7688.579" dur="2.691">the system you have will be aligned and will
stay aligned?</text><text start="7691.27" dur="1.0">Right?</text><text start="7692.27" dur="1.53">Like those are all some of the questions in
the space.</text><text start="7693.8" dur="7.37">And then do our current trajectories of AI
research like transformer tech or just neural</text><text start="7701.17" dur="5.949">networks or deep learning in general, do these
converge on general intelligence?</text><text start="7707.119" dur="2.241">And if so, in what time period?</text><text start="7709.36" dur="5.74">Those are all some of the questions regarding
the AGI risk space.</text><text start="7715.1" dur="8.209">Now I want to talk about that risk, but I
want to talk about other risks using that</text><text start="7723.309" dur="2.921">as an example in the space.</text><text start="7726.23" dur="2.349">Any questions or thoughts on that one to begin
with?</text><text start="7728.579" dur="1.0">Sure.</text><text start="7729.579" dur="3.841">Number one is that we may already have artificial
intelligence in a baby form, like we have</text><text start="7733.42" dur="1.0">hugging face.</text><text start="7734.42" dur="1.0">I don&apos;t know if you know what that is.</text><text start="7735.42" dur="2.86">And then there&apos;s the paper sparks of artificial
general intelligence.</text><text start="7738.28" dur="2.93">And that&apos;s distinguished from something that
updates itself.</text><text start="7741.21" dur="3.099">I just want to make that clear.</text><text start="7744.309" dur="5.401">So there&apos;s a lot of questions regarding does
it have to be better than humans at everything</text><text start="7749.71" dur="4.93">to be an existential risk?</text><text start="7754.64" dur="8.579">We could imagine a von Neumann machine that
was self-replicating and self-evolving that</text><text start="7763.219" dur="5.611">was not better at everything but better at
turning what was around it into more of itself</text><text start="7768.83" dur="2.349">and evolving its ability to do so.</text><text start="7771.179" dur="1.871">And just having way faster feedback loops.</text><text start="7773.05" dur="5.339">And we could imagine that becoming an existential
risk with a speed of a particular type of</text><text start="7778.389" dur="2.19">intelligence that does not mean better than
us at everything.</text><text start="7780.579" dur="1.181">Yeah, that&apos;s a great point.</text><text start="7781.76" dur="4.549">Like an asteroid is not better than us at
almost anything, but it can destroy us.</text><text start="7786.309" dur="5.27">And it, yeah, it&apos;s not doing it through some
kind of process that involves learning or</text><text start="7791.579" dur="2.301">navigating competitions at all.</text><text start="7793.88" dur="1.449">It&apos;s just kinetic impact.</text><text start="7795.329" dur="4.85">This would be a kind of intelligence, but
it could be one that&apos;s a lot more like a very</text><text start="7800.179" dur="1.5">bad pandemic, right?</text><text start="7801.679" dur="8.341">And the intelligence of a pathogen than the
intelligence of a god.</text><text start="7810.02" dur="6.67">So talking about if the system is generally
– like what type of intelligence it would</text><text start="7816.69" dur="1.0">need.</text><text start="7817.69" dur="1.41">Is it generally intelligent?</text><text start="7819.1" dur="1.289">Is it autonomous?</text><text start="7820.389" dur="1.111">Is it agentic?</text><text start="7821.5" dur="1.69">Is it self-upgrading?</text><text start="7823.19" dur="8.39">They&apos;re related concepts, but they&apos;re not
identical concepts.</text><text start="7831.58" dur="9.499">So let&apos;s go ahead and put the category of
AGI risk as one topic in the AI risk space.</text><text start="7841.079" dur="9.48">Let&apos;s come to a much nearer term set of things,
which is AI empowering bad actors.</text><text start="7850.559" dur="6.381">And we can talk about what of that is possible
with the existing technology.</text><text start="7856.94" dur="5.23">What of that is possible with impending technology
that we&apos;re for sure going to get on the current</text><text start="7862.17" dur="4.389">course versus things where we don&apos;t know how
long it&apos;s going to take or even if we&apos;ll get</text><text start="7866.559" dur="3.83">there.</text><text start="7870.389" dur="6.381">So with regard to AI empowering bad actors,
we could say – how one defines bad actor</text><text start="7876.77" dur="5.94">is obviously – because one person&apos;s freedom
fighter is another person&apos;s terrorist.</text><text start="7882.71" dur="8.529">But – so we can imagine someone who is terrified
about environmental collapse deciding to become</text><text start="7891.239" dur="3.771">an eco-terrorist being a maximally good actor
in their world view.</text><text start="7895.01" dur="4.189">But saying that the only answer is to start
taking out massive chunks of the civilizational</text><text start="7899.199" dur="3.73">system that&apos;s destroying the environment.</text><text start="7902.929" dur="6.781">So I&apos;m simply saying that I&apos;m not being simplistic
about what we mean by bad actor, but oriented</text><text start="7909.71" dur="4.639">to from whatever motivational type, whether
it was pure sadism, whether it&apos;s nihilistic</text><text start="7914.349" dur="4.611">burn it all down, or whether it&apos;s well-motivated
but maybe misguided considerations.</text><text start="7918.96" dur="6.25">But AI for some destructive purpose.</text><text start="7925.21" dur="8.369">So now – this is something we have to address
first.</text><text start="7933.579" dur="7.881">One thing I have found when people think about
how significant AI risk will be and how significant</text><text start="7941.46" dur="2.29">AI upside will be.</text><text start="7943.75" dur="3.619">First on AI upside, it&apos;s just important because
if we talk about risk and we don&apos;t talk about</text><text start="7947.369" dur="5.94">upside, it will be easy for a lot of people
to say, oh, this is a techno-pessimist Luddite</text><text start="7953.309" dur="3.301">perspective and kind of dismiss it at that.</text><text start="7956.61" dur="8.879">So I would like to say there is a – the
upsides of AI, the best case examples that</text><text start="7965.489" dur="2.541">everyone is interested in, everyone is interested
in.</text><text start="7968.03" dur="1.0">They&apos;re awesome, right?</text><text start="7969.03" dur="3.97">Can we – all the things that we care about
that we use intelligence to figure out where</text><text start="7973.0" dur="4.65">intelligence is rate limiting, figure them
out, rate limiting to figure out the rest</text><text start="7977.65" dur="2.98">of the problems, could we use it to solve
those problems?</text><text start="7980.63" dur="5.509">So could AI make major breakthroughs in cancer
and immuno-oncology?</text><text start="7986.139" dur="4.6">And does anyone who&apos;s talking about slowing
down AI, are they factoring all the kids that</text><text start="7990.739" dur="1.38">are dying of cancer right now?</text><text start="7992.119" dur="5.741">And if we could speed that thing up, could
we affect that in our – like, that&apos;s a very</text><text start="7997.86" dur="2.46">personal, very real thing, right?</text><text start="8000.32" dur="8.78">So AI applied to curing all kinds of diseases,
and AI applied to psychiatric diseases and</text><text start="8009.1" dur="5.46">scientific breakthroughs and maybe resource
optimization issues that help the environment</text><text start="8014.56" dur="4.839">and maybe the ability to help with coordination
challenges if applied in certain ways.</text><text start="8019.399" dur="12.2">The positive applications, the kind of customized
AI tutoring that could provide Marcus Aurelius-level</text><text start="8031.599" dur="3.29">education where the best tutors of all of
Rome were personally tutoring him in every</text><text start="8034.889" dur="3.651">topic could provide something better than
that to every human, right?</text><text start="8038.54" dur="3.699">Could democratize aristocratic tutoring.</text><text start="8042.239" dur="4.711">There was Eric Hole&apos;s essays on aristocratic
tutoring are really good, you should bring</text><text start="8046.95" dur="4.919">them on here, but basically it was something
many people have come to, which is that the</text><text start="8051.869" dur="5.85">great polymaths and super geniuses, the highest
statistical correlator that pops out is that</text><text start="8057.719" dur="3.98">they all had something like aristocratic tutoring
when they were young, or the vast majority</text><text start="8061.699" dur="1.0">of them.</text><text start="8062.699" dur="5.631">That even von Neumann and Einstein had mathematicians
as governesses before they went to school.</text><text start="8068.33" dur="5.07">And Terry Tao had Paul Erdos, there&apos;s this
famous image of, I don&apos;t know who had Edwin</text><text start="8073.4" dur="1.0">though.</text><text start="8074.4" dur="9.089">So that is a – many of the people simply
had parents that were very actively involved,</text><text start="8083.489" dur="5.261">scientists, philosophers, thinkers, you know.</text><text start="8088.75" dur="5.02">But if you think about why Marcus Aurelius
dedicated the whole first chapter of Meditations</text><text start="8093.77" dur="4.619">to his tutors, and if you think about how
the Dalai Lama was conditioned where you find</text><text start="8098.389" dur="5.591">this three-year-old boy and have the top Lamas
in all of Tibet to do everything, that is</text><text start="8103.98" dur="1.98">the whole canon of knowledge.</text><text start="8105.96" dur="3.22">Of course, if that was applied to everybody,
we&apos;d have a very different world, right?</text><text start="8109.18" dur="3.98">And I think this is a very interesting insight
because it says that the upper boundaries</text><text start="8113.16" dur="5.36">on a lot of what we call human nature because
it&apos;s ubiquitous, it&apos;s not nature, it&apos;s nature</text><text start="8118.52" dur="4.79">through ubiquitous conditioning, that the
edge cases on human behavior show conditioning</text><text start="8123.31" dur="1.88">in common.</text><text start="8125.19" dur="5.08">And that if you could make that kind of conditioning
ubiquitous, you would actually change the</text><text start="8130.27" dur="2.34">human condition pretty profoundly.</text><text start="8132.61" dur="4.47">But as we move from feudalism to democracy
and wanted to kind of get rid of all the dreadful,</text><text start="8137.08" dur="6.17">unequal aspects of feudalism, looking at the
fact that like you can&apos;t learn to be a world-class</text><text start="8143.25" dur="4.14">mathematician by a person who&apos;s not a world-class
mathematician the same way you can by one</text><text start="8147.39" dur="1.0">who is.</text><text start="8148.39" dur="4.32">And you don&apos;t get a bunch of world-class mathematicians
becoming third-grade or eighth-grade high</text><text start="8152.71" dur="2.79">school teachers or school teachers.</text><text start="8155.5" dur="1.0">So how would you do that?</text><text start="8156.5" dur="5.86">So it&apos;s kind of repugnant from a privileged
lack of democratized capability point of view,</text><text start="8162.36" dur="1.0">right?</text><text start="8163.36" dur="8.879">And yet could you have – could I make LLM-trained
AIs and better than LLM ones where I can have</text><text start="8172.239" dur="5.221">von Neumann and Einstein and Gödel all in
a conversation with me about formal logic?</text><text start="8177.46" dur="4.56">Where they are not only representing their
ideas but maybe even now have access to all</text><text start="8182.02" dur="7.27">the ideas since then and are pedagogically
regulating themselves to my learning style.</text><text start="8189.29" dur="1.57">That&apos;s kind of amazing, right?</text><text start="8190.86" dur="8.38">Like – and could they maybe be doing that
based on also psychological development theories?</text><text start="8199.24" dur="4.55">A colleague of mine, Zach Stein, has been
working on this a lot of how to be evolving</text><text start="8203.79" dur="5.72">not just their cognitive capacity but their
psychosocial, moral, aesthetic, ethical, et</text><text start="8209.51" dur="3.45">cetera, full suite of human capacities.</text><text start="8212.96" dur="8.24">So I&apos;m simply saying AI applied rightly, there&apos;s
a lot of things to be excited and optimistic</text><text start="8221.2" dur="1.75">about.</text><text start="8222.95" dur="1.0">So that&apos;s a given.</text><text start="8223.95" dur="6.689">And we could do a whole long conversation
on more of those examples.</text><text start="8230.639" dur="9.731">There is a –
when I look at how people orient to the topic</text><text start="8240.37" dur="6.979">of AI risk, one of the things that seems to
be a common kind of where their knee-jerk</text><text start="8247.349" dur="4.76">reactions before understanding all the arguments
pro and con well comes</text><text start="8252.109" dur="6.391">is how much they have a bias towards a kind
of techno-optimism or techno-pessimism, kind</text><text start="8258.5" dur="6.33">of a – where Pinker, Hans Rosling, there
are still problems but they&apos;re getting better.</text><text start="8264.83" dur="4.43">The world is getting progressively better,
and it&apos;s a result of things like capitalism</text><text start="8269.26" dur="2.87">and technology and science and progress.</text><text start="8272.13" dur="2.83">And so more of that will just keep equaling
better.</text><text start="8274.96" dur="4.37">And yes, there will be problems, but they&apos;re
worth it, right?</text><text start="8279.33" dur="8.38">Versus – so that&apos;s – I would call that
techno-capital optimism.</text><text start="8287.71" dur="4.45">But the naive version that doesn&apos;t look at
the cost of that thing, we would call a naive</text><text start="8292.16" dur="4.92">progress dialectic.</text><text start="8297.08" dur="7.13">In the dialectic of progress is good, progress
is not good, or we&apos;re really making progress</text><text start="8304.21" dur="2.769">versus we&apos;re actually losing critical things
or causing harm or whatever.</text><text start="8306.979" dur="1.49">That&apos;s a dialectic.</text><text start="8308.469" dur="6.061">That&apos;s on the progress side but a naive version
of it.</text><text start="8314.53" dur="12.02">And so most of the – and just to address
that briefly, so the naive progress story</text><text start="8326.55" dur="2.379">looks at all the things that have gotten better.</text><text start="8328.929" dur="5.071">And you can see this in lots of good books
and Pinker&apos;s books, Diamandis&apos; books, Rosling&apos;s</text><text start="8334.0" dur="1.349">talks, on and on.</text><text start="8335.349" dur="4.941">And then the extension of that into the future,
Diamandis starts to do and we could say Kurzweil</text><text start="8340.29" dur="5.12">is kind of an extension of that far out.</text><text start="8345.41" dur="4.229">Why naive is if it doesn&apos;t look at what is
lost in that process and what is harmed in</text><text start="8349.639" dur="6.441">that process as well as the increase in the
types of risk that are happening.</text><text start="8356.08" dur="5.28">And so I would argue that most of those things
in every kind of Rosling presentation is cherry-picking</text><text start="8361.36" dur="3.76">its data out of a humongous set to make a
cherry-picked argument.</text><text start="8365.12" dur="3.319">This is one of the reasons that fact-checking
is not enough is because you can cherry-pick</text><text start="8368.439" dur="1.0">your facts.</text><text start="8369.439" dur="4.53">You can frame them in a particular way and
create a conclusion that the totality of knowledge</text><text start="8373.969" dur="4.96">wouldn&apos;t support it all because of that process,
right?</text><text start="8378.929" dur="5.971">I would say there is a naive techno-pessimism
or Luddite direction that looks at the real</text><text start="8384.9" dur="9.1">harms tech causes culturally, socially, environmentally
or other things and wants back to the land</text><text start="8394.0" dur="7.03">of movement and organic, natural, traditional,
whatever, various types of – and if it is</text><text start="8401.03" dur="5.159">not paying attention to the types of benefit
that are legitimate, that&apos;s naive.</text><text start="8406.189" dur="3.782">But also if it&apos;s not paying attention to the
fact that that worldview will simply, as we</text><text start="8409.971" dur="4.759">talked about before, not forward itself because
the one that advances more tech will develop</text><text start="8414.73" dur="5.86">more power and end up becoming the dominant
world system, that also means that it&apos;s not</text><text start="8420.59" dur="5.17">actually having a worldview that can orient
towards shaping the world.</text><text start="8425.76" dur="9.03">So we have to – so putting those together,
I would say all of the things that the techno-optimists</text><text start="8434.79" dur="3.78">say tech has made better and all of us like
a world where going to the dentist involves</text><text start="8438.57" dur="4.52">Novocain versus not Novocain and where we
have painkillers and where we have antibiotics</text><text start="8443.09" dur="2.8">under infection and stuff like that.</text><text start="8445.89" dur="5.03">All of the things that tech has made better
have not come for free.</text><text start="8450.92" dur="6.38">There have been externalized costs, and the
cumulative effect of all of those costs is</text><text start="8457.3" dur="2.2">really, really significant.</text><text start="8459.5" dur="7.0">And so if you look at the progress narrative,
the indigenous people that were genocided</text><text start="8466.5" dur="2.16">don&apos;t see it as a progress narrative.</text><text start="8468.66" dur="4.59">The fact that there are more – there&apos;s more
biomass of animals in factory farms than there</text><text start="8473.25" dur="4.95">is in the wild today does not see that as
a sign of progress.</text><text start="8478.2" dur="4.54">The animals that live in factory farms or
all the species that are extinct don&apos;t see</text><text start="8482.74" dur="1.571">it as progress.</text><text start="8484.311" dur="6.338">The fact that we have many, many different
possibilities of destroying the life support</text><text start="8490.649" dur="7.101">capacity of the planet relative to any previous
time or that almost no teen girl growing up</text><text start="8497.75" dur="5.14">in the industrialized world doesn&apos;t have body
dysmorphia where that was not an ubiquitous</text><text start="8502.89" dur="1.0">thing.</text><text start="8503.89" dur="3.7">There&apos;s a lot of things where you can say,
damn, those technologies upregulated some</text><text start="8507.59" dur="2.96">things and externalized costs somewhere else.</text><text start="8510.55" dur="7.11">If you factor the totality of that, then you
can say, okay, there are a lot of positive</text><text start="8517.66" dur="4.94">examples any new type of tech can have, but
there&apos;s also a lot of externalities and harms</text><text start="8522.6" dur="1.15">it can have.</text><text start="8523.75" dur="5.601">And we want to see how to get more of the
upsides with less of the downsides, and that</text><text start="8529.351" dur="2.729">can&apos;t be a rush-forward process, right?</text><text start="8532.08" dur="3.29">That actually requires a lot of thinking about
how to do that.</text><text start="8535.37" dur="9.311">So I&apos;m actually – I actually am a techno-optimist
in a way, meaning I do see a future that is</text><text start="8544.681" dur="11.429">high nature stewardship, high touch, and there&apos;s
also high tech.</text><text start="8556.11" dur="1.0">High touch?</text><text start="8557.11" dur="9.999">High touch, yeah, meaning
that the tech does not move us into being</text><text start="8567.109" dur="4.54">disembodied heads mediating exclusively through
a digital world.</text><text start="8571.649" dur="6.821">So I would argue that your online relationships
don&apos;t do everything that offline relationships</text><text start="8578.47" dur="1.0">do.</text><text start="8579.47" dur="4.54">They do some additional things like distance
and network dynamics, whatever.</text><text start="8584.01" dur="4.54">But if you&apos;re not doing that with your relationships,
they&apos;re causing harm.</text><text start="8588.55" dur="7.78">If the online relationships improve your embodied
relationships, not just the create online</text><text start="8596.33" dur="3.65">relationships and debase them, then that&apos;s
a different thing, right?</text><text start="8599.98" dur="4.03">So that&apos;s what I mean by high touch.</text><text start="8604.01" dur="12.91">So I want to say that naive techno-optimism
is – if we look at the history of corporations</text><text start="8616.92" dur="6.76">that are developing technology, market technology
advancement focused on the upside and not</text><text start="8623.68" dur="2.429">terribly focused on the downside.</text><text start="8626.109" dur="3.96">We look at four out of five doctors choose
Camel cigarettes.</text><text start="8630.069" dur="7.321">We look at better living through chemistry
providing DDT and parathion and malathion.</text><text start="8637.39" dur="6.139">We look at adding lead to gasoline in a way
that took a toxic chemical that was bound</text><text start="8643.529" dur="4.361">in ore underneath the biosphere and sprayed
it into the atmosphere ubiquitously and dropped</text><text start="8647.89" dur="3.799">about a billion IQ points off the planet and
made everybody more violent in terms of its</text><text start="8651.689" dur="4.821">neurotoxicology effects.</text><text start="8656.51" dur="4.93">Trusting the groups that are making the upside
on moving the thing to figure out the risks</text><text start="8661.44" dur="1.54">historically is not a very good idea.</text><text start="8662.98" dur="7.01">And I&apos;m mentioning that in terms of now trusting
the AI groups to do their own risk assessment.</text><text start="8669.99" dur="7.329">And if you think about the totality of risks
well, then you want to say, how do we move</text><text start="8677.319" dur="4.501">the positive applications of this technology
forward in a way that mitigates the really</text><text start="8681.82" dur="5.459">negative applications of it that if one wants
to be a techno-optimist responsibly, they</text><text start="8687.279" dur="2.261">have to be thinking about that well.</text><text start="8689.54" dur="3.63">So what about the AI companies that say we
do third party testing for safety?</text><text start="8693.17" dur="5.51">Are you still consider that somehow internal
because they&apos;re the ones going out?</text><text start="8698.68" dur="1.92">Depends.</text><text start="8700.6" dur="7.92">So when I early in the process of getting
into risk assessment, I had times where corporations</text><text start="8708.52" dur="3.69">asked me to come do risk assessment on a technology
or process.</text><text start="8712.21" dur="3.6">And then when I did an honest risk assessment,
they were not happy because what they wanted</text><text start="8715.81" dur="4.089">me to do was some kind of box checking exercise
that wouldn&apos;t cost them very much and wouldn&apos;t</text><text start="8719.899" dur="1.271">limit what they were going to do.</text><text start="8721.17" dur="4.05">So they had plausible deniability to say they
had done the thing and move forward quickly.</text><text start="8725.22" dur="3.11">Because what they didn&apos;t want was for me to
say, actually, there is no way for you to</text><text start="8728.33" dur="5.84">pursue the market viability of this that does
not cause excessive harms or where you&apos;re</text><text start="8734.17" dur="4.0">dealing with those harms messes up your possibility
for margins.</text><text start="8738.17" dur="3.92">Without breaking any NDA of yours that you
may have signed, are you able to go into what</text><text start="8742.09" dur="2.15">a company did that you disapproved of?</text><text start="8744.24" dur="1.39">And what was the result of it?</text><text start="8745.63" dur="3.87">Just as an example to make this more concrete
for people who are listening.</text><text start="8749.5" dur="1.54">Yeah, totally.</text><text start="8751.04" dur="11.03">I have seen this in the example of something
like mining technology or a new type of packaging</text><text start="8762.07" dur="4.92">technology that is wanting to say why it&apos;s
doing something that addresses some of the</text><text start="8766.99" dur="1.0">environmental concerns.</text><text start="8767.99" dur="3.68">It addresses the environmental concerns it
identified.</text><text start="8771.67" dur="3.84">We identify a bunch of other ones that it
doesn&apos;t address well that it moves some of</text><text start="8775.51" dur="1.65">the harm from this area to the other one.</text><text start="8777.16" dur="2.97">That&apos;s an example of where some of the problem
would come.</text><text start="8780.13" dur="4.49">But I find this is just as bad in the non-profit
space or in the government space as well,</text><text start="8784.62" dur="1.62">not just in the for-profit space.</text><text start="8786.24" dur="5.73">Because they also have – even if it&apos;s not
a profit motive, they have an institutional</text><text start="8791.97" dur="1.0">mandate.</text><text start="8792.97" dur="1.0">And their institutional mandate is narrow.</text><text start="8793.97" dur="1.0">They can advance that narrow thing.</text><text start="8794.97" dur="2.33">This is now the same thing as an AI objective,
right?</text><text start="8797.3" dur="6.07">If the AI has an objective function to optimize
X, whatever X is, or optimize a weighted function</text><text start="8803.37" dur="7.22">of X, Y, Z and metrics, everything that falls
outside of that set, harm can be externalized</text><text start="8810.59" dur="2.42">to that and achieve its objective function.</text><text start="8813.01" dur="4.62">So I remember talking to groups, UN-associated
groups.</text><text start="8817.63" dur="6.45">They were working on world hunger, and their
particular solutions involved bringing conventional</text><text start="8824.08" dur="5.949">agriculture to areas in the world that didn&apos;t
have it, which meant all of the pesticides,</text><text start="8830.029" dur="1.051">herbicides, and nitrogen fertilizers.</text><text start="8831.08" dur="7.04">And it was a huge increase in nitrogen fertilizer
by a bunch of river deltas where it currently</text><text start="8838.12" dur="1.0">wasn&apos;t.</text><text start="8839.12" dur="4.609">It would increase dead zones in oceans from
nitrogen effluent.</text><text start="8843.729" dur="5.031">That would affect the fisheries in those areas
and everything else and the total biodiversity.</text><text start="8848.76" dur="3.04">And when I brought it up to them, they&apos;re
like, oh, I guess that&apos;s true.</text><text start="8851.8" dur="2.5">But those are not the metrics we&apos;re tasked
with.</text><text start="8854.3" dur="4.34">We&apos;re tasked with how many people get fed
this year, not how much the environment is</text><text start="8858.64" dur="2.18">ruined in the process.</text><text start="8860.82" dur="5.38">And so the reduction of the totality of an
interconnected world to a finite set of metrics</text><text start="8866.2" dur="6.35">we&apos;re going to optimize for, whether it&apos;s
one metric called net profit or GDP, or it&apos;s</text><text start="8872.55" dur="7.051">the metric of whatever the institution is
tasked with or getting elected or something</text><text start="8879.601" dur="6.298">like that, it is entirely possible to advance
that metric at the cost of other ones.</text><text start="8885.899" dur="7.031">And then it&apos;s entirely possible that other
groups who see that create counter responses</text><text start="8892.93" dur="3.009">to that who do the same thing in opposite
directions.</text><text start="8895.939" dur="5.471">And the totality of human behavior optimizing
narrow metrics while both driving arms races</text><text start="8901.41" dur="4.74">and externalizing metrics in wide areas is
at the heart of the coordination failures</text><text start="8906.15" dur="1.539">we face.</text><text start="8907.689" dur="6.511">And so it happens to be that this is already
something that we see with humans outside</text><text start="8914.2" dur="7.3">of AI, but giving an AI an objective function
is the same type of issue.</text><text start="8921.5" dur="3.56">So I was mentioning examples in the nonprofit
space.</text><text start="8925.06" dur="7.44">I think there are examples of how to do AI
safety that can also be dangerous.</text><text start="8932.5" dur="5.939">And so it&apos;s important.</text><text start="8938.439" dur="3.351">Sure.</text><text start="8941.79" dur="6.311">So somebody proposes an idea like here&apos;s a
type of AI that could be good and we should</text><text start="8948.101" dur="7.249">build it, or on the other side, here&apos;s an
AI safety protocol that would be good and</text><text start="8955.35" dur="3.76">we should instantiate it in regulation or
whatever.</text><text start="8959.11" dur="6.3">We want to red team those ideas, meaning see
how they break or fail, and violet team them,</text><text start="8965.41" dur="4.15">meaning see how they externalize harm somewhere
else that they didn&apos;t intend before implementing</text><text start="8969.56" dur="5.4">them, which just means think through the causal
set beyond the obvious set you&apos;re intending</text><text start="8974.96" dur="4.45">it for.</text><text start="8979.41" dur="8.73">So there was this call for a six-month pause
on training large language models bigger than</text><text start="8988.14" dur="1.549">GPT-4.</text><text start="8989.689" dur="2.84">I&apos;m not saying that a pause is a bad idea.</text><text start="8992.529" dur="7.83">I&apos;m saying as instantiated, it&apos;s not implementable,
and it&apos;s not obviously good.</text><text start="9000.359" dur="4.781">So you saw the pushback as people were like,
all right, so that means that whatever actors</text><text start="9005.14" dur="9.71">are not included in this, which might mean
bad actors, rush ahead, relative.</text><text start="9014.85" dur="6.23">That&apos;s a real consideration, and one has to
say, okay, so are we stopping the accumulation</text><text start="9021.08" dur="2.0">of larger GPU clusters during that time?</text><text start="9023.08" dur="5.441">Are we stopping the development of larger
access to larger data sets during that time</text><text start="9028.521" dur="1.729">that we&apos;ll be able to quickly configure them?</text><text start="9030.25" dur="6.25">Are we also stopping – there are plenty
of other types of AI that are not LLMs being</text><text start="9036.5" dur="2.33">deployed to the public but that are very powerful.</text><text start="9038.83" dur="6.16">BlackRock&apos;s Aladdin played some role in the
fact that it has more assets under management</text><text start="9044.99" dur="2.21">than the GDP of the United States.</text><text start="9047.2" dur="6.15">And there are military application AIs in
development.</text><text start="9053.35" dur="7.74">And so can you – so what is the actual risk
space, and are we talking about slowing the</text><text start="9061.09" dur="1.0">whole thing?</text><text start="9062.09" dur="3.07">Or are we talking about slowing some parts
relative to other parts where these kind of</text><text start="9065.16" dur="2.22">game-theoretic questions emerge?</text><text start="9067.38" dur="2.99">How would we ensure that the whole space was
slowing?</text><text start="9070.37" dur="1.43">How would we enforce that?</text><text start="9071.8" dur="2.91">Those are all things that have to be considered.</text><text start="9074.71" dur="2.771">You mentioned that GDP is not a great indicator,
and –</text><text start="9077.481" dur="4.559">GDP goes up with war and more military manufacturing.</text><text start="9082.04" dur="3.24">It goes up with increased consumerism and
the cost to the environment.</text><text start="9085.28" dur="1.18">It goes up with addiction.</text><text start="9086.46" dur="2.94">Addiction is great for lifetime value of a
customer.</text><text start="9089.4" dur="1.3">So –</text><text start="9090.7" dur="3.02">There&apos;s something called Goodhart&apos;s Law, so
I&apos;m sure you&apos;re familiar with Goodhart.</text><text start="9093.72" dur="1.99">Is this at the core of what you&apos;re saying?</text><text start="9095.71" dur="1.0">It&apos;s like, hey –</text><text start="9096.71" dur="1.0">It&apos;s one of them.</text><text start="9097.71" dur="1.0">Okay.</text><text start="9098.71" dur="1.83">Go ahead and explain it.</text><text start="9100.54" dur="3.84">As soon as you have a metric that you try
to optimize for, it ceases to become a good</text><text start="9104.38" dur="1.0">metric.</text><text start="9105.38" dur="2.7">For instance, I think this is from The Simpsons,
but it may be real, that there was a town</text><text start="9108.08" dur="1.83">overrun by snakes or rats.</text><text start="9109.91" dur="3.819">I think it was rats, and then you say, hey,
give me rat tails because it implies that</text><text start="9113.729" dur="1.0">you killed a rat.</text><text start="9114.729" dur="2.521">I&apos;ll give you a dollar every time you bring
me a rat tail, and we&apos;ll reduce the amount</text><text start="9117.25" dur="1.0">of rats.</text><text start="9118.25" dur="3.49">Maybe it initially did so, but then people
realized, I can farm rats and then just kill</text><text start="9121.74" dur="1.0">them and give you tails.</text><text start="9122.74" dur="1.8">And thus, I have more total rats.</text><text start="9124.54" dur="1.84">This is a much more general phenomenon.</text><text start="9126.38" dur="2.16">So perhaps if you think Twitter followers
are great –</text><text start="9128.54" dur="1.0">Okay, yeah.</text><text start="9129.54" dur="4.68">If you incentivize any metric, there are perverse
forms of fulfilling that metric.</text><text start="9134.22" dur="3.82">Meaning there&apos;s a way to fulfill that metric
that either no longer provides the good, or</text><text start="9138.04" dur="2.771">it provides the good while also affecting
some other bats.</text><text start="9140.811" dur="1.0">Right?</text><text start="9141.811" dur="7.319">Which basically means you probably thought
of that metric in a specific context.</text><text start="9149.13" dur="4.25">Like there&apos;s a bunch of wild rats, and the
only way to get a rat tail is to kill a wild</text><text start="9153.38" dur="3.63">rat, not in the context of farmed rats.</text><text start="9157.01" dur="3.37">And so it kind of relates to the topic we
were mentioning earlier about government,</text><text start="9160.38" dur="5.45">that it&apos;s not just instantiating a government
that makes sense on the current landscape,</text><text start="9165.83" dur="2.3">but recognizing the landscape is going to
keep changing.</text><text start="9168.13" dur="8.221">And it&apos;ll change in a way that has an incentive
to figure out how to control the regulatory</text><text start="9176.351" dur="3.859">systems and how to game the metric systems.</text><text start="9180.21" dur="3.62">So with regard to the topic of AI alignment,
right?</text><text start="9183.83" dur="6.739">Because if we tell that AI maximized the number
of rat tails, then it could, like Bostrom&apos;s</text><text start="9190.569" dur="1.891">paperclip maximizer.</text><text start="9192.46" dur="4.53">Before we continue, it&apos;s imperative that we
have a brief overview of Bostrom&apos;s thought</text><text start="9196.99" dur="3.579">experiment called the paperclip maximizer.</text><text start="9200.569" dur="5.241">The paperclip maximizer scenario, initially
conceived by philosopher Nick Bostrom in 2003,</text><text start="9205.81" dur="4.04">illustrates the potential dangers associated
with misaligned goals of artificial general</text><text start="9209.85" dur="4.61">intelligence, that is, AGI agents.</text><text start="9214.46" dur="5.39">In this hypothetical scenario, an AGI is tasked
with the seemingly innocuous goal of maximizing</text><text start="9219.85" dur="2.05">the number of paperclips it produces.</text><text start="9221.9" dur="4.68">However, rather than competence and focus
serving as a salutary quality, it&apos;s in fact</text><text start="9226.58" dur="5.46">due to its extreme competence and single-minded
focus that it proceeds to transform the entire</text><text start="9232.04" dur="5.92">planet and eventually the universe into paperclips,
annihilating humanity and all of life in the</text><text start="9237.96" dur="1.0">process.</text><text start="9238.96" dur="4.3">The core ideas to understand from this scenario
are the importance of value alignment, the</text><text start="9243.26" dur="4.769">orthogonality thesis, and instrumental convergence.</text><text start="9248.029" dur="4.361">Value alignment is the process of ensuring
that an AGI shares our values and goals in</text><text start="9252.39" dur="3.73">order to prevent cataclysmic outcomes, such
as the aforementioned paperclip maximizer.</text><text start="9256.12" dur="5.481">The orthogonality thesis states that intelligence
and goals can be independent, implying that</text><text start="9261.601" dur="3.029">a highly intelligent AGI can have arbitrary
goals.</text><text start="9264.63" dur="3.09">You hear this, by the way, when people say
that we&apos;ve become more knowledgeable with</text><text start="9267.72" dur="2.67">time, yet our ancestors were wiser.</text><text start="9270.39" dur="4.97">Instrumental convergence refers to the phenomenon
where diverse goals lead to similar instrumental</text><text start="9275.36" dur="2.81">behaviors, like resource acquisition and self-preservation.</text><text start="9278.17" dur="4.59">For instance, as Marvin Minsky points out,
both goals of prove the Riemann hypothesis</text><text start="9282.76" dur="5.92">and make paperclips may result in all of the
Earth&apos;s resources being dismantled, disintegrated,</text><text start="9288.68" dur="5.72">in an effort to accomplish these goals.</text><text start="9294.4" dur="7.69">Thus, despite the ultimate goal being different,
for instance, the Riemann hypothesis and make</text><text start="9302.09" dur="3.59">paperclips are not the same, there&apos;s a convergence
along the way.</text><text start="9305.68" dur="4.69">What&apos;s often overlooked in AGI development
is something called the value loading problem.</text><text start="9310.37" dur="4.78">This refers to the difficulty of encoding
our moral and ethical principles into a machine.</text><text start="9315.15" dur="2.411">That is, how do you load the values?</text><text start="9317.561" dur="4.309">Keep in mind that AGI needs to be corrigible
and robust to distributional shifts.</text><text start="9321.87" dur="5.02">AGI, or even baby AGI, needs to maintain its
alignment even when encountering situations</text><text start="9326.89" dur="2.28">deviating from its training data.</text><text start="9329.17" dur="4.51">Additionally, something that we want is that
the AGI should be able to recognize ambiguity</text><text start="9333.68" dur="5.75">in its objectives and seek clarification,
rather than optimizing based on flawed interpretations.</text><text start="9339.43" dur="3.99">Of course, we as people have ambiguity and
flawed interpretations.</text><text start="9343.42" dur="5.721">The difference is that AGI could decidedly
exacerbate our own existing known and unknown</text><text start="9349.141" dur="1.218">flawed nature.</text><text start="9350.359" dur="4.5">Another difference is that we can&apos;t replicate
on a second to second or millisecond to second</text><text start="9354.859" dur="1.0">basis.</text><text start="9355.859" dur="2.641">At least not yet.</text><text start="9358.5" dur="4.82">One promising approach to this AGI alignment
scenario or misalignment scenario is something</text><text start="9363.32" dur="1.97">called reward modeling.</text><text start="9365.29" dur="4.721">This involves estimating a reward function
based upon observing our preferences, rather</text><text start="9370.011" dur="2.419">than us providing predefined objectives.</text><text start="9372.43" dur="4.75">And some of the more hilarious examples I
found of the predefined sort are as follows.</text><text start="9377.18" dur="5.36">The aircraft landing problem was explicated
in 1998 when Feldhut attempted to evolve an</text><text start="9382.54" dur="3.21">algorithm for landing aircraft using genetic
programming.</text><text start="9385.75" dur="4.5">The evolved algorithm exploited some overflow
errors in the physics simulator, creating</text><text start="9390.25" dur="4.939">extreme forces that were estimated to be zero
because of the error, resulting in a perfect</text><text start="9395.189" dur="4.021">score without actually solving the problem
that it was intended to solve.</text><text start="9399.21" dur="1.66">Another example is the case of the Roomba.</text><text start="9400.87" dur="4.83">In a tweet, Custard Smigley described connecting
a neural network to this Roomba to navigate</text><text start="9405.7" dur="1.74">without bumping into objects.</text><text start="9407.44" dur="3.91">The reward scheme encouraged speed and discouraged
hitting the bumper sensors.</text><text start="9411.35" dur="1.41">Okay, so think about it.</text><text start="9412.76" dur="1.0">What could happen?</text><text start="9413.76" dur="2.57">Well, the Roomba learned to drive backward.</text><text start="9416.33" dur="4.82">There are no sensors in the back, so it just
went about bumping frequently and merrily.</text><text start="9421.15" dur="3.789">In a more recent example, a reinforcement
learning agent was trained to play the video</text><text start="9424.939" dur="2.141">game Road Runner.</text><text start="9427.08" dur="5.319">It was penalized for losing in level two,
so did it just become fantastic at the game?</text><text start="9432.399" dur="1.0">Not quite.</text><text start="9433.399" dur="5.221">The agent discovered that it could kill itself
at the end of level one to avoid losing in</text><text start="9438.62" dur="5.46">level two, thus exploiting the reward system
without actually improving its performance.</text><text start="9444.08" dur="3.989">What would happen if it was tasked to keep
us from hitting some tipping point?</text><text start="9448.069" dur="3.461">By the way, is living more valuable than not
living?</text><text start="9451.53" dur="1.789">What&apos;s the rational answer to this?</text><text start="9453.319" dur="6.601">This is perhaps the most important fundamental
question.</text><text start="9459.92" dur="4.92">With regard to the topic of AI alignment,
right, because if we tell the AI, maximize</text><text start="9464.84" dur="10.78">the number of rat tails, then it could, like
Bostrom&apos;s paperclip maximizer, start clear-cutting</text><text start="9475.62" dur="5.01">forests to grow massive factory farms of rats
and whatever.</text><text start="9480.63" dur="6.08">You can do the reducto ad absurdum of a very
powerful system.</text><text start="9486.71" dur="7.34">And so then the question is you say, okay,
well, do the rat tails or the GDP or whatever</text><text start="9494.05" dur="2.29">it is while also factoring this other metric.</text><text start="9496.34" dur="2.91">Okay, well, you can do those two metrics and
there&apos;s still something armed.</text><text start="9499.25" dur="1.0">What about these three?</text><text start="9500.25" dur="5.7">The question is, is there a finite, describable
set of things that is adequate for something</text><text start="9505.95" dur="4.93">that can do optimization that powerfully?</text><text start="9510.88" dur="5.93">That is a way of thinking, and so it&apos;s, is
there a finitely describable definition of</text><text start="9516.81" dur="7.3">good is another way of thinking about it,
right, or in terms of optimization theory.</text><text start="9524.11" dur="2.77">Yeah, that&apos;s something I think about, the
misalignment problem.</text><text start="9526.88" dur="1.8">Is it in principle impossible</text><text start="9528.68" dur="2.46">to make the explicit what&apos;s implicit?</text><text start="9531.14" dur="2.7">When we state a goal, it carries with it manifold</text><text start="9533.84" dur="1.41">unstated assumptions.</text><text start="9535.25" dur="2.74">For instance, I say, bring me coffee or bring
me Uber food.</text><text start="9537.99" dur="1.0">We</text><text start="9538.99" dur="4.25">imply indirectly, don&apos;t run over a pedestrian
to bring me the Uber food.</text><text start="9543.24" dur="1.0">Don&apos;t take it from</text><text start="9544.24" dur="1.47">the kitchen prior to it being packaged.</text><text start="9545.71" dur="1.859">Don&apos;t break through my door to give it to
me.</text><text start="9547.569" dur="1.0">And</text><text start="9548.569" dur="3.261">we cloak all of that and say, that&apos;s just
common sense.</text><text start="9551.83" dur="1.49">Common sense is extremely difficult</text><text start="9553.32" dur="1.2">to make explicit.</text><text start="9554.52" dur="2.419">Even object recognition is extremely difficult.</text><text start="9556.939" dur="1.0">And then as soon as</text><text start="9557.939" dur="2.981">we can get a robot to do something that is
human-like, then it becomes more and more</text><text start="9560.92" dur="1.309">black box-like.</text><text start="9562.229" dur="2.8">And then you have this huge problem of interpretability
of AI.</text><text start="9565.029" dur="1.0">So it is</text><text start="9566.029" dur="3.581">an extremely difficult problem, and I wonder
how much of the misalignment problem is just</text><text start="9569.61" dur="1.0">that.</text><text start="9570.61" dur="3.38">Is it just the fact that we can&apos;t make explicit
what&apos;s implicit, and we overvalue</text><text start="9573.99" dur="4.36">how much the explicit matters, and implicit
is far more complex.</text><text start="9578.35" dur="1.0">I don&apos;t know.</text><text start="9579.35" dur="1.0">This is</text><text start="9580.35" dur="1.65">just something that I&apos;m putting out there
and asking.</text><text start="9582.0" dur="3.149">In other words, to relate this to what you
were saying is, is it finite?</text><text start="9585.149" dur="1.0">And even if</text><text start="9586.149" dur="3.351">it&apos;s finite, is it like a tractable amount
of finiteness that either we can handle it</text><text start="9589.5" dur="5.29">or we can design an AI that we feel like we
have a handle over that can understand it?</text><text start="9594.79" dur="1.44">Yeah.</text><text start="9596.23" dur="7.66">And if you try to say, okay, can I mine myself,
my brain for all the implicit assumptions</text><text start="9603.89" dur="2.32">and put them all?</text><text start="9606.21" dur="4.77">I think every version of the thought experiment,
you realize you can&apos;t.</text><text start="9610.98" dur="6.31">But even if you do, that&apos;s only the ones that
are associated with the kinds of context you&apos;ve</text><text start="9617.29" dur="1.76">been exposed to so far.</text><text start="9619.05" dur="3.03">But there are a heap of things that nobody
has ever done that maybe</text><text start="9622.08" dur="8.5">an AI could do that now also have to be factored
in there that you didn&apos;t think to say because</text><text start="9630.58" dur="3.13">it was never something that happened previously
where there is evolved knowledge to say don&apos;t</text><text start="9633.71" dur="4.37">do those things.</text><text start="9638.08" dur="6.37">There&apos;s also something that because humans
all co-evolved and have</text><text start="9644.45" dur="5.51">similarish nervous systems and all kind of
need to breathe oxygen and want a world that</text><text start="9649.96" dur="4.17">has similar physics and whatever, there&apos;s
some stuff where the implicit processing is</text><text start="9654.13" dur="4.41">kind of baked into the evolutionary process
that brought us that is not true for a silica-based</text><text start="9658.54" dur="5.649">system that is not subject to the same physical
constraints, right?</text><text start="9664.189" dur="1.0">Optimize itself in a very</text><text start="9665.189" dur="3.511">different physical environment.</text><text start="9668.7" dur="2.41">And so even the thing that we would call just
kind of</text><text start="9671.11" dur="9.05">an intuitive thing is very different for a
very different type of system.</text><text start="9680.16" dur="3.48">So I would say when it</text><text start="9683.64" dur="5.7">comes to the topic of AGI alignment, there
are different positions on alignment.</text><text start="9689.34" dur="1.0">I would</text><text start="9690.34" dur="6.58">say the strongest position is AGI alignment
is not – well, first we actually have to</text><text start="9696.92" dur="2.43">discuss what we even mean by alignment, right?</text><text start="9699.35" dur="4.64">Because initially, the topic of alignment</text><text start="9703.99" dur="4.369">means can we ensure that the AI is aligned
with human values and human intentions so</text><text start="9708.359" dur="4.751">that when you say bring me a cup of coffee
that you&apos;re – all those implicit intentions</text><text start="9713.11" dur="4.6">that you have are not damaged in the process.</text><text start="9717.71" dur="3.47">But if we look at the – all of the animals</text><text start="9721.18" dur="4.299">and factory farms and the extinct species
and the disruption to the environment and</text><text start="9725.479" dur="4.361">the conflict between humans and other humans
and class subjugation and all those things,</text><text start="9729.84" dur="6.33">you can say human intent is not unproblematic.</text><text start="9736.17" dur="3.519">And exponentiating human intent as is is not</text><text start="9739.689" dur="2.951">actually an awesome solution.</text><text start="9742.64" dur="2.62">And so do you want it to be aligned with human
intention?</text><text start="9745.26" dur="4.83">Well, it currently looks like human intention
has created a social sphere and a technosphere</text><text start="9750.09" dur="5.21">that is fundamentally misaligned with the
biosphere they depend upon and it is the technosphere</text><text start="9755.3" dur="12.31">social sphere complex is kind of auto-poetically
scaling while debasing the substrate it depends</text><text start="9767.61" dur="1.0">upon.</text><text start="9768.61" dur="2.76">In other words, it&apos;s on a self-termination
path.</text><text start="9771.37" dur="2.55">So – and that represents something</text><text start="9773.92" dur="4.62">like the collective intent of humans currently
in this context.</text><text start="9778.54" dur="1.63">So if you ensure that the</text><text start="9780.17" dur="8.71">AI is aligned with intent in the narrow obvious
definition, that is also not a good definition</text><text start="9788.88" dur="2.08">of alignment.</text><text start="9790.96" dur="4.12">So insofar as the humans are not aligned,
their intent is not aligned with</text><text start="9795.08" dur="3.26">the biosphere they depend upon and is not
aligned with the well-being of other humans</text><text start="9798.34" dur="2.099">who will produce counter responses.</text><text start="9800.439" dur="2.141">And most of the time isn&apos;t even aligned with
their</text><text start="9802.58" dur="3.28">own future good as is the case with all addictive
behavior.</text><text start="9805.86" dur="1.0">Right?</text><text start="9806.86" dur="1.0">Sorry to interrupt.</text><text start="9807.86" dur="1.0">I&apos;m so sorry.</text><text start="9808.86" dur="1.579">Is this a place where you disagree with Yudkowsky</text><text start="9810.439" dur="5.581">or has he also expressed points that are in
alignment with your point about alignment?</text><text start="9816.02" dur="1.87">I don&apos;t know if he has.</text><text start="9817.89" dur="2.36">There&apos;s nothing that I know of that I disagree
with.</text><text start="9820.25" dur="1.0">I think</text><text start="9821.25" dur="4.89">when he&apos;s – I&apos;m sure he&apos;s thought about
this.</text><text start="9826.14" dur="1.99">I just haven&apos;t read that of</text><text start="9828.13" dur="1.0">him.</text><text start="9829.13" dur="3.599">When he&apos;s talking about alignment, he&apos;s talking
about this more basic issue</text><text start="9832.729" dur="6.71">of as he tries to give the example, if you
have a very powerful AI and you ask it to</text><text start="9839.439" dur="4.501">do something that would be very hard for us
to do but should be a tractable task for it</text><text start="9843.94" dur="7.03">like replicate the strawberry at a cellular
level, that can you make an AI that could</text><text start="9850.97" dur="4.71">do that that doesn&apos;t destroy the world in
the process?</text><text start="9855.68" dur="1.6">Even that level, not being clear</text><text start="9857.28" dur="3.15">how to do it at all is the thing he&apos;s generally
focused on.</text><text start="9860.43" dur="1.81">I&apos;m sure he has deeper arguments</text><text start="9862.24" dur="6.13">beyond it that if we got that thing down,
what else would we have to get?</text><text start="9868.37" dur="8.02">So one could say – like if we look at all
of the social media issues that the social</text><text start="9876.39" dur="10.37">dilemma addressed where you can say – Facebook
can say we&apos;re giving people what they want</text><text start="9886.76" dur="4.04">or TikTok or the YouTube algorithm or Instagram
or whatever because we&apos;re not forcing people</text><text start="9890.8" dur="5.21">to use it except it&apos;s saying we&apos;re giving
people what we want in the same way that the</text><text start="9896.01" dur="3.011">drug dealer who gives drugs to kids is saying
that, right?</text><text start="9899.021" dur="2.689">Which is we can create addiction.</text><text start="9901.71" dur="5.2">We can kind of prey on the lower angels of
people&apos;s nature and if they&apos;re individual</text><text start="9906.91" dur="3.44">people who don&apos;t even know they&apos;re in such
a competition and we&apos;re talking about</text><text start="9910.35" dur="5.12">a major fraction of a trillion-dollar organization
employing supercomputers and AI in an asymmetric</text><text start="9915.47" dur="6.96">warfare against them to say we are giving
them what they want while engineering what</text><text start="9922.43" dur="1.06">they want.</text><text start="9923.49" dur="2.11">That&apos;s – it&apos;s a tricky proposition.</text><text start="9925.6" dur="4.11">But we can see how the algorithm that optimizes</text><text start="9929.71" dur="5.21">for – whether it&apos;s time on site or engagement,
both have happened.</text><text start="9934.92" dur="1.83">That&apos;s a perverse metric,</text><text start="9936.75" dur="1.04">right?</text><text start="9937.79" dur="3.96">Because you can get it through driving addiction
and driving tribalism and driving</text><text start="9941.75" dur="2.87">fear and limbic hijacks and all those things.</text><text start="9944.62" dur="1.32">What&apos;s important to acknowledge, that&apos;s</text><text start="9945.94" dur="1.3">already an AI, right?</text><text start="9947.24" dur="3.31">It&apos;s already a type of artificial intelligence
that is taking</text><text start="9950.55" dur="4.91">personal – collecting personal data about
me and then looking at the totality of content</text><text start="9955.46" dur="7.2">that it can draw from and being able to create
a news feed for me that continues to learn</text><text start="9962.66" dur="4.04">based on what is stickiest for me, right?</text><text start="9966.7" dur="2.84">What I engage with the most.</text><text start="9969.54" dur="3.58">Now in this case, the AI isn&apos;t creating the
content.</text><text start="9973.12" dur="1.97">It&apos;s just choosing which content</text><text start="9975.09" dur="1.87">gets put in front of people.</text><text start="9976.96" dur="3.7">In doing so, it is now incentivizing all content
creators</text><text start="9980.66" dur="2.83">to create the content that does best on those
algorithms.</text><text start="9983.49" dur="2.55">So it&apos;s actually in a way farming</text><text start="9986.04" dur="5.59">all human content creators because it&apos;s incentivizing
them to do whatever it is that</text><text start="9991.63" dur="1.649">is within the algorithm&apos;s bidding.</text><text start="9993.279" dur="3.051">Now as soon as we have synthetic media, which
is</text><text start="9996.33" dur="6.17">rapidly emerging, where we can not just have
humans creating whatever the TikTok video</text><text start="10002.5" dur="4.29">is, but we can have deep fake versions of
them that are being created very rapidly.</text><text start="10006.79" dur="5.01">And now you have a curation AI where that
first one&apos;s AI was just to curate the stickiest</text><text start="10011.8" dur="5.57">stuff personalized to people and creation
AI&apos;s that can be creating multiple things</text><text start="10017.37" dur="3.69">to split test relative to each other and the
feedback loop between those, you can just</text><text start="10021.06" dur="3.65">see that the problem that has been there just
hypertrophies.</text><text start="10024.71" dur="5.35">Let alone the breakdown of</text><text start="10030.06" dur="4.3">the epistemic comment and the ability to tell
what is real and not real and all those types</text><text start="10034.36" dur="1.0">of issues.</text><text start="10035.36" dur="4.47">I want to come back to where we were.</text><text start="10039.83" dur="4.46">So obviously the curation algorithm</text><text start="10044.29" dur="3.439">is saying that it&apos;s aligned with human intent,
but not really.</text><text start="10047.729" dur="1.0">It&apos;s aligned with human</text><text start="10048.729" dur="2.431">intent because it&apos;s giving stuff that they
empirically like because they&apos;re engaging</text><text start="10051.16" dur="1.17">with it.</text><text start="10052.33" dur="5.3">But most people then actually end up having
regret of how much time they spend</text><text start="10057.63" dur="2.72">on those platforms and wish that they did
less of it.</text><text start="10060.35" dur="1.19">And they don&apos;t plan in the day,</text><text start="10061.54" dur="3.04">I want to spend this much time doom scrolling.</text><text start="10064.58" dur="3.609">And so is it really aligned with their intent?</text><text start="10068.189" dur="5.101">And in general, is aligning with intent that
includes the lowest angels of people&apos;s nature</text><text start="10073.29" dur="1.79">type intent, is that a good thing?</text><text start="10075.08" dur="1.85">Is that a good type of alignment when you
factor the</text><text start="10076.93" dur="6.759">totality of effects it has?</text><text start="10083.689" dur="6.771">So we could say that the solution to the algorithm
issue should</text><text start="10090.46" dur="9.64">be that because the social media platform
is gathering personal data about me, and it&apos;s</text><text start="10100.1" dur="5.049">gathering based on its ability to model my
psyche based on all of who my friends are</text><text start="10105.149" dur="4.311">and what I like and what I don&apos;t like and
all those things and my mouse hover patterns.</text><text start="10109.46" dur="4.83">It has an amount of data about me that can
model my future behavior better than a lawyer</text><text start="10114.29" dur="4.14">or a psychotherapist or anybody else could.</text><text start="10118.43" dur="2.96">So there are provisions in law of privileged</text><text start="10121.39" dur="1.009">information, right?</text><text start="10122.399" dur="1.891">If you have privileged information, what are
you allowed to do with</text><text start="10124.29" dur="1.28">it?</text><text start="10125.57" dur="3.96">And there are provisions in law about undue
influence.</text><text start="10129.53" dur="2.6">So we could argue that the</text><text start="10132.13" dur="4.49">platforms are gathering privileged information,
that they have undue influence.</text><text start="10136.62" dur="1.0">As a result,</text><text start="10137.62" dur="1.1">there should be a fiduciary responsibility.</text><text start="10138.72" dur="1.95">This is one of the things that we do when</text><text start="10140.67" dur="2.21">there&apos;s a radical asymmetry of power.</text><text start="10142.88" dur="1.92">Because if there&apos;s a symmetry of power, we
say caveat</text><text start="10144.8" dur="1.35">emptor buyer beware.</text><text start="10146.15" dur="3.04">It&apos;s kind of on you to make sure that you
don&apos;t get sold a shitty</text><text start="10149.19" dur="1.0">thing or engage with.</text><text start="10150.19" dur="3.76">But if there&apos;s a radical asymmetry of power,
can you tell the kid buyer</text><text start="10153.95" dur="3.529">beware about an adult that is playing them?</text><text start="10157.479" dur="2.571">No, you can&apos;t, right?</text><text start="10160.05" dur="1.4">And so in that way,</text><text start="10161.45" dur="5.14">can the person who isn&apos;t a doctor know that
they really don&apos;t need a kidney transplant</text><text start="10166.59" dur="3.51">if the doctor tells them that they do because
the doctor gets paid when they give kidney</text><text start="10170.1" dur="1.0">transplants?</text><text start="10171.1" dur="1.0">Well, that&apos;s so bad.</text><text start="10172.1" dur="1.021">We don&apos;t want that to happen.</text><text start="10173.121" dur="1.089">We make law saying doctors</text><text start="10174.21" dur="1.79">can&apos;t do that.</text><text start="10176.0" dur="4.58">There&apos;s a Hippocratic oath to act not just
in their own economic interest,</text><text start="10180.58" dur="6.93">but they are an agent on behalf of the principal
because the principal cannot buyer beware,</text><text start="10187.51" dur="1.469">right?</text><text start="10188.979" dur="4.101">And so then there is a board of other doctors
who are also at that upper asymmetry</text><text start="10193.08" dur="2.47">who can verify did the person do malpractice
or not.</text><text start="10195.55" dur="1.57">Same with a lawyer.</text><text start="10197.12" dur="1.0">If the lawyer</text><text start="10198.12" dur="5.989">wanted to just bill by the 15-minute sections
maximally to drain as much money from me,</text><text start="10204.109" dur="3.561">they could because there&apos;s no way I can know
that what they&apos;re telling me about law is</text><text start="10207.67" dur="4.5">wrong because they have so much asymmetric
knowledge relative to me that we have to make</text><text start="10212.17" dur="1.0">that illegal.</text><text start="10213.17" dur="5.54">We have to make sure that the lawyer is an
agent on behalf of me as the</text><text start="10218.71" dur="1.0">principal.</text><text start="10219.71" dur="5.319">So with lawyers and doctors and therapists
and financial advisors, we have</text><text start="10225.029" dur="2.801">this fiduciary principal agent binding thing,
right?</text><text start="10227.83" dur="2.27">And it&apos;s because there&apos;s such an asymmetry</text><text start="10230.1" dur="2.75">that there cannot be self-protection, right?</text><text start="10232.85" dur="1.44">If I&apos;m engaging with them and giving them</text><text start="10234.29" dur="4.56">this privileged information and they wanted
to fuck me, they could, right?</text><text start="10238.85" dur="1.31">And so but</text><text start="10240.16" dur="3.21">for my own well-being, I have to engage with
them and give them this information.</text><text start="10243.37" dur="1.0">So we</text><text start="10244.37" dur="1.46">have to have some legal way of binding that.</text><text start="10245.83" dur="1.59">But of course, in the case to bind it where</text><text start="10247.42" dur="5.37">the lawyers all have some practice law that
they can be bound by, they can be shown they</text><text start="10252.79" dur="6.319">did malpractice and same with doctors, that
requires a legal body of lawyers or a body</text><text start="10259.109" dur="3.141">of doctors that can assess if what that doctor
or lawyer did was wrong.</text><text start="10262.25" dur="1.0">So somebody else</text><text start="10263.25" dur="2.8">that has even higher asymmetry, right?</text><text start="10266.05" dur="2.22">The group of the top things.</text><text start="10268.27" dur="1.0">This becomes very</text><text start="10269.27" dur="2.85">hard when it comes to AI.</text><text start="10272.12" dur="5.8">So let&apos;s start by saying the rather than the
AI being a rivalrous</text><text start="10277.92" dur="6.75">relationship with me, when I&apos;m on social media,
and it is actually gathering the information</text><text start="10284.67" dur="5.75">about me not to optimize my well-being, but
to optimize ad sales for other for the corporation</text><text start="10290.42" dur="5.56">that is the platform and the corporations
that are its actual customers.</text><text start="10295.98" dur="1.4">Right?</text><text start="10297.38" dur="1.0">In which</text><text start="10298.38" dur="6.48">case it has the incentive to prey on the lowest
angels of my nature and then be able to say</text><text start="10304.86" dur="2.119">it was my intent and I had free choice.</text><text start="10306.979" dur="2.651">So we could say that should be a violation
of</text><text start="10309.63" dur="2.34">the principal agent issue.</text><text start="10311.97" dur="2.75">And because there&apos;s undue influence, we can
show there&apos;s undue</text><text start="10314.72" dur="1.84">influence.</text><text start="10316.56" dur="2.77">Consilience Project, we wrote some articles
on this.</text><text start="10319.33" dur="2.22">There&apos;s one on undue influence</text><text start="10321.55" dur="3.21">that makes this argument more deeply.</text><text start="10324.76" dur="2.55">And because you can show it&apos;s gathering privileged</text><text start="10327.31" dur="6.54">information, it should be in fiduciary relationship
where it has to pay attention to my goals</text><text start="10333.85" dur="3.42">and optimize aligned with my goals rather
than I&apos;m the product and it&apos;s optimizing</text><text start="10337.27" dur="2.77">with the goals of the corporation or its customers.</text><text start="10340.04" dur="1.0">Right?</text><text start="10341.04" dur="1.0">In order to do that, that would change</text><text start="10342.04" dur="1.0">its business model.</text><text start="10343.04" dur="1.0">It couldn&apos;t have an ad model anymore.</text><text start="10344.04" dur="1.17">I would either have to pay</text><text start="10345.21" dur="5.15">a monthly fee for it, or the state or some
commons would have to pay for it and everybody</text><text start="10350.36" dur="5.2">had access to it or some other thing.</text><text start="10355.56" dur="4.15">That seems like a very good step in the right
direction.</text><text start="10359.71" dur="1.52">And that is an alignment issue thing.</text><text start="10361.23" dur="1.0">Right?</text><text start="10362.23" dur="1.0">The principal agent issue is a way of trying</text><text start="10363.23" dur="7.1">to solve the alignment, which is to say that
this more powerful AI system here, the curatorial</text><text start="10370.33" dur="6.85">AI, social media, would be aligned with my
interest in bound in some way.</text><text start="10377.18" dur="1.0">And maybe that</text><text start="10378.18" dur="2.32">would be a way to, maybe we would extend that
to all types of AI.</text><text start="10380.5" dur="1.54">Well, of course, in the</text><text start="10382.04" dur="6.06">AGI case where it becomes fully autonomous
and becomes more powerful than any other systems,</text><text start="10388.1" dur="4.36">what other system can check it to see if what
it is doing is actually aligned or not?</text><text start="10392.46" dur="1.0">There</text><text start="10393.46" dur="2.1">isn&apos;t a group of lawyers that can check that
lawyer.</text><text start="10395.56" dur="1.0">Right?</text><text start="10396.56" dur="1.0">So that becomes a big</text><text start="10397.56" dur="1.0">issue.</text><text start="10398.56" dur="1.96">And if it really becomes autonomous, as opposed
to empowering a corporation, which</text><text start="10400.52" dur="5.81">is what it is, it&apos;s different.</text><text start="10406.33" dur="5.66">And so this is one part on the topic of alignment
and</text><text start="10411.99" dur="2.72">alignment with our intention or well-being.</text><text start="10414.71" dur="1.66">You can do superficial alignment with our</text><text start="10416.37" dur="4.25">intention, which the social media thing already
does, but it&apos;s not aligned with our actual</text><text start="10420.62" dur="7.489">well-being because an asymmetric agent is
capable of exploiting your sense of intentionality.</text><text start="10428.109" dur="7.421">So, and similarly, when you say there&apos;s a
common sense that says, don&apos;t bring the</text><text start="10435.53" dur="5.93">door down, you&apos;re bringing me coffee, there
should be a common sense that says, don&apos;t</text><text start="10441.46" dur="3.609">overfish the entire ocean and cut all the
damn trees down and turn them into forests</text><text start="10445.069" dur="1.341">in the process of growing GDP.</text><text start="10446.41" dur="3.34">And there is clearly not.</text><text start="10449.75" dur="2.31">Right?</text><text start="10452.06" dur="1.429">And so we can see that</text><text start="10453.489" dur="5.152">the current, without AI, human world system
already actually doesn&apos;t have that kind</text><text start="10458.641" dur="3.959">of check and balance in it, in all the areas
that it should, just so long as the harms</text><text start="10462.6" dur="4.29">are externalized somewhere far enough that
we don&apos;t instantly notice them and change</text><text start="10466.89" dur="2.91">them.</text><text start="10469.8" dur="4.46">So the question of what do we, if we have
radically more powerful optimizer than</text><text start="10474.26" dur="5.03">we already have, what do we align its goal
with?</text><text start="10479.29" dur="1.689">If we just say align it with our intention,</text><text start="10480.979" dur="3.701">but it can change our intention because it
can behavior mod me and we can&apos;t possibly</text><text start="10484.68" dur="4.809">deal with that because of the asymmetry, that&apos;s
no good as in the Facebook case.</text><text start="10489.489" dur="5.441">If we try to align it with the interest of
a nation state that can drive arms races with</text><text start="10494.93" dur="4.55">other ones and other nation states and war
or drive it, align it with the current economy</text><text start="10499.48" dur="3.82">that&apos;s misaligned with the biosphere, that&apos;s
not good.</text><text start="10503.3" dur="1.79">So the topic of alignment is actually</text><text start="10505.09" dur="3.74">an incredibly deep topic.</text><text start="10508.83" dur="2.319">And this now gets to what you&apos;ve probably
addressed on your</text><text start="10511.149" dur="1.181">show in other places.</text><text start="10512.33" dur="6.449">It gets to a very philosophic issue, which
is the kind of is-ought issue.</text><text start="10518.779" dur="3.441">Which is science can say what is, it can&apos;t
say what ought, right?</text><text start="10522.22" dur="1.86">And that kind of distinction</text><text start="10524.08" dur="5.29">by Mill and others historically, and that
the applied side of science is technology</text><text start="10529.37" dur="6.38">and engineering can change what is, but what
ought to be, what is the ethics that is somehow</text><text start="10535.75" dur="6.3">compatible with science is a challenge.</text><text start="10542.05" dur="2.17">The best answer we have had, arguably,</text><text start="10544.22" dur="8.47">that came from the mind that created both
a lot of our nuclear technology and our foundations</text><text start="10552.69" dur="2.299">of AI, von Neumann, was game theory, right?</text><text start="10554.989" dur="3.45">That the idea that is good is the idea that
doesn&apos;t</text><text start="10558.439" dur="1.431">lose.</text><text start="10559.87" dur="6.859">And we can arguably say that that thing instantiated
by markets and national security</text><text start="10566.729" dur="8.221">protocols has actually been the dominant definition
of ought that ends up driving the power of</text><text start="10574.95" dur="1.0">technology.</text><text start="10575.95" dur="2.97">Because if science says, we can&apos;t say what
ought, we can&apos;t.</text><text start="10578.92" dur="1.41">We can only say what is.</text><text start="10580.33" dur="3.88">But we&apos;re really fucking powerful at saying
what is in a way that reduces to technology</text><text start="10584.21" dur="1.0">that</text><text start="10585.21" dur="3.019">changes what is, where we can optimize some
metrics and say it&apos;s good, even if we externalize</text><text start="10588.229" dur="3.061">a lot of harm to other metrics or optimize
in groups with expensive outgroups or whatever</text><text start="10591.29" dur="1.08">it is,</text><text start="10592.37" dur="1.03">right?</text><text start="10593.4" dur="5.43">But we say that not only do we not have an
ought, but that any system of ought is not</text><text start="10598.83" dur="1.989">the philosophy of science.</text><text start="10600.819" dur="6.151">So is, insofar as that&apos;s concerned, out of
scope or gibberish,</text><text start="10606.97" dur="4.24">well, then what ends up guiding the power
of technology?</text><text start="10611.21" dur="1.0">Markets do.</text><text start="10612.21" dur="1.14">And to some extent,</text><text start="10613.35" dur="1.0">national security does.</text><text start="10614.35" dur="3.5">In other words, rival risk game theoretic
kind of interests.</text><text start="10617.85" dur="5.4">And so what gets researched, the thing that
has the most market potential.</text><text start="10623.25" dur="5.989">And so then, again, it is what is actually
developing the technology?</text><text start="10629.239" dur="1.0">Because as Einstein</text><text start="10630.239" dur="5.461">said, like, I was developing science not knowing
it was going to do that application,</text><text start="10635.7" dur="3.47">didn&apos;t want that application, wanted science
for social responsibility.</text><text start="10639.17" dur="1.0">But what ends up,</text><text start="10640.17" dur="6.37">for the most part, the research that gets
funded is R&amp;D towards something that ends</text><text start="10646.54" dur="1.0">up either</text><text start="10647.54" dur="4.51">advancing the interest of a nation state or
the interest of a corporation or whatever</text><text start="10652.05" dur="1.12">the metric</text><text start="10653.17" dur="8.16">set, the game theoretic metric set of the
group of people that is doing the thing, right?</text><text start="10661.33" dur="4.342">And so what I would say is that as we get
to more and more powerful is, more and more</text><text start="10665.672" dur="1.0">powerful</text><text start="10666.672" dur="3.288">science that creates more and more powerful
tech and exponentially powerful tech, especially</text><text start="10669.96" dur="1.0">as</text><text start="10670.96" dur="2.2">we&apos;re already hitting fragility of the planetary
systems.</text><text start="10673.16" dur="1.541">And when we say more powerful, we mean</text><text start="10674.701" dur="5.409">like exponentially more powerful, not iteratively
more powerful.</text><text start="10680.11" dur="1.81">You have to have a system of</text><text start="10681.92" dur="3.75">ought powerful enough to guide, bind, and
direct it.</text><text start="10685.67" dur="2.16">Because if you don&apos;t, it is powerful enough</text><text start="10687.83" dur="6.23">to, in whatever it is optimizing for, destroy
enough that what it optimizes for doesn&apos;t</text><text start="10694.06" dur="1.0">matter</text><text start="10695.06" dur="1.28">anymore.</text><text start="10696.34" dur="6.92">Now, this is a fundamentally deep metaphysical
philosophical issue.</text><text start="10703.26" dur="1.809">And of course,</text><text start="10705.069" dur="6.261">when we talk about regulation, law, the basis
of law is jurisprudence, right?</text><text start="10711.33" dur="1.899">And is ought questions,</text><text start="10713.229" dur="1.0">right?</text><text start="10714.229" dur="6.561">Applied ethics that get institutionalized
for exactly this reason.</text><text start="10720.79" dur="1.18">And so when we say if we</text><text start="10721.97" dur="6.27">have tech that is powerful enough to do pretty
much fucking anything, what should be guiding</text><text start="10728.24" dur="1.0">that</text><text start="10729.24" dur="2.53">and what should be binding it?</text><text start="10731.77" dur="3.839">And if we don&apos;t answer those well, what is
the default of what</text><text start="10735.609" dur="2.811">will be guiding it and binding it currently?</text><text start="10738.42" dur="2.8">And what does that world look like?</text><text start="10741.22" dur="2.04">So this is super cheerful conversation.</text><text start="10743.26" dur="3.42">What is the call to action?</text><text start="10746.68" dur="5.52">Okay, we&apos;re quite a far ways away from that.</text><text start="10752.2" dur="2.75">Let me try to expedite a couple other parts.</text><text start="10754.95" dur="1.0">When we</text><text start="10755.95" dur="5.06">were mentioning the AI risk, we said AI empowering
bad actors.</text><text start="10761.01" dur="2.91">So you can think about whether a bad</text><text start="10763.92" dur="4.23">actor is a domestic terrorist who the best
thing they can do right now is get an AR-15</text><text start="10768.15" dur="1.0">and shoot</text><text start="10769.15" dur="1.7">up a transformer station to take down the
power lines.</text><text start="10770.85" dur="2.562">AR-15 is a kind of tech that has a lot of</text><text start="10773.412" dur="1.0">capability.</text><text start="10774.412" dur="1.678">It&apos;s a kind of tech that has some capability.</text><text start="10776.09" dur="2.269">As you have more people getting a</text><text start="10778.359" dur="7.92">sense of being disenfranchised by the current
system and being motivated to utilize what</text><text start="10786.279" dur="1.0">is at</text><text start="10787.279" dur="5.091">their resources to do something about it,
and the barrier of entry of the more powerful</text><text start="10792.37" dur="1.0">tech is</text><text start="10793.37" dur="6.5">getting lowered, you can put those things
together, right?</text><text start="10799.87" dur="3.25">And – but whether you&apos;re talking about</text><text start="10803.12" dur="4.23">that or you&apos;re talking about international
terrorism from larger groups, or you&apos;re talking</text><text start="10807.35" dur="2.66">about full military applications.</text><text start="10810.01" dur="7.179">But let&apos;s just go ahead and say – and like,
can we make deep</text><text start="10817.189" dur="12.151">fakes that make the worst kinds of confusion,
conspiracy theory, in-group, out-group thinking,</text><text start="10829.34" dur="1.41">propaganda, of course, right?</text><text start="10830.75" dur="2.46">Like, that is an emerging technology that&apos;s
eminent.</text><text start="10833.21" dur="2.67">Can we use</text><text start="10835.88" dur="6.809">people&apos;s voices and what looks like their
video and text for ransom and fucked up stuff?</text><text start="10842.689" dur="1.0">Can we –</text><text start="10843.689" dur="3.821">like, so you can think of all the bad actor
applications, and then you can pretty much</text><text start="10847.51" dur="5.3">apply it to – this is a piece of theory
I wanted to say.</text><text start="10852.81" dur="2.75">Every technology has certain affordances.</text><text start="10855.56" dur="4.509">If you build it, it can do things, where without
that technology, you couldn&apos;t do those things,</text><text start="10860.069" dur="1.0">right?</text><text start="10861.069" dur="3.481">A tractor allows me to do things that I couldn&apos;t
do without a tractor, just the shovel,</text><text start="10864.55" dur="7.28">in terms of volume and types of work and various
things.</text><text start="10871.83" dur="2.409">Every technology is also combinatorial</text><text start="10874.239" dur="5.891">with other tech because what a hammer can
do, if I don&apos;t have a saw to cut the timber</text><text start="10880.13" dur="1.0">first,</text><text start="10881.13" dur="1.54">is very different than what it can do if you
have that.</text><text start="10882.67" dur="2.141">And obviously, it requires the blacksmithing</text><text start="10884.811" dur="1.019">hammer, right?</text><text start="10885.83" dur="3.98">So, you have – you don&apos;t just have individual
tech, you have tech ecosystems.</text><text start="10889.81" dur="5.419">And the combinatorial potential of these pieces
of tech together have different affordances,</text><text start="10895.229" dur="1.17">right?</text><text start="10896.399" dur="4.851">So – but then what do we use it for is based
on the motivational landscape.</text><text start="10901.25" dur="5.569">I can use something like a hammer to be Jimmy
Carter and build houses for the homeless with</text><text start="10906.819" dur="5.511">Habitat for Humanity, or I can use it as a
weapon and kill people with it.</text><text start="10912.33" dur="3.85">And so, the tech has the affordances to do
both of those.</text><text start="10916.18" dur="4.25">So, the tech will be developed and utilized
based on motivational landscapes.</text><text start="10920.43" dur="1.0">Sure.</text><text start="10921.43" dur="4.38">And just briefly, and going back to earlier,
it&apos;s not just dual because that would be double-edged,</text><text start="10925.81" dur="1.0">it&apos;s multipolar.</text><text start="10926.81" dur="1.0">Omni.</text><text start="10927.81" dur="1.0">Yes.</text><text start="10928.81" dur="1.0">Okay.</text><text start="10929.81" dur="4.82">So, what we can say is the tech will end up
getting utilized by all – potentially getting</text><text start="10934.63" dur="4.96">utilized by all agents for all motives, that
that tech offers affordances relevant to their</text><text start="10939.59" dur="2.309">motives.</text><text start="10941.899" dur="2.311">Right?</text><text start="10944.21" dur="3.75">And so, when we&apos;re building a piece of tech,
we don&apos;t want to think about what is our motive</text><text start="10947.96" dur="1.0">to use it.</text><text start="10948.96" dur="4.34">We want to think about are we making a new
capacity that didn&apos;t exist before, lowering</text><text start="10953.3" dur="4.97">the barrier of entry to a particular kind
of capacity where now what are all the motives</text><text start="10958.27" dur="4.06">that all agents have who have access to that
technology and what is the world that results</text><text start="10962.33" dur="3.96">from everybody utilizing it that way?</text><text start="10966.29" dur="4.17">That&apos;s factoring second, third, fourth order
thinking into the development of something</text><text start="10970.46" dur="4.46">– a new capacity that changes the landscape
of the world.</text><text start="10974.92" dur="6.08">I would say that every scientist who is working
on synthetic bio for curing cancer or AI for</text><text start="10981.0" dur="8.59">solving some awesome problem, every scientist
and engineer and et cetera has an entrepreneur,</text><text start="10989.59" dur="6.13">an ethical responsibility to think about the
new capability they&apos;re bringing into the world</text><text start="10995.72" dur="3.31">that didn&apos;t exist, not just how they want
to use it,</text><text start="10999.03" dur="4.34">but the totality of use that will happen by
them having brought it into the world that</text><text start="11003.37" dur="4.05">wouldn&apos;t have they not.</text><text start="11007.42" dur="4.19">There is no current – when I say there&apos;s
an ethical responsibility, there is no legal</text><text start="11011.61" dur="1.0">responsibility.</text><text start="11012.61" dur="3.31">There is no fiduciary responsibility where
you are liable for the harms that get produced</text><text start="11015.92" dur="4.87">by a thing that you bring about that someone
else reverses and uses a different way.</text><text start="11020.79" dur="5.569">But there is financial incentive and Nobel
Prizes for developing the thing for your purpose</text><text start="11026.359" dur="3.891">and then, again, socializing the losses of
whatever anybody else does with it.</text><text start="11030.25" dur="5.43">So this is one of those cases where the personal
near-term narrow motive – this is us being</text><text start="11035.68" dur="7.86">fucking narrow AIs in an ethical sense – is
to do the thing even if the net result of</text><text start="11043.54" dur="3.439">the thing ends up being catastrophically harmful.</text><text start="11046.979" dur="3.871">So the incentive deterrent, the motivational
landscape is messed up.</text><text start="11050.85" dur="5.02">So every tech – now I want to make a couple
more philosophy of tech arguments.</text><text start="11055.87" dur="4.66">Tech is not values neutral, meaning the hammer
is not good or bad.</text><text start="11060.53" dur="3.709">It&apos;s just a hammer, and whether you use it
to build a house for the homeless or beat</text><text start="11064.239" dur="1.301">someone&apos;s head is up to you.</text><text start="11065.54" dur="2.51">The motivational landscape and the tech have
nothing to do with each other.</text><text start="11068.05" dur="3.16">This is not true.</text><text start="11071.21" dur="9.06">If a technology gives the capacity to do something
that provides advantage, relative advantage</text><text start="11080.27" dur="4.541">in a competitive environment, whether it&apos;s
one nation-state competing with another nation-state</text><text start="11084.811" dur="4.309">or one corporation or one tribe with another
one.</text><text start="11089.12" dur="6.42">If it provides significant competitive advantage,
if you use it a particular way, then anyone</text><text start="11095.54" dur="4.67">using it that way creates a multipolar trap
that obligates the others to use it that way</text><text start="11100.21" dur="3.28">or a related way.</text><text start="11103.49" dur="3.3">And so we end up getting a couple things.</text><text start="11106.79" dur="5.81">This is the classic example I&apos;ve used a lot
is if we think about – and it&apos;s because</text><text start="11112.6" dur="4.49">there&apos;s been so much analysis on this example.</text><text start="11117.09" dur="5.02">If you think about the plow as a technology
that was one of the key technologies that</text><text start="11122.11" dur="7.27">moved us from sub-Dunbar number, hunter-gatherer,
maybe horticultural subsistence cultures to</text><text start="11129.38" dur="4.55">large agricultural civilizations.</text><text start="11133.93" dur="4.7">The plow is not a neutral technology where
you can choose to use it or not choose to</text><text start="11138.63" dur="1.0">use it.</text><text start="11139.63" dur="4.519">The populations that used it made it through
famines and grew their populations way faster</text><text start="11144.149" dur="4.531">than the ones who didn&apos;t, and they used their
much, much larger populations to win at wars.</text><text start="11148.68" dur="3.83">So the meanset that goes along with using
it ends up making it through evolution.</text><text start="11152.51" dur="3.79">The meanset that doesn&apos;t doesn&apos;t make it through
evolution.</text><text start="11156.3" dur="7.28">And correspondingly, there are – in order
to implement the tech, it has ethical consequences.</text><text start="11163.58" dur="8.021">I had to clearcut in order to do the kind
of row cropping that the plow really makes</text><text start="11171.601" dur="1.539">possible.</text><text start="11173.14" dur="7.4">I have to get an open piece of land that is
now being used for just human food production</text><text start="11180.54" dur="3.96">and not any other purpose, so I&apos;m going to
clearcut the forest or a meadow or something</text><text start="11184.5" dur="1.95">to be able to do that.</text><text start="11186.45" dur="6.17">I&apos;m going to – so I&apos;m already starting the
Anthropocene with that, right, changing natural</text><text start="11192.62" dur="5.4">environments from whatever value – whatever
habitat and home they were for all the life</text><text start="11198.02" dur="5.57">that was there to now serving the purpose
of optimizing human productivity.</text><text start="11203.59" dur="5.24">And I have to yoke an ox, and I probably have
to castrate it and do a bunch of other things</text><text start="11208.83" dur="3.94">to be able to make that work and probably
beat the ox all day to keep pulling the plow.</text><text start="11212.77" dur="6.15">In order to do that, I have to move from the
animism of I respect the great spirit of the</text><text start="11218.92" dur="4.112">buffalo and we kill this one with reverence
knowing that as it nourishes our bodies, our</text><text start="11223.032" dur="4.638">bodies will be put in the dirt and make grass
that its ancestors will eat and are part of</text><text start="11227.67" dur="4.44">the great circle of life and whatever kind
of idea like that to it&apos;s just a dumb animal.</text><text start="11232.11" dur="3.85">It&apos;s put here for human purposes, man&apos;s dominion
over – it doesn&apos;t have feelings like us,</text><text start="11235.96" dur="5.2">that kind of thing, which then spills over
to it&apos;s just – it&apos;s not like us, so we remove</text><text start="11241.16" dur="4.66">our empathy from it and we apply that to other
races, other classes, other species, other</text><text start="11245.82" dur="2.55">whatever, right?</text><text start="11248.37" dur="3.92">So, something like the plow is not values
neutral.</text><text start="11252.29" dur="5.46">To be able to utilize it, I have to rationalize
its use realizing it creates certain externalities</text><text start="11257.75" dur="5.15">and if I see those externalities, I have to
have a value system that goes along with that.</text><text start="11262.9" dur="1.06">Wait, sorry.</text><text start="11263.96" dur="5.39">To be particular with the word choice, it&apos;s
not that the plow is not value neutral.</text><text start="11269.35" dur="2.15">It&apos;s the use of the plow.</text><text start="11271.5" dur="2.01">Exactly, exactly.</text><text start="11273.51" dur="3.31">And that the plow doesn&apos;t use itself, right?</text><text start="11276.82" dur="2.55">And so, the use of the plow, not values neutral.</text><text start="11279.37" dur="4.69">Now, a life where I am hunting and gathering
versus a life where I&apos;m plowing are also totally</text><text start="11284.06" dur="1.0">different lives.</text><text start="11285.06" dur="3.74">So, it codes a completely different behavior
set.</text><text start="11288.8" dur="3.76">In doing that, it makes completely new mythos,
which is why the hunter-gatherer mythos and</text><text start="11292.56" dur="3.129">the agrarian mythos are completely different,
right?</text><text start="11295.689" dur="4.46">And they have different views towards men
and women and towards sky gods versus animism</text><text start="11300.149" dur="2.371">and towards all kinds of things.</text><text start="11302.52" dur="4.35">And so – but the other thing is that it
provides so much game-theoretic advantage</text><text start="11306.87" dur="4.31">of those who use it relative to those who
don&apos;t that when they hit competitive situations,</text><text start="11311.18" dur="4.85">that&apos;s why there are not many hunter-gatherers
left and why the whole society went agricultural.</text><text start="11316.03" dur="7.56">So, it&apos;s not just that the tech – so, the
tech codes – the tech requires people using</text><text start="11323.59" dur="2.61">it, which changes the patterns of human behavior.</text><text start="11326.2" dur="3.94">Changing the patterns of human behavior automatically
changes the patterns of perception in human</text><text start="11330.14" dur="2.91">psyche, metaphors, cultures, etc.</text><text start="11333.05" dur="4.62">And the externalities that it creates and
the benefits that it causes become implicit</text><text start="11337.67" dur="4.34">to the value system because the value system
can&apos;t be totally incommensurate with the power</text><text start="11342.01" dur="1.74">system, right?</text><text start="11343.75" dur="6.39">And so the dominant narrative ends up becoming
support for – one could argue apologism</text><text start="11350.14" dur="6.111">for the dominant power system because we can&apos;t
feel totally bad about how we meet our needs.</text><text start="11356.251" dur="4.249">So we have to have a value system that deals
with the problems of how we do so.</text><text start="11360.5" dur="3.92">But then it&apos;s also that the tech that does
this becomes obligate because when anyone</text><text start="11364.42" dur="3.6">is using it, everyone else has to or they
kind of lose by default.</text><text start="11368.02" dur="9.35">So when you recognize that tech affects – that
technology when utilized affects patterns</text><text start="11377.37" dur="2.64">of human behavior, humans now do the thing
they do with the tech.</text><text start="11380.01" dur="2.969">So people do this thing and they didn&apos;t used
to do this thing, right?</text><text start="11382.979" dur="3.441">On the cell phone or whatever.</text><text start="11386.42" dur="3.5">To get the benefits of the tech, you have
a totally different pattern of human behavior.</text><text start="11389.92" dur="3.54">As a result, you have different nature of
mind.</text><text start="11393.46" dur="4.59">You have different value systems that emerge
and it becomes obligate or some version of</text><text start="11398.05" dur="3.801">a compensatory tech becomes obligate because
the Amish are not really shaping the world.</text><text start="11401.851" dur="2.519">They&apos;re no longer engaged in the great power
competition.</text><text start="11404.37" dur="1.409">I have a bone to pick there.</text><text start="11405.779" dur="3.601">I had watched a few months ago and I don&apos;t
know anything about the Amish or didn&apos;t know</text><text start="11409.38" dur="4.519">anything about the Amish and I&apos;m just someone
who grew up in this city and so I dismissed</text><text start="11413.899" dur="3.751">them as Luddites like we&apos;ve used that term
several times and they&apos;re backward.</text><text start="11417.65" dur="1.65">They don&apos;t know what they&apos;re talking about.</text><text start="11419.3" dur="1.729">And then I watched a video.</text><text start="11421.029" dur="1.251">The Amish aren&apos;t idiots.</text><text start="11422.28" dur="1.73">They&apos;re not asinine.</text><text start="11424.01" dur="3.74">There&apos;s a reason why they do what they do
and they either explicitly or intuitively</text><text start="11427.75" dur="3.922">understand that the technology changes the
social dynamics in the way that they view</text><text start="11431.672" dur="1.328">the world.</text><text start="11433.0" dur="1.27">Totally.</text><text start="11434.27" dur="1.57">And has ethical considerations.</text><text start="11435.84" dur="1.03">But that influenced me.</text><text start="11436.87" dur="3.35">That influenced perhaps millions of people
because it&apos;s a video I think has a few million</text><text start="11440.22" dur="1.0">hits.</text><text start="11441.22" dur="2.87">Even if they&apos;re local, just them saying, you
know what, I don&apos;t care.</text><text start="11444.09" dur="2.22">I&apos;m going to continue to act right.</text><text start="11446.31" dur="2.18">It can still influence outward.</text><text start="11448.49" dur="2.02">Anyway, and we&apos;re talking about it now.</text><text start="11450.51" dur="4.139">Maybe this will influence people and hopefully
to something positive and hopefully to myself</text><text start="11454.649" dur="1.711">something positive.</text><text start="11456.36" dur="1.35">Yeah.</text><text start="11457.71" dur="6.21">Okay, so it&apos;s not that you come to this a
few times, which is even if you have a meme</text><text start="11463.92" dur="5.32">plex that is not, that doesn&apos;t become part
of the dominant system, can it infect or influence</text><text start="11469.24" dur="3.17">the meme plex in a way that steers it?</text><text start="11472.41" dur="1.34">Yes.</text><text start="11473.75" dur="3.47">But one does not want to be naive about how
much influence that&apos;s going to have.</text><text start="11477.22" dur="3.13">They want to be thoughtful about exactly how
that will work and what kinds of influences.</text><text start="11480.35" dur="4.73">We mentioned not all of Buddhism got picked
up everywhere, right?</text><text start="11485.08" dur="5.75">Like the parts that had to do with why people
should take vows of poverty and live on very</text><text start="11490.83" dur="1.239">little, that didn&apos;t really get picked up.</text><text start="11492.069" dur="3.491">The parts on how to reduce stress got picked
up.</text><text start="11495.56" dur="4.56">The parts on what a healthy motive is didn&apos;t
get picked up as much as the parts on how</text><text start="11500.12" dur="3.199">to empower your motive through a more functional
mind.</text><text start="11503.319" dur="9.141">So it&apos;s important to get the memes might live
in a complex, in a context when they influence,</text><text start="11512.46" dur="3.7">parts of them are going to interact with another
meme plex and the techno plex and everything</text><text start="11516.16" dur="1.0">else.</text><text start="11517.16" dur="5.46">And so you are right to say that it&apos;s not
that they have no influence, but obviously</text><text start="11522.62" dur="7.05">the Amish, not speaking to that they&apos;re dumb
and backwards, but that in their don&apos;t want</text><text start="11529.67" dur="4.97">to engage tech for these reasons argument,
they don&apos;t have a significant say in whether</text><text start="11534.64" dur="3.21">we engage a particular nuclear war or not.</text><text start="11537.85" dur="4.16">Or they were not the ones that overfish the
ocean, cause species extinction, but they</text><text start="11542.01" dur="1.78">also couldn&apos;t stop it.</text><text start="11543.79" dur="5.37">They are not the ones that are creating synthetic
biology that can make totally new species.</text><text start="11549.16" dur="5.489">So the – and this is why I say there is
a naive techno optimism that focuses on the</text><text start="11554.649" dur="3.191">upsides and doesn&apos;t focus on all the nth order
effects and downsides.</text><text start="11557.84" dur="4.68">And as we were just mentioning, the externalities
of tech are not just physical, right?</text><text start="11562.52" dur="3.25">You do this mining to get the thing you want,
but there&apos;s a lot of mining pollution or the</text><text start="11565.77" dur="4.19">herbicide does make farming easier in this
way, but it harms the environment and human</text><text start="11569.96" dur="1.18">health in this other way.</text><text start="11571.14" dur="1.07">That&apos;s physical externalities.</text><text start="11572.21" dur="2.71">But you also get these psychosocial externalities.</text><text start="11574.92" dur="4.0">You use Facebook for this purpose and it ends
up eroding democracy and doubling down on</text><text start="11578.92" dur="3.51">bias and increasing addiction and body dysmorphia
and things like that, right?</text><text start="11582.43" dur="5.24">So the tech affects not – it doesn&apos;t – it
has effects that are not the ones you intended,</text><text start="11587.67" dur="1.0">some of which might be positive.</text><text start="11588.67" dur="3.68">You can have a positive externality, but it
might have a lot of negative externalities.</text><text start="11592.35" dur="4.21">And those negative externalities can cascade,
second, third, fourth order effects.</text><text start="11596.56" dur="4.64">So there&apos;s a naive techno optimism that doesn&apos;t
pay enough attention to that.</text><text start="11601.2" dur="3.88">There&apos;s a naive techno pessimism that says,
well, I&apos;m aware of those negative externalities.</text><text start="11605.08" dur="1.55">I don&apos;t want those for our people.</text><text start="11606.63" dur="5.82">We think we can isolate our people from everybody
else and say we&apos;re going to not do that.</text><text start="11612.45" dur="6.739">But we&apos;re going to have decreased influence
over what everyone who is doing that has,</text><text start="11619.189" dur="1.0">right?</text><text start="11620.189" dur="4.271">Which is what then some of the AI labs argue
is there&apos;s an arms race.</text><text start="11624.46" dur="1.87">We can&apos;t stop the arms race on it.</text><text start="11626.33" dur="2.81">And so only being at the front of the arms
race can we steer it.</text><text start="11629.14" dur="4.75">I would argue that that is a naive version
of that particular thing, but nonetheless.</text><text start="11633.89" dur="11.19">So if we want to – and one way of reading
one of the problematic lessons of the elves</text><text start="11645.08" dur="8.04">in Tolkien is – and I&apos;m just making this
as like a toy model – is in some ways, they</text><text start="11653.12" dur="5.109">figured out how to have a nicer life than
the humans and dwarves and whatever else.</text><text start="11658.229" dur="4.531">They were able to do radical life extension
and figure out great GDP per capita where</text><text start="11662.76" dur="2.849">the poorest people were doing well.</text><text start="11665.609" dur="4.54">And they were so kind of – but they became
insular because they were so disenchanted</text><text start="11670.149" dur="3.04">with the world of men and elves and whatever
that they&apos;re like, fuck it.</text><text start="11673.189" dur="2.671">We&apos;re just going to kind of isolate and do
our own thing our own way.</text><text start="11675.86" dur="3.64">But it ends up being that you&apos;re still all
sharing Middle Earth.</text><text start="11679.5" dur="3.63">And the problem somewhere else can cascade
into catastrophic problems that end up messing</text><text start="11683.13" dur="1.0">up your world too.</text><text start="11684.13" dur="3.59">So you can&apos;t be isolationist forever in an
interconnected world.</text><text start="11687.72" dur="4.99">So they actually had to – they were kind
of obligated if we rewrote the story to take</text><text start="11692.71" dur="4.03">whatever they had learned and try to help
everybody else have it or have enough of it</text><text start="11696.74" dur="4.16">that you didn&apos;t get work dominance and stuff
like that.</text><text start="11700.9" dur="4.78">So basically arguing that a isolationist – we
see a problem.</text><text start="11705.68" dur="7.179">We&apos;re going to avoid that for ourselves – doesn&apos;t
work with planetary problems.</text><text start="11712.859" dur="4.891">And so I&apos;m not interested in the naive techno-negative
versions that say because we see a problem</text><text start="11717.75" dur="1.779">with tech, we&apos;re not going to do it.</text><text start="11719.529" dur="3.811">But we&apos;re also going to kind of lotus-eat
in the process and not engage with the fact</text><text start="11723.34" dur="3.389">that we actually care about what happens for
the world overall and we have to engage with</text><text start="11726.729" dur="4.681">how the world as a whole is doing that thing.</text><text start="11731.41" dur="4.84">Makes sense what I mean by the naive techno-pessimism.</text><text start="11736.25" dur="7.32">And it is that you do not get to do effective
isolationism on an interconnected planet that</text><text start="11743.57" dur="2.68">is hitting planetary boundaries with exponential
tech.</text><text start="11746.25" dur="5.0">Yeah, I guess what I&apos;m trying to express is
that even the Amish with what they&apos;re doing,</text><text start="11751.25" dur="4.93">it&apos;s not as simple as the meme-plex that&apos;s
exported by the Amish is the Amish meme-plex.</text><text start="11756.18" dur="1.82">There&apos;s something else that even influenced
them.</text><text start="11758.0" dur="5.06">Even yourself, you may be in a position that
saves Earth at least for now from a hugely</text><text start="11763.06" dur="1.11">catastrophic event.</text><text start="11764.17" dur="1.97">Same with Yodkowsky and same with some others.</text><text start="11766.14" dur="1.719">But what influenced you?</text><text start="11767.859" dur="3.861">There&apos;s some good in you, hopefully, that
was influenced by something else, by something</text><text start="11771.72" dur="2.469">else that&apos;s good, which also influenced the
Amish.</text><text start="11774.189" dur="1.861">Each person is corrupt in some way.</text><text start="11776.05" dur="4.26">So I&apos;m saying that there&apos;s something that&apos;s
like the unity of virtues that influences</text><text start="11780.31" dur="1.0">us.</text><text start="11781.31" dur="3.11">And that as long as we go back and we think
or constantly we&apos;re assessing ourselves and</text><text start="11784.42" dur="1.59">saying, is what I&apos;m doing good?</text><text start="11786.01" dur="4.21">Then these other meme-plexes that are being
thrown to us, yes, it&apos;s in a different context.</text><text start="11790.22" dur="4.5">Somehow it can orient and pick up the good.</text><text start="11794.72" dur="7.78">We&apos;re completely on the same page, which is
that that happens, does not always happen,</text><text start="11802.5" dur="3.85">and that that is an important thing to have
happen.</text><text start="11806.35" dur="8.839">But if that happened adequately or at the
– yeah, I will say adequately, then we wouldn&apos;t</text><text start="11815.189" dur="2.141">have extinct all the species that we have.</text><text start="11817.33" dur="2.86">We would not have turned as many old growth
forests into deserts.</text><text start="11820.19" dur="6.69">We would not have had as many genocides and
unnecessary wars and et cetera.</text><text start="11826.88" dur="6.53">So seeing the failure in where either someone&apos;s
definition of good is too narrow, get our</text><text start="11833.41" dur="4.3">god to win and fuck everybody else or grow
GDP and that&apos;ll take care of everything.</text><text start="11837.71" dur="4.6">We can well-intendedly pursue a definition
of good that&apos;s too narrow and externalize</text><text start="11842.31" dur="1.5">harm unintentionally.</text><text start="11843.81" dur="4.0">We can pursue a definition of good that we
really believe in that other people don&apos;t</text><text start="11847.81" dur="3.71">believe in and our answer is to win over them,
but it creates an arms race where now we&apos;re</text><text start="11851.52" dur="2.6">in competition over the thing.</text><text start="11854.12" dur="3.42">Or where there are people who are really not
pursuing the good of all even – they&apos;re</text><text start="11857.54" dur="2.79">not even trying to.</text><text start="11860.33" dur="5.1">Whether it&apos;s sociopathy from a head injury
or genes or trauma or whatever it is, they</text><text start="11865.43" dur="3.9">are pursuing a different thing.</text><text start="11869.33" dur="6.96">But they&apos;re good at acquiring power, and this
is actually a very important thing is that</text><text start="11876.29" dur="10.43">the psychologies that are – that want power
and are good at getting it and the psychologies</text><text start="11886.72" dur="4.019">that would be the best stewards of power for
the well-being of all are not the same set</text><text start="11890.739" dur="3.881">of psychological attributes.</text><text start="11894.62" dur="4.08">They&apos;re pretty close to inversely correlated.</text><text start="11898.7" dur="3.05">So those types of things have to be calculated
in this.</text><text start="11901.75" dur="5.62">So what you&apos;re saying right now, which is
great, you&apos;re saying it is that there is some</text><text start="11907.37" dur="2.83">odd impulse that is not only an impulse, right?</text><text start="11910.2" dur="3.5">That you&apos;re calling a universal virtue or
good or something.</text><text start="11913.7" dur="7.43">And you&apos;re saying that some people feel very
called by that and that that&apos;s important.</text><text start="11921.13" dur="4.55">I agree completely.</text><text start="11925.68" dur="8.93">Now, why is that historically and currently
not a strong enough binding is the important</text><text start="11934.61" dur="1.0">question.</text><text start="11935.61" dur="3.68">Why has that not been a strong enough binding
for all the species that are extinct and all</text><text start="11939.29" dur="4.029">the animals in factory farms and all the disruption,
et cetera?</text><text start="11943.319" dur="4.221">And then what would it take for it to become
a strong enough binding or the nature of the</text><text start="11947.54" dur="1.0">question here?</text><text start="11948.54" dur="6.6">And that&apos;s actually at the heart of the metacrisis
question is to have – like what is a system</text><text start="11955.14" dur="5.82">of ought that is actually commensurable with
the system of is and what is a way of having</text><text start="11960.96" dur="9.53">that actually sufficiently influence behavior
such that the catastrophic behaviors don&apos;t</text><text start="11970.49" dur="2.699">occur?</text><text start="11973.189" dur="6.181">And that the nature of the influence is not
so top-down that it becomes dystopic and that&apos;s</text><text start="11979.37" dur="5.691">something like is there either a – so one
way of thinking about this is I&apos;ve mentioned</text><text start="11985.061" dur="3.428">the term a couple of times superstructure,
social structure, infrastructure.</text><text start="11988.489" dur="7.301">That comes from Marvin Harris&apos; work on cultural
materialism basically saying every civilization</text><text start="11995.79" dur="5.42">you can think of in those ways, what is its
kind of mean plex, what is its social coordination</text><text start="12001.21" dur="3.02">strategies, and what is the physical tooling
set upon which it depends.</text><text start="12004.23" dur="4.43">And different social theorists will say which
of these they think is most fundamental.</text><text start="12008.66" dur="4.31">The value systems are ultimately what steer
behavior and determine the types of tech we</text><text start="12012.97" dur="2.809">build and the types of societies.</text><text start="12015.779" dur="1.901">Religious thinkers think there, right?</text><text start="12017.68" dur="2.49">Enlightenment thinkers think there.</text><text start="12020.17" dur="6.769">The social system actually, whatever you incentivize
financially is what&apos;s going to win because</text><text start="12026.939" dur="4.26">whether it&apos;s good or not, the people who do
that get the money, can incentivize more people,</text><text start="12031.199" dur="1.071">create the law, et cetera.</text><text start="12032.27" dur="4.66">So ultimately the most powerful thing is the
social coordination systems.</text><text start="12036.93" dur="4.309">And then the other schools of thought say
no, actually the thing that changes in time</text><text start="12041.239" dur="1.221">the most is the tech.</text><text start="12042.46" dur="4.8">And the tech influences the patterns of human
behavior, values, everything else.</text><text start="12047.26" dur="5.95">And so – and that&apos;s actually what Marvin
Harris roughly was saying was that the change</text><text start="12053.21" dur="4.55">in the tech plex ends up being the most influential
thing to the change because it does affect</text><text start="12057.76" dur="2.61">world views and it does affect social systems.</text><text start="12060.37" dur="3.76">In the way we already mentioned, the change
of the tech plex of the printing press affected</text><text start="12064.13" dur="6.84">both world views and social systems, so did
the plow, so did the internet, so is about</text><text start="12070.97" dur="2.25">to be AI.</text><text start="12073.22" dur="4.58">I would argue that these three are interacting
with each other in complex ways.</text><text start="12077.8" dur="6.55">They all inter-inform each other and what
we have to think about is what changes in</text><text start="12084.35" dur="4.62">all three of them simultaneously, factoring
all the feedback loops, produce a virtuous</text><text start="12088.97" dur="3.11">cycle that orients in a direction that isn&apos;t
catastrophes or dystopias.</text><text start="12092.08" dur="3.5">It&apos;s the right way of thinking about it.</text><text start="12095.58" dur="9.63">And ultimately, the direction actually has
to be the superstructure informing the social</text><text start="12105.21" dur="4.91">structure, informing or guide, bind, direct
the infrastructure.</text><text start="12110.12" dur="1.0">Sorry.</text><text start="12111.12" dur="1.96">Can you repeat that once more?</text><text start="12113.08" dur="1.37">Yeah.</text><text start="12114.45" dur="7.35">Right now, especially post-industrial revolution,
physical technology, infrastructure had way</text><text start="12121.8" dur="4.03">faster feedback loops on it than the others
did, right?</text><text start="12125.83" dur="9.08">And because of that, it started breaking the
previous industrial-era tech at massive scales</text><text start="12134.91" dur="4.3">with those externalities, whatever can&apos;t be
managed by agrarian era or hunter-gatherer</text><text start="12139.21" dur="3.8">era world views and political systems, right?</text><text start="12143.01" dur="7.1">So we ended up getting a whole new set of
political systems, both nation democratic,</text><text start="12150.11" dur="5.8">liberal democracy and capitalism and social
communism emerging as writing the industrial</text><text start="12155.91" dur="7.239">revolution and what should be the political
economy that governs that thing, right?</text><text start="12163.149" dur="6.661">But the feedback loops from tech and specifically
whether it&apos;s a nation-state caught in multipolar</text><text start="12169.81" dur="4.57">traps that&apos;s building the tech in a central
government communist type place or a market</text><text start="12174.38" dur="6.47">building it but that has perverse incentives
built into what its incentive structure is.</text><text start="12180.85" dur="6.139">That has influence on our social structures
and our cultures, superstructures.</text><text start="12186.989" dur="5.221">That – we could say the dominance of that
direction is one way of thinking about the</text><text start="12192.21" dur="2.58">driver of the meta-crisis.</text><text start="12194.79" dur="6.75">Now, the other direction, if we are to say,
no, these examples of the tech won&apos;t be built</text><text start="12201.54" dur="2.94">or we&apos;re not going to use the tech in these
ways, right?</text><text start="12204.48" dur="4.85">We&apos;re not going – yes, you can use a tech
that extracts some parts of rocks from other</text><text start="12209.33" dur="1.0">parts of rocks.</text><text start="12210.33" dur="2.159">It gives us metal we want but also gives a
lot of waste.</text><text start="12212.489" dur="4.841">No, you can&apos;t put all that waste in the waterway,
right?</text><text start="12217.33" dur="3.01">Or you can&apos;t put that pollution there or you
can&apos;t cut all the trees down in that area</text><text start="12220.34" dur="2.03">because we&apos;re calling it a national park,
right?</text><text start="12222.37" dur="2.93">That law or regulation that is not just a
tech thing.</text><text start="12225.3" dur="2.12">That&apos;s the social layer.</text><text start="12227.42" dur="6.109">So that layer has to bind the tech and guide
and direct it, say, these applications and</text><text start="12233.529" dur="1.261">not these ones.</text><text start="12234.79" dur="1.0">Yeah.</text><text start="12235.79" dur="1.0">Right?</text><text start="12236.79" dur="1.0">Yeah.</text><text start="12237.79" dur="4.62">So if the social system is not an instantiation,
if the social structure is not an instantiation</text><text start="12242.41" dur="3.59">of the superstructure, i.e. it&apos;s not an instantiation
of the will of the people, then it will be</text><text start="12246.0" dur="1.45">oppressive, right?</text><text start="12247.45" dur="7.4">That&apos;s why the idea of democracy emerged out
of the idea of the Enlightenment.</text><text start="12254.85" dur="6.0">Which was, this was a kind of governance that
only worked for a comprehensively educated</text><text start="12260.85" dur="5.9">– and you read all the founding documents
– the comprehensive education had to be</text><text start="12266.75" dur="1.0">is and ought, right?</text><text start="12267.75" dur="3.93">It said you must have a moral education as
well as a scientific education and you must</text><text start="12271.68" dur="3.1">be schooled in the science of governance.</text><text start="12274.78" dur="3.659">And only a people like that – going back
to what we said earlier – could check the</text><text start="12278.439" dur="1.131">government, right?</text><text start="12279.57" dur="6.1">Could both know the jurisprudence to instantiate
what is good law, could engage in dialectics</text><text start="12285.67" dur="5.29">to listen to other people&apos;s point of view
to come up with democratic answers.</text><text start="12290.96" dur="5.41">So it was the idea that there was a kind of
superstructure possibility which was some</text><text start="12296.37" dur="7.069">kind of Enlightenment or era values that could
make a type of social structure that could</text><text start="12303.439" dur="6.021">both utilize the tech and guide it but also
bind its destructive applications.</text><text start="12309.46" dur="4.54">So when you&apos;re saying isn&apos;t there some superstructure,
isn&apos;t there some sense of good that will make</text><text start="12314.0" dur="7.229">us bind our capacities, I would argue that
if the sense of good doesn&apos;t emerge from the</text><text start="12321.229" dur="4.651">collective understanding and will of the people
but is instantiated in government because</text><text start="12325.88" dur="4.12">we the technocrats know the right answer or
we the Enlightened know the right answer,</text><text start="12330.0" dur="1.0">that will be oppressive.</text><text start="12331.0" dur="2.64">And people are right to be concerned by it.</text><text start="12333.64" dur="3.921">But if the collective understanding is I want
what I want and I don&apos;t care what the effects</text><text start="12337.561" dur="4.939">are or fuck those guys over there or I&apos;m not
paying attention to externalities or whatever,</text><text start="12342.5" dur="6.31">then the collective will of the people is
too dumb to govern and misguided to govern</text><text start="12348.81" dur="2.75">exponential tech and will self-terminate.</text><text start="12351.56" dur="10.06">So you cannot have an uneducated, unevolved
set of values in a libertarian way guide exponential</text><text start="12361.62" dur="1.02">tech well.</text><text start="12362.64" dur="1.299">It has to be more considerate.</text><text start="12363.939" dur="2.011">It has to think through end order effects.</text><text start="12365.95" dur="5.17">But you also can&apos;t have a government that
says we are the Enlightened Ones and we figured</text><text start="12371.12" dur="4.39">it out and we&apos;re going to impose it on everyone
else without it being oppressive and tyrannical,</text><text start="12375.51" dur="5.75">which means nothing less than a kind of cultural
enlightenment is required long term.</text><text start="12381.26" dur="4.08">So the collective will of the people – now
this gets back to the alignment topic – is</text><text start="12385.34" dur="4.05">the will of the people aligned with itself
actually, right?</text><text start="12389.39" dur="5.28">Is what I want factoring the effects of what
I want, the end order effects, which means</text><text start="12394.67" dur="3.66">how other people will respond to that and
all that comes from it.</text><text start="12398.33" dur="7.21">Is what I want actually even aligned with
a viable future, right?</text><text start="12405.54" dur="4.97">And so that is when we&apos;re talking about getting
alignment right, alignment with my intention</text><text start="12410.51" dur="3.97">where my intention is that my country wins
at all costs where then China&apos;s like, well,</text><text start="12414.48" dur="3.86">fuck, we&apos;re going to do the same thing or
Russia, et cetera, so you get arms races.</text><text start="12418.34" dur="5.309">That intent or my intent is I want more stuff
and keep up with the Joneses and I&apos;m not paying</text><text start="12423.649" dur="2.691">attention to planetary boundaries.</text><text start="12426.34" dur="7.22">Those intents are not aligned with their own
fulfillment because the world self-terminates</text><text start="12433.56" dur="3.54">for too long in that process.</text><text start="12437.1" dur="6.9">And so with the power of exponential tech
and the cumulative effects of industrial tech,</text><text start="12444.0" dur="6.75">we do have to actually get ought that combine
the power of that is and it can&apos;t be imposed.</text><text start="12450.75" dur="5.569">It does have to be emergent, which does mean
something like that sense that you&apos;re saying</text><text start="12456.319" dur="4.101">has to become very universal and nurtured,
right?</text><text start="12460.42" dur="4.1">And then has to also instantiate itself in
reformation of systems of governance.</text><text start="12464.52" dur="2.02">I love what you said.</text><text start="12466.54" dur="1.99">Let&apos;s see if I can recapitulate it.</text><text start="12468.53" dur="1.34">There&apos;s tech at the bottom.</text><text start="12469.87" dur="2.31">There&apos;s a social structure here and then there&apos;s
culture.</text><text start="12472.18" dur="2.61">Okay, so these are people up here.</text><text start="12474.79" dur="1.52">There&apos;s people in all three.</text><text start="12476.31" dur="1.719">There&apos;s people&apos;s values up here.</text><text start="12478.029" dur="3.731">So values are up here and then there&apos;s the
social structure over here and then there&apos;s</text><text start="12481.76" dur="1.15">tech over here.</text><text start="12482.91" dur="4.1">Okay, so currently the tech influences the
way our society is structured, which also</text><text start="12487.01" dur="4.16">influences our values and a part of the meta
crisis is saying that that&apos;s upside down,</text><text start="12491.17" dur="3.859">but it shouldn&apos;t just be whatever values that
just get imposed onto the social structure</text><text start="12495.029" dur="3.601">onto the values have to somehow come from
someplace else.</text><text start="12498.63" dur="3.79">And then the mystics have their other they
have to be coherent with reality.</text><text start="12502.42" dur="1.0">Sure.</text><text start="12503.42" dur="4.97">So the spiritual people may call this something
akin to God and the enlightenment people may</text><text start="12508.39" dur="3.07">say, I don&apos;t know, maybe there&apos;s some evolutionary
will that comes out.</text><text start="12511.46" dur="3.8">And if we just close our eyes and hope for
the best somehow that emerges, whatever it&apos;s</text><text start="12515.26" dur="2.52">called, it&apos;s not entirely us.</text><text start="12517.78" dur="2.03">It&apos;s not entirely our conscious selves.</text><text start="12519.81" dur="4.25">The conscious self would be the more humanistic,
the enlightenment way of thinking about it</text><text start="12524.06" dur="2.18">is that we can impose our own values.</text><text start="12526.24" dur="2.05">Nietzsche had something similar to this.</text><text start="12528.29" dur="1.0">So I like this.</text><text start="12529.29" dur="1.0">I&apos;m in agreement with it.</text><text start="12530.29" dur="2.65">I think we&apos;ve just been using different terminology.</text><text start="12532.94" dur="15.999">You and I both know that when you, in many
ways, how to say this, an evolution of cultural</text><text start="12548.939" dur="5.71">worldview and values adequate to steward the
power of exponential technology in non-catastrophic</text><text start="12554.649" dur="7.0">or non-dystopic ways is happening in some
areas, but a regress is also happening in</text><text start="12561.649" dur="1.17">some areas.</text><text start="12562.819" dur="1.0">Right?</text><text start="12563.819" dur="5.931">There is increasing left-right polarization.</text><text start="12569.75" dur="3.43">I thought you were going to say there&apos;s a
regress happening like in demand.</text><text start="12573.18" dur="3.45">So for instance, we generally think like it
has to be Malthusian and the more that we</text><text start="12576.63" dur="3.22">use, the more the demand for that increases.</text><text start="12579.85" dur="4.66">And that&apos;s obviously removing some of the
more scarce objects like art and gold, which</text><text start="12584.51" dur="1.78">their value comes from scarcity.</text><text start="12586.29" dur="2.699">But there is like the largest health trend
right now is fasting.</text><text start="12588.989" dur="4.431">It&apos;s like we&apos;ve gotten so much food that we&apos;re
like, let&apos;s just not have any food.</text><text start="12593.42" dur="1.2">And then there&apos;s also recycling.</text><text start="12594.62" dur="3.369">Like, just imagine that we think about recycling
at all.</text><text start="12597.989" dur="3.782">So there is some recognition that, hey, look,
we&apos;re consuming too much.</text><text start="12601.771" dur="1.0">Let&apos;s cut back.</text><text start="12602.771" dur="2.609">And so it&apos;s not purely just an exponential
function.</text><text start="12605.38" dur="2.84">It is we take into account the rate of production.</text><text start="12608.22" dur="7.79">Well, so what we can see is that the percentage
of the total, let&apos;s go ahead and say US, but</text><text start="12616.01" dur="9.92">we could look at UAE or Nigeria or whatever,
various places, the percent of the US population</text><text start="12625.93" dur="7.61">that is regularly doing fasting is still a
relatively small percentage.</text><text start="12633.54" dur="4.13">And in the same way that – like it is true
that when there&apos;s a market race to the bottom</text><text start="12637.67" dur="5.7">that we saw in food, which Hostess and McDonald&apos;s,
et cetera, kind of won, which is how do we</text><text start="12643.37" dur="6.409">make the food more and more combinations of
salt, fat, and sugar and texture and palatability</text><text start="12649.779" dur="5.231">that maximize kind of stickiness and addiction,
which of course, if I have a fiduciary responsibility</text><text start="12655.01" dur="4.86">to shareholder maximization and the key to
that is to optimize the lifetime value of</text><text start="12659.87" dur="2.77">a customer, addiction is awesome.</text><text start="12662.64" dur="1.97">It&apos;s actually not awesome.</text><text start="12664.61" dur="5.6">It&apos;s legally obligate because of maximized
shareholder returns.</text><text start="12670.21" dur="4.25">So that created a race to the bottom where
rather than starvation being the leading cause</text><text start="12674.46" dur="4.21">of death, obesity was the leading cause of
health-related death in the West.</text><text start="12678.67" dur="1.0">Okay.</text><text start="12679.67" dur="4.84">Well, that bottom of the race to the bottom
also creates a race to the top for a different</text><text start="12684.51" dur="1.0">niche.</text><text start="12685.51" dur="5.05">So then Whole Foods becomes the fastest-growing
supermarket and biohacking and et cetera.</text><text start="12690.56" dur="1.24">So that&apos;s true.</text><text start="12691.8" dur="5.21">Did that affect the overall population demographics
regarding obesity very significantly?</text><text start="12697.01" dur="1.04">Not really.</text><text start="12698.05" dur="1.62">Did it stop the race to the bottom?</text><text start="12699.67" dur="1.0">No.</text><text start="12700.67" dur="4.0">It just added another little niche race, which
also then separated – which created more</text><text start="12704.67" dur="2.18">class system separation.</text><text start="12706.85" dur="4.81">So it&apos;s not that those effects don&apos;t happen.</text><text start="12711.66" dur="4.35">Are they happening at the scale and speed
necessary when we look at catastrophic risks?</text><text start="12716.01" dur="5.03">So of course I can also pay more for a post-consumer
recycled thing and there is more recycling</text><text start="12721.04" dur="1.0">happening.</text><text start="12722.04" dur="5.37">But there&apos;s also more net extraction of raw
resources and more net waste and pollution</text><text start="12727.41" dur="4.71">per year than there was the previous year
because the whole system is growing exponentially.</text><text start="12732.12" dur="4.051">So even if the recycling is growing, it&apos;s
actually not growing enough to even keep up</text><text start="12736.171" dur="2.319">with demand, right?</text><text start="12738.49" dur="4.77">So what I&apos;m saying is now let&apos;s come – bring
that back to the memetic space, which is where</text><text start="12743.26" dur="1.0">I was.</text><text start="12744.26" dur="5.33">There are both evolution of values where people
are wanting to think through catastrophic</text><text start="12749.59" dur="3.78">risk, existential risk, planetary well-being
of everybody long term.</text><text start="12753.37" dur="3.029">So that&apos;s good.</text><text start="12756.399" dur="6.571">But there is also cultural kind of regress
where people are getting narrower value systems</text><text start="12762.97" dur="4.3">with more antipathy towards other people they
share the planet with that have narrow value</text><text start="12767.27" dur="1.44">systems on the other side.</text><text start="12768.71" dur="5.47">And left-right polarization in the US is one
classic example.</text><text start="12774.18" dur="7.509">And so the point is cultural enlightenment
is not impossible, but it&apos;s also not a given,</text><text start="12781.689" dur="1.781">right?</text><text start="12783.47" dur="8.83">The kind of memetic shift, and this is obviously
I think a big part of why you do the public</text><text start="12792.3" dur="9.7">education memetic work that you do is because
of having a sensibility about is it possible</text><text start="12802.0" dur="6.82">to support the development and evolution of
worldviews and people in ways that can propagate</text><text start="12808.82" dur="1.82">and create good.</text><text start="12810.64" dur="2.21">Well, you&apos;re saying it much more benevolently.</text><text start="12812.85" dur="5.009">Honestly, it&apos;s just selfish that I&apos;m just
super, super, super curious about all of these</text><text start="12817.859" dur="1.0">topics.</text><text start="12818.859" dur="4.721">And by luck, some other people care to listen
and follow along.</text><text start="12823.58" dur="2.12">And I just get to elucidate myself.</text><text start="12825.7" dur="1.17">It&apos;s so fun.</text><text start="12826.87" dur="1.69">It bangs on every cylinder.</text><text start="12828.56" dur="1.58">And some other people seem to like it.</text><text start="12830.14" dur="2.03">I hope that what I&apos;m doing is something positive.</text><text start="12832.17" dur="2.51">I hope that it&apos;s not producing more harm.</text><text start="12834.68" dur="3.759">I&apos;m not even considering this is sent over
the internet and it&apos;s using up energy and</text><text start="12838.439" dur="1.0">–</text><text start="12839.439" dur="6.181">Okay, what you just said takes somewhere that
I wanted to go that&apos;s very interesting.</text><text start="12845.62" dur="8.32">So we&apos;re talking about alignment and is a
particular alignment – is a particular,</text><text start="12853.94" dur="3.86">say, human intention aligned with the collective
well-being of everybody or even their own</text><text start="12857.8" dur="2.52">long-term future?</text><text start="12860.32" dur="3.909">At the base of the alignment problem is that
we are not aligned with the other parts of</text><text start="12864.229" dur="1.401">our own self, right?</text><text start="12865.63" dur="5.9">So from a kind of Jungian parts conflict point
of view, motivation is complex because there&apos;s</text><text start="12871.53" dur="2.55">different parts of us that have different
motivations.</text><text start="12874.08" dur="5.39">And one way of thinking about psychological
health, the parts integration view, is the</text><text start="12879.47" dur="3.39">degree to which those different parts are
in good communication with each other and</text><text start="12882.86" dur="5.11">see synergistic strategies to meet their needs
that don&apos;t require that part of self&apos;s motivation</text><text start="12887.97" dur="1.47">harming another part of self.</text><text start="12889.44" dur="1.69">But they&apos;re actually doing synergetic stuff.</text><text start="12891.13" dur="3.22">So the whole of self pulls in the same direction.</text><text start="12894.35" dur="4.93">If you think of like the parts of self as
sled dogs, they can be pulling in opposite</text><text start="12899.28" dur="1.0">directions.</text><text start="12900.28" dur="1.0">You get nowhere.</text><text start="12901.28" dur="1.0">They&apos;re all choking themselves.</text><text start="12902.28" dur="3.14">So we can see psychological health and ill
health as how conflicted are the parts of</text><text start="12905.42" dur="2.45">our self versus how synergistic are they.</text><text start="12907.87" dur="1.0">Synergistic does not mean homogenous.</text><text start="12908.87" dur="1.46">It doesn&apos;t mean we just have one motive.</text><text start="12910.33" dur="3.09">It means that the various motives find synergistic
alignment rather than –</text><text start="12913.42" dur="2.51">Yeah, like our bodies are synergistic.</text><text start="12915.93" dur="2.309">Our heart is not the same as the liver.</text><text start="12918.239" dur="1.0">Exactly.</text><text start="12919.239" dur="4.551">Now, in your heart, it&apos;s not going to optimize
itself.</text><text start="12923.79" dur="2.84">It delivers long-term harm.</text><text start="12926.63" dur="5.101">Even though on its own, you could say it has
a different incentive, it is part of an interconnected</text><text start="12931.731" dur="2.029">system where that actually doesn&apos;t really
make sense.</text><text start="12933.76" dur="7.24">But a cancer cell will optimize itself or
its both itself, how much sugar it consumes</text><text start="12941.0" dur="2.729">and its reproduction cycles at the expense
of things around it.</text><text start="12943.729" dur="4.061">And in doing so, it actually is on a self-terminating
curve because it ends up killing the host</text><text start="12947.79" dur="2.04">and then killing itself.</text><text start="12949.83" dur="5.46">And so the cancer that does not want to bind
its consumption and regulation aligned with</text><text start="12955.29" dur="6.21">the pattern of the whole ends up actually
doing better in the short term, meaning consuming</text><text start="12961.5" dur="1.64">more and producing more.</text><text start="12963.14" dur="2.712">And then there&apos;s a maximum number of cancer
cells right before the body dies and they&apos;re</text><text start="12965.852" dur="1.208">all dead.</text><text start="12967.06" dur="5.62">So there is a – if something is inextricably
interconnected with the rest of reality like</text><text start="12972.68" dur="5.26">the heart and the liver or the various cells,
but it forgets that or doesn&apos;t understand</text><text start="12977.94" dur="3.89">that and optimizes itself at the expense of
the other things, it can be on what looks</text><text start="12981.83" dur="2.159">like a short-term winning path but it self-terminates.</text><text start="12983.989" dur="3.301">It would be an evolutionary cul-de-sac.</text><text start="12987.29" dur="3.859">And I would argue that the collective action
failures of humanity as a whole are pursuing</text><text start="12991.149" dur="2.521">an evolutionary cul-de-sac.</text><text start="12993.67" dur="4.24">And so one way of thinking about this, when
we say we aren&apos;t even that aligned with ourself,</text><text start="12997.91" dur="4.67">we think of motive.</text><text start="13002.58" dur="1.09">We like to think of leaders.</text><text start="13003.67" dur="4.48">What is Putin doing or what is Biden or Xi
doing in a particular thing?</text><text start="13008.15" dur="2.14">What is their motive?</text><text start="13010.29" dur="4.659">Or what is the founder of an AI lab motive?</text><text start="13014.949" dur="3.341">Motive will always be that each of the parts
of the self has a different motive.</text><text start="13018.29" dur="8.67">So typically like some unconscious part of
me still wants to get the amount of parental</text><text start="13026.96" dur="4.39">approval that I didn&apos;t get and then projecting
that on the world through some idea of success</text><text start="13031.35" dur="3.19">or to prove that it&apos;s enough or whatever.</text><text start="13034.54" dur="3.54">And some part of me is just directly motivated
by money.</text><text start="13038.08" dur="5.01">Some evolutionary part is motivated by maximizing
mate selection opportunities.</text><text start="13043.09" dur="2.66">Some part of me genuinely wants to do good.</text><text start="13045.75" dur="4.58">Some part of me wants intellectual congruence.</text><text start="13050.33" dur="9.47">And so there can absolutely be a burn it all
down part.</text><text start="13059.8" dur="4.149">And this is why shadow work is important,
which is look at and talk to all of these</text><text start="13063.949" dur="4.231">parts and see how to get them to move forward
together.</text><text start="13068.18" dur="3.969">This is basically governance at the level
of the self.</text><text start="13072.149" dur="4.941">So I don&apos;t know if you ever watched and this
might be because we&apos;re long even though there&apos;s</text><text start="13077.09" dur="7.04">so much left to discuss a decent place for
us to wrap up on alignment.</text><text start="13084.13" dur="5.26">I would say one of the better a number of
the theorists who you have referenced on the</text><text start="13089.39" dur="4.94">show would be good references for what I would
consider the deepest drivers of the meta crisis</text><text start="13094.33" dur="3.43">and also what the alignment considerations.</text><text start="13097.76" dur="2.05">If you think of like in Iain McGilchrist&apos;s
work</text><text start="13099.81" dur="1.69">with the Master and the Emissary, the right</text><text start="13101.5" dur="6.64">hemisphere is the master and the left hemisphere
is the master&apos;s emissary.</text><text start="13108.14" dur="6.83">In his 2009 opus, The Master and His Emissary,
Iain McGilchrist discusses the distinct functions</text><text start="13114.97" dur="2.67">of the brain&apos;s left and right hemispheres.</text><text start="13117.64" dur="2.19">Generally, there&apos;s plenty of &quot;pop science
woo&quot;</text><text start="13119.83" dur="2.47">around this concept, but then you can dispel</text><text start="13122.3" dur="3.37">by going even further to find the correctness
about it.</text><text start="13125.67" dur="5.649">The left hemisphere focus on processes such
as formal logic, symbol manipulation, and</text><text start="13131.319" dur="5.851">linear analysis, while the right hemisphere
is concerned with context awareness, the appreciation</text><text start="13137.17" dur="3.39">of unique instances, and topological understanding.</text><text start="13140.56" dur="5.52">Hey, maybe there&apos;s some stone duality between
them, but I haven&apos;t thought much about this.</text><text start="13146.08" dur="9.449">John Vervaeke&apos;s work, by the way, explores
the primacy of cognitive processes like relevance</text><text start="13155.529" dur="4.561">realization, aiming
to bridge the gap between analytic and intuitive</text><text start="13160.09" dur="1.0">thinking.</text><text start="13161.09" dur="4.32">Both McGilchrist and Vervaeke emphasize
the importance of integrating the strengths</text><text start="13165.41" dur="5.14">of each hemisphere or modes of cognition when
attempting to tackle intricate problems such</text><text start="13170.55" dur="2.86">as the risks of ever more powerful AI&apos;s.</text><text start="13173.41" dur="5.52">The argument is that AI systems primarily
operate using left hemisphere capabilities</text><text start="13178.93" dur="4.64">like pattern recognition, logical reasoning,
and general optimization problems.</text><text start="13183.57" dur="5.26">However, they fail to adequately consider
the subtleties of human values and ethical</text><text start="13188.83" dur="3.21">implications, which thus leads to unintended
consequences.</text><text start="13192.04" dur="5.26">To mitigate AI risks and prevent an arms race,
incorporating insights from both hemispheres</text><text start="13197.3" dur="2.81">and embracing context awareness is crucial.</text><text start="13200.11" dur="5.32">This requires interdisciplinary collaboration
between mathematicians, computer scientists,</text><text start="13205.43" dur="3.65">physicists, philosophers, neuroscientists,
and by the way, it&apos;s something that we&apos;re</text><text start="13209.08" dur="3.51">attempting in our humble manner on the Theories
of Everything channel.</text><text start="13212.59" dur="5.91">By exploring concepts in complex systems theory
and how it applies to our current unprecedented</text><text start="13218.5" dur="4.42">situation, we at least hope to understand
the interconnectedness of the factors at play</text><text start="13222.92" dur="1.01">in AI development.</text><text start="13223.93" dur="5.36">For instance, addressing AI risks can involve
analyzing multi-agent systems, considering</text><text start="13229.29" dur="4.689">network effects, and potential feedback loops,
which Iain McGilchrist would argue greatly</text><text start="13233.979" dur="9.861">benefits from your right hemisphere&apos;s contextual
understanding.</text><text start="13243.84" dur="2.99">We do not think ourselves into a new way of
living.</text><text start="13246.83" dur="2.88">We live ourselves into a new way of thinking.</text><text start="13249.71" dur="6.24">You could say, and I talked to Ian about this,
and I said, so would you say that the meta-crisis</text><text start="13255.95" dur="5.37">as I formulate it is the result of getting
the master and the emissary wrong, which is</text><text start="13261.32" dur="4.14">kind of getting the principal and agent between
those two different aspects of human wrong?</text><text start="13265.46" dur="1.2">And he goes, yes, exactly.</text><text start="13266.66" dur="5.02">That&apos;s kind of the whole key, that there is
a function that he&apos;s calling the master that</text><text start="13271.68" dur="7.35">perceives the kind of unmediated field of
interconnected wholeness, multimodally perceives</text><text start="13279.03" dur="1.89">and experiences it.</text><text start="13280.92" dur="6.16">And then there is this other set of networks,
capacities, or dispositions that perceive</text><text start="13287.08" dur="7.0">parts relative to parts, name them, do symbol
grounding, orient more in the domain of symbol,</text><text start="13294.08" dur="1.699">and can do relevance realization.</text><text start="13295.779" dur="4.04">What part is relevant to a particular goal
I have, and salience realization, what things</text><text start="13299.819" dur="4.641">should be relevant to some goal and I should
be tracking, and information compression,</text><text start="13304.46" dur="4.2">which are largely things that we think of
as human intelligence, which of course AI</text><text start="13308.66" dur="7.17">is taking that emissary part and turning it
into an external tool.</text><text start="13315.83" dur="4.5">Rather than that&apos;s the thing that makes tools
in us, now take that thing and make that as</text><text start="13320.33" dur="6.069">a tool, but also unbound by the master function,
the way he would call that.</text><text start="13326.399" dur="5.221">That&apos;s a very interesting way of thinking
about AI alignment and whatever, and that</text><text start="13331.62" dur="5.289">the master function that is perceiving the
unmediated, ground directly, not mediated</text><text start="13336.909" dur="5.401">by symbol field of interconnected wholeness,
that the other function that can do relevance</text><text start="13342.31" dur="6.379">realization, parts realization, salience realization,
info compression, basically utility function</text><text start="13348.689" dur="1.0">stuff.</text><text start="13349.689" dur="3.911">That that has to be in service of the field
of interconnected wholeness, if not, we&apos;ll</text><text start="13353.6" dur="3.02">upregulate some parts at the expense of other
ones, and the cumulative effect of that on</text><text start="13356.62" dur="4.109">an exponential curve will eventually bring
us to the metacrisis in self-terminate.</text><text start="13360.729" dur="5.92">I would say what McGilchrist was saying was
expanding on what Bohm said about the implicate</text><text start="13366.649" dur="1.0">order and wholeness.</text><text start="13367.649" dur="2.431">Bohm&apos;s theory of wholeness and the implicate
order states that there is something like</text><text start="13370.08" dur="2.28">life and mind unfolded in everything.</text><text start="13372.36" dur="9.31">A tremendous number of ways in which one can
see unfoldment in the mind.</text><text start="13381.67" dur="3.54">One can see the thoughts unfold, feelings
unfold, thoughts, because given by a feeling</text><text start="13385.21" dur="3.989">will give rise to a thought, thoughts unfold
feelings, the thought that the snake is dangerous</text><text start="13389.199" dur="4.681">will unfold the feeling of danger, which will
then unfold when you see a snake, right?</text><text start="13393.88" dur="9.25">Bohm was looking at the orientation of a mind
that mostly thinks in words, of Western mind,</text><text start="13403.13" dur="6.54">you know, in particular, to break reality
into parts and make sure that our word, the</text><text start="13409.67" dur="3.68">symbol that would correspond with the ground
there, corresponded with the things that it</text><text start="13413.35" dur="3.33">was supposed to and not the other things,
so try to draw rigorous boundaries to, you</text><text start="13416.68" dur="5.299">know, divide everything up, led to us fundamentally
relating to everything as parts first.</text><text start="13421.979" dur="6.201">So how then, now we have this human mind that&apos;s,
you know, paleolithic and it&apos;s now put in</text><text start="13428.18" dur="5.49">a world where we have a different technology
that is relying on reward circuits that maybe</text><text start="13433.67" dur="2.33">are not as virtuous as we would like.</text><text start="13436.0" dur="1.87">Is that where we are now in this conversation?</text><text start="13437.87" dur="6.77">And when Bohm and Krishnamurti did their dialogues,
which I don&apos;t know if you&apos;ve watched those</text><text start="13444.64" dur="7.4">or some of my favorite dialogues in history,
Bohm was basically answering what is the cause</text><text start="13452.04" dur="3.199">of all the problems, what&apos;s the cause of the
meta crisis, he didn&apos;t call it that at the</text><text start="13455.239" dur="1.0">time.</text><text start="13456.239" dur="4.42">And he basically said a kind of fragmented
or fractured consciousness that sees everything</text><text start="13460.659" dur="5.301">as parts where you can upregulate some parts
relative to other ones without thinking about</text><text start="13465.96" dur="3.72">the effect of that on the whole, right?</text><text start="13469.68" dur="3.73">And that obviously comes from Einstein being
one of his teachers, where Einstein said it&apos;s</text><text start="13473.41" dur="3.51">an optical delusion of consciousness to believe
there are separate things, there is in reality</text><text start="13476.92" dur="2.04">one thing we call universe.</text><text start="13478.96" dur="3.97">Regarding the theme of consciousness, it&apos;s
prudent to give an explication here, as often</text><text start="13482.93" dur="4.49">at least I found that mysteries arise because
we&apos;re calling different phenomenon by the</text><text start="13487.42" dur="1.0">same term.</text><text start="13488.42" dur="4.8">And this applies to consciousness, which doesn&apos;t
refer to just one aspect, but rather several</text><text start="13493.22" dur="1.17">that can be delineated.</text><text start="13494.39" dur="4.92">To further differentiate, I spoke to Professor
Greg Henricks on this very topic.</text><text start="13499.31" dur="4.669">I&apos;m attempting to delineate a few concepts,
that is adjectival consciousness, adverbial</text><text start="13503.979" dur="4.25">consciousness and phenomenal consciousness,
which I believe is the same as P-consciousness.</text><text start="13508.229" dur="2.981">But if that&apos;s not the same, then that&apos;s four
different concepts.</text><text start="13511.21" dur="1.1">So what are they?</text><text start="13512.31" dur="4.86">Can you give the audience and myself an explanation
as to when are some satisfied but not others</text><text start="13517.17" dur="1.269">so that we can delineate?</text><text start="13518.439" dur="1.0">Totally.</text><text start="13519.439" dur="1.0">Yep.</text><text start="13520.439" dur="1.0">Yeah.</text><text start="13521.439" dur="4.771">And actually, adjectival, adverbial are going
to, when we use P, when John and I certainly</text><text start="13526.21" dur="6.67">use P-consciousness, phenomenological consciousness
is reflecting on adjectival adverbial consciousness.</text><text start="13532.88" dur="2.25">And John refers to John Vervaeke.</text><text start="13535.13" dur="1.17">John Vervaeke, yeah.</text><text start="13536.3" dur="5.139">Because we then did a whole series, Untangling
the World Knot, to make sure that our systems</text><text start="13541.439" dur="5.5">were in line, both in terms of our definitional
systems and our causal explanatory framework.</text><text start="13546.939" dur="6.391">So we did a 10-part series on, just the two
of us, on Untangling the World Knot, of consciousness.</text><text start="13553.33" dur="2.06">And then we did one on the self.</text><text start="13555.39" dur="1.08">Then we did one on well-being.</text><text start="13556.47" dur="3.99">And we also did one on development and transformation
with Zach Stein.</text><text start="13560.46" dur="4.529">So all of we, our systems, I think, are now
completely synced up, at least in relation</text><text start="13564.989" dur="3.191">to the language structures that we have.</text><text start="13568.18" dur="4.54">And so I can tell you what we would mean by
adjectival adverbial consciousness, which</text><text start="13572.72" dur="4.17">then sort of is what most people mean by phenomenological
consciousness.</text><text start="13576.89" dur="1.0">Okay.</text><text start="13577.89" dur="3.02">So if I understand correctly, one has to do
with degrees?</text><text start="13580.91" dur="1.0">Mm-hmm.</text><text start="13581.91" dur="2.869">And then another has to do with a here-ness
and a now-ness?</text><text start="13584.779" dur="1.0">Yeah, exactly.</text><text start="13585.779" dur="4.591">So actually, there&apos;s, I like to, so I would
encourage us to say, let&apos;s define conscious,</text><text start="13590.37" dur="5.27">there are three different kinds of definitions
of consciousness, okay, that I think of.</text><text start="13595.64" dur="4.16">The first definition of consciousness is functional
awareness and responsivity.</text><text start="13599.8" dur="1.0">Okay.</text><text start="13600.8" dur="3.359">This is something that shows awareness and
can respond with control.</text><text start="13604.159" dur="5.071">And at the broadest definition, then even
things like bacteria can show a kind of functional</text><text start="13609.23" dur="1.959">awareness and responsivity.</text><text start="13611.189" dur="1.0">Okay.</text><text start="13612.189" dur="2.431">That&apos;s the behavioral responsiveness.</text><text start="13614.62" dur="1.75">And when you say, hey, is that guy conscious?</text><text start="13616.37" dur="2.65">What you mean is he&apos;s not responding at all.</text><text start="13619.02" dur="1.0">Okay.</text><text start="13620.02" dur="1.53">He&apos;s not showing any functional awareness</text><text start="13621.55" dur="1.0">and responsivity.</text><text start="13622.55" dur="2.649">He&apos;s either knocked out or blacked out or
asleep.</text><text start="13625.199" dur="1.0">Okay.</text><text start="13626.199" dur="1.661">So when you say consciousness in that way,</text><text start="13627.86" dur="2.499">that&apos;s functional awareness and responsivity,</text><text start="13630.359" dur="2.931">which you can see from the outside and you
see in the way</text><text start="13633.29" dur="2.92">in which the agent&apos;s operating on the arena,</text><text start="13636.21" dur="2.699">because they&apos;re showing functional awareness</text><text start="13638.909" dur="1.0">and responsivity.</text><text start="13639.909" dur="1.0">Okay.</text><text start="13640.909" dur="2.381">The second meaning of consciousness is subjective</text><text start="13643.29" dur="3.1">conscious experience of being in the world.</text><text start="13646.39" dur="3.06">The first person experience of being,</text><text start="13649.45" dur="1.88">and this is where the hard problem of consciousness</text><text start="13651.33" dur="1.54">comes online.</text><text start="13652.87" dur="2.289">And that&apos;s what most people mean by P-consciousness</text><text start="13655.159" dur="1.62">or phenomenological consciousness.</text><text start="13656.779" dur="2.671">It&apos;s a subjective experience of being,</text><text start="13659.45" dur="5.11">which is only available from the inside out.</text><text start="13664.56" dur="4.02">So that&apos;s, and then the final one is a self-conscious
access.</text><text start="13668.58" dur="1.0">Okay.</text><text start="13669.58" dur="4.069">So that now I can be, know that I have had
an experience</text><text start="13673.649" dur="3.851">retrieve it, and then in its highest form
report on it.</text><text start="13677.5" dur="2.83">So self-consciousness, then is the capacity</text><text start="13680.33" dur="4.0">to recursively access one&apos;s phenomenological
thing</text><text start="13684.33" dur="1.949">and an explicit self-consciousness,</text><text start="13686.279" dur="2.96">which is what humans have and other animals
generally don&apos;t,</text><text start="13689.239" dur="1.7">is this capacity to say, Curt,</text><text start="13690.939" dur="2.29">I am thinking about your question.</text><text start="13693.229" dur="1.8">I&apos;m experiencing your face,</text><text start="13695.029" dur="1.891">and this is my narrative in relation.</text><text start="13696.92" dur="2.79">So that&apos;s explicit self-conscious awareness.</text><text start="13699.71" dur="1.0">Uh-huh.</text><text start="13700.71" dur="1.0">Just a moment.</text><text start="13701.71" dur="2.13">You said access, is that the same as access
consciousness</text><text start="13703.84" dur="1.0">or is that&apos;s different?</text><text start="13704.84" dur="2.12">No, that&apos;s Ned Block&apos;s access consciousness,</text><text start="13706.96" dur="2.57">which basically there&apos;s the, do you have the
experience?</text><text start="13709.53" dur="2.98">And then is there a memory access loop that
stores it</text><text start="13712.51" dur="1.49">and then can use it?</text><text start="13714.0" dur="2.42">So if I can gain access to it,</text><text start="13716.42" dur="3.479">that&apos;s sort of access consciousness as relates
to that.</text><text start="13719.899" dur="1.441">I want to make sure that I understand this.</text><text start="13721.34" dur="1.04">There&apos;s a door behind me.</text><text start="13722.38" dur="3.819">If I go and I access, is what I&apos;m accessing
qualia?</text><text start="13726.199" dur="1.951">And is it the action of accessing</text><text start="13728.15" dur="1.31">that&apos;s called access consciousness?</text><text start="13729.46" dur="2.25">Like the manipulation of data or is-</text><text start="13731.71" dur="2.319">Right, it&apos;s the, well, it&apos;s basically,</text><text start="13734.029" dur="1.061">so you have awareness</text><text start="13735.09" dur="1.58">and then you have memory of the awareness</text><text start="13736.67" dur="1.95">that you know that some aspects of it</text><text start="13738.62" dur="1.76">knows that you were aware.</text><text start="13740.38" dur="2.62">So it&apos;s like, so you can imagine awareness</text><text start="13743.0" dur="2.67">without really, like one way of differentiating
it</text><text start="13745.67" dur="3.27">would be sort of, we have what a sensory perceptual
awareness</text><text start="13748.94" dur="3.06">that lasts say three tenths of a second to
three seconds.</text><text start="13752.0" dur="2.03">It&apos;s like a flash, okay?</text><text start="13754.03" dur="1.53">Then you have working memory,</text><text start="13755.56" dur="3.28">which extends it across time and puts it on
a loop.</text><text start="13758.84" dur="2.53">That loop is what allows you to then gain
access to it</text><text start="13761.37" dur="1.06">and manipulate it.</text><text start="13762.43" dur="3.73">So working memory can be thought of then as
a part,</text><text start="13766.16" dur="4.73">as the whiteboard that allows continuous access</text><text start="13770.89" dur="1.0">to the flash.</text><text start="13771.89" dur="1.86">So there&apos;s a flash</text><text start="13773.75" dur="2.62">and then there&apos;s the extension and manipulation
of the flash</text><text start="13776.37" dur="3.14">which you then need access to, okay?</text><text start="13779.51" dur="3.0">The basic layers of this, what John and I
argue</text><text start="13782.51" dur="2.16">is that out of the body comes what we call</text><text start="13784.67" dur="1.45">valence qualia.</text><text start="13786.12" dur="4.08">Valence qualia basically orients and gives
value to</text><text start="13790.2" dur="2.59">and can be thought of as in like pleasure
and pain</text><text start="13792.79" dur="2.02">in the body, okay?</text><text start="13794.81" dur="2.95">And it yokes a sensory state with an affective
state</text><text start="13797.76" dur="1.54">and points you in a direction.</text><text start="13799.3" dur="1.929">Or yoke means?</text><text start="13801.229" dur="3.031">Tie together, like to yoke stuff together.</text><text start="13804.26" dur="3.2">So this is the sort of the earliest form of
consciousness</text><text start="13807.46" dur="3.29">is probably of kind of valence consciousness,
okay?</text><text start="13810.75" dur="1.76">That basically pulls you, you know,</text><text start="13812.51" dur="2.969">oh, it feels good, feels bad kind of deal.</text><text start="13815.479" dur="2.581">Gets me active, gets me passive.</text><text start="13818.06" dur="1.179">But it&apos;s this sort of like, hmm,</text><text start="13819.239" dur="3.681">this kind of felt sense of the body, okay?</text><text start="13822.92" dur="1.96">That&apos;s the argument from John and I&apos;s position</text><text start="13824.88" dur="2.37">is that that probably is the base</text><text start="13827.25" dur="2.71">of your subjective conscious experience</text><text start="13829.96" dur="5.54">or the base of your phenomenological experience,
okay?</text><text start="13835.5" dur="2.26">Then what happens, and that would be maybe
present in,</text><text start="13837.76" dur="2.74">you know, in say reptiles, fish,</text><text start="13840.5" dur="3.42">maybe down into insects at some level, okay?</text><text start="13843.92" dur="2.62">Then the argument would be in birds and mammals</text><text start="13846.54" dur="1.79">and maybe lower, we don&apos;t really know,</text><text start="13848.33" dur="2.05">but there&apos;s good reason to believe in birds
and mammals.</text><text start="13850.38" dur="3.03">You get a global workspace.</text><text start="13853.41" dur="2.36">The global workspace is when it extends</text><text start="13855.77" dur="3.94">from these sensory flashes into a workspace</text><text start="13859.71" dur="2.92">where you have access and recursive looping
on it.</text><text start="13862.63" dur="5.71">And it&apos;s the framing of that is the adverbial
consciousness</text><text start="13868.34" dur="1.66">is the framing and extension of that.</text><text start="13870.0" dur="2.17">The here-ness, now-ness, and togetherness</text><text start="13872.17" dur="3.14">that indexes the thing, pulls it together.</text><text start="13875.31" dur="3.62">That&apos;s what John calls adverbial consciousness,
okay?</text><text start="13878.93" dur="1.009">Okay.</text><text start="13879.939" dur="1.571">And then it&apos;s what&apos;s on the screen</text><text start="13881.51" dur="1.551">of that adverbial consciousness</text><text start="13883.061" dur="2.758">is what John calls adjectival consciousness.</text><text start="13885.819" dur="1.931">So it&apos;s like, it&apos;s the screen of attention</text><text start="13887.75" dur="4.47">that orients and indexes, that&apos;s adverbial,
okay?</text><text start="13892.22" dur="1.92">And then what is actually the properties</text><text start="13894.14" dur="2.24">that you experience, that&apos;s adjectival.</text><text start="13896.38" dur="2.09">First, I came in with three questions</text><text start="13898.47" dur="2.15">and I have so many more.</text><text start="13900.62" dur="3.24">Okay, this valence, is it purely pain and
pleasure</text><text start="13903.86" dur="1.4">or is there something else?</text><text start="13905.26" dur="2.349">Are the third, fourth elements?</text><text start="13907.609" dur="2.62">There&apos;s certainly pleasure, pain, active,
passive</text><text start="13910.229" dur="4.241">to orient and like and want, basically.</text><text start="13914.47" dur="3.53">But basically you have what&apos;s called</text><text start="13918.0" dur="2.29">the circumplex model of affect,</text><text start="13920.29" dur="3.819">which basically is the core energizing structure</text><text start="13924.109" dur="1.411">of your motivational, emotional,</text><text start="13925.52" dur="2.56">and its broadest frame is two poles.</text><text start="13928.08" dur="2.52">One is active, passive, okay?</text><text start="13930.6" dur="3.45">It&apos;s like sort of spend energy or conserve
energy.</text><text start="13934.05" dur="2.42">And the other is pleasure.</text><text start="13936.47" dur="1.51">That is either something that you want</text><text start="13937.98" dur="1.45">or something you like, or pain.</text><text start="13939.43" dur="1.46">That&apos;s something that you don&apos;t want</text><text start="13940.89" dur="2.17">or don&apos;t like at its basic.</text><text start="13943.06" dur="2.93">So that&apos;s the, so the valence is what we sort
of focused on</text><text start="13945.99" dur="2.76">in relationship to just the ground of it.</text><text start="13948.75" dur="2.26">But there are definitely at least these two
poles</text><text start="13951.01" dur="4.07">of active, passive and pleasure pain at a
minimum.</text><text start="13955.08" dur="3.199">Can you make an analogy with this computer
screen right now?</text><text start="13958.279" dur="3.061">So the computer screen is somehow the workspace</text><text start="13961.34" dur="2.349">and then the pixels and the fact that they&apos;re
bound together</text><text start="13963.689" dur="3.351">is adjectival or the intensity is adverbial</text><text start="13967.04" dur="1.0">or the other way around.</text><text start="13968.04" dur="1.811">Like, can you just spell out an analogy?</text><text start="13969.851" dur="1.0">Absolutely, right.</text><text start="13970.851" dur="3.008">So the screening, the framing of the screen,</text><text start="13973.859" dur="1.641">which Bren basically says, okay,</text><text start="13975.5" dur="1.899">this is the frame and the relevance</text><text start="13977.399" dur="2.79">and the here-ness now-ness of what is gonna
be brought.</text><text start="13980.189" dur="1.51">That is adverbial.</text><text start="13981.699" dur="1.45">That&apos;s what John called adverbial consciousness.</text><text start="13983.149" dur="2.891">And he has a whole argument as to why,</text><text start="13986.04" dur="1.88">especially through what&apos;s called the pure
consciousness</text><text start="13987.92" dur="1.83">event that&apos;s achieved in meditation</text><text start="13989.75" dur="2.75">and several other things, there&apos;s a differentiation</text><text start="13992.5" dur="3.37">but what he calls the indexing function of
consciousness,</text><text start="13995.87" dur="1.54">which basically is the framing.</text><text start="13997.41" dur="3.42">You bring, you index, you say that thing</text><text start="14000.83" dur="3.06">without specifying what the thing is.</text><text start="14003.89" dur="3.139">Okay, it&apos;s the that thing that brings it.</text><text start="14007.029" dur="3.541">And then you then discriminate on the properties.</text><text start="14010.57" dur="1.22">That&apos;s the different pixel shapes</text><text start="14011.79" dur="2.279">that give rise to a form, that give rise</text><text start="14014.069" dur="4.87">to an experience quality, and that&apos;s the adjectival
quality.</text><text start="14018.939" dur="2.151">And these are, both of these are John&apos;s terms</text><text start="14021.09" dur="4.019">but I&apos;ve incorporated them in my work when
I use them.</text><text start="14025.109" dur="2.241">Okay, another analogy now to abandon the screen.</text><text start="14027.35" dur="2.73">It&apos;d be like if looking is one aspect</text><text start="14030.08" dur="2.29">and then what you&apos;re looking at is another,</text><text start="14032.37" dur="2.13">what you&apos;re looking at is akin to the qualia</text><text start="14034.5" dur="1.29">in a pure consciousness event.</text><text start="14035.79" dur="3.07">The at may not be there, but you&apos;re looking.</text><text start="14038.86" dur="1.14">Exactly.</text><text start="14040.0" dur="1.0">It&apos;s the framing.</text><text start="14041.0" dur="1.42">Exactly, index framing.</text><text start="14042.42" dur="2.64">That&apos;s why John, when he takes off his glasses,
okay,</text><text start="14045.06" dur="4.31">the glasses are much more like the adverbial
framing.</text><text start="14049.37" dur="4.43">They pull and organize, okay, and it&apos;s the
looking, okay,</text><text start="14053.8" dur="1.221">the pointing, the indexing.</text><text start="14055.021" dur="4.559">In fact, he actually, he uses work in cognitive
science,</text><text start="14059.58" dur="3.529">okay, where you can track, like if I give
you</text><text start="14063.109" dur="2.491">like four different things to track on a screen,
okay,</text><text start="14065.6" dur="2.93">and they&apos;re changing colors and changing shapes,</text><text start="14068.53" dur="1.66">four different things you can track.</text><text start="14070.19" dur="3.709">Five, six, seven, you stop losing ability
to track.</text><text start="14073.899" dur="2.871">However, what you lose first is the ability</text><text start="14076.77" dur="1.929">to track the specifics.</text><text start="14078.699" dur="1.631">You can tell where something is,</text><text start="14080.33" dur="2.73">but you can&apos;t tell what it is actually.</text><text start="14083.06" dur="1.32">So in other words, it&apos;s sort of like you&apos;re
trying</text><text start="14084.38" dur="1.99">to track everything, but it changes like from
red</text><text start="14086.37" dur="1.82">to blue to green, you&apos;re much better.</text><text start="14088.19" dur="1.79">Like I think it&apos;s over there.</text><text start="14089.98" dur="3.27">It indexes, but I can&apos;t tell you whether it&apos;s
an A, a B,</text><text start="14093.25" dur="2.92">a red or a green, I can&apos;t tell you the specificity.</text><text start="14096.17" dur="2.5">So in other words, I&apos;m tracking the entity,
okay,</text><text start="14098.67" dur="2.309">that&apos;s the index, and that&apos;s different</text><text start="14100.979" dur="3.09">than specifying the nature of the form.</text><text start="14104.069" dur="1.991">And indeed we have lots of different systems</text><text start="14106.06" dur="3.28">that track the, like what is the thing</text><text start="14109.34" dur="1.68">versus how is it moving?</text><text start="14111.02" dur="2.6">The how is it moving is more of an index structure.</text><text start="14113.62" dur="3.64">But if we think of this kind of Bohmian wholeness,</text><text start="14117.26" dur="2.37">we could say that the metacrisis is a function</text><text start="14119.63" dur="4.43">of missing Bohmian wholeness</text><text start="14124.06" dur="4.62">and doing optimization on parts.</text><text start="14128.68" dur="3.69">And so I can optimize self at the expense
of other,</text><text start="14132.37" dur="4.23">but of course that then leads to others figuring
out</text><text start="14136.6" dur="2.65">how to do that and needing to for protection.</text><text start="14139.25" dur="2.25">And now arms races of everybody doing that.</text><text start="14141.5" dur="1.199">The whole externality said,</text><text start="14142.699" dur="1.83">I can optimize self at expense of other,</text><text start="14144.529" dur="3.021">I can optimize in group at the expense of
out group.</text><text start="14147.55" dur="2.49">I can optimize one metric at the expense of
other metrics.</text><text start="14150.04" dur="2.57">I can optimize one species at the expense
of other species.</text><text start="14152.61" dur="3.5">I can optimize my current at the expense of
our future,</text><text start="14156.11" dur="2.15">all the way down to one part of self</text><text start="14158.26" dur="2.54">relative to the other parts of self.</text><text start="14160.8" dur="5.47">So the wholeness of all the parts of self
in synergy</text><text start="14166.27" dur="3.23">and all of the people, species, et cetera,</text><text start="14169.5" dur="1.5">and how to consider the whole,</text><text start="14171.0" dur="3.52">how to consider the effects on the whole,</text><text start="14174.52" dur="2.57">maybe that was something that other animals</text><text start="14177.09" dur="1.0">did not have to do.</text><text start="14178.09" dur="1.75">Maybe it was something that even earlier humans</text><text start="14179.84" dur="1.84">didn&apos;t have to do because they couldn&apos;t affect
the whole</text><text start="14181.68" dur="1.63">all that much.</text><text start="14183.31" dur="2.339">When we have the ability to affect the whole
this much,</text><text start="14185.649" dur="2.821">this quickly because of tech, right?</text><text start="14188.47" dur="2.5">Because, and particularly because of exponentially</text><text start="14190.97" dur="3.66">powerful tech, whatever ways we are either</text><text start="14194.63" dur="1.87">consciously saying this is a part of the whole,</text><text start="14196.5" dur="4.14">I don&apos;t care about, or I&apos;m happy to destroy
conflict theory,</text><text start="14200.64" dur="2.53">or this is a part of the whole, I&apos;m just not
even factoring.</text><text start="14203.17" dur="1.18">Maybe I don&apos;t even know the factor</text><text start="14204.35" dur="1.42">that&apos;s in the unknown, unknown set,</text><text start="14205.77" dur="4.1">but I&apos;m still gonna affect it by the thing
I do.</text><text start="14209.87" dur="4.92">So what is outside of my care or my consideration,
right?</text><text start="14214.79" dur="2.98">Conflict theory and mistake theory,</text><text start="14217.77" dur="2.339">with exponential tech gets harmed,</text><text start="14220.109" dur="3.09">produces its own counter responses and cascade
effects.</text><text start="14223.199" dur="3.21">The net effect of that is termination.</text><text start="14226.409" dur="1.421">With this much power,</text><text start="14227.83" dur="2.79">what does it take to steward the power adequately</text><text start="14230.62" dur="5.48">is to think about the total cascading effects
of the choices</text><text start="14236.1" dur="2.389">and all agents doing that</text><text start="14238.489" dur="3.151">and say, how do we coordinate all agents doing
that</text><text start="14241.64" dur="4.58">in a way that has the integrity of the whole
upregulated</text><text start="14246.22" dur="2.05">rather than downregulated.</text><text start="14248.27" dur="1.509">And so I would say,</text><text start="14249.779" dur="3.901">Bohmian wholeness is a good framework for
alignment,</text><text start="14253.68" dur="4.09">not alignment of an AI with human intent,</text><text start="14257.77" dur="4.429">but aligned with the interconnected complexity
of reality.</text><text start="14262.199" dur="2.99">May I inquire, how did you attain</text><text start="14265.189" dur="2.111">such a vast array of knowledge?</text><text start="14267.3" dur="1.7">What&apos;s your educational background?</text><text start="14269.0" dur="2.5">What does your routine look like for studying?</text><text start="14271.5" dur="2.58">Is it a daily one where you read a certain
type of book</text><text start="14274.08" dur="2.27">and you vary the field week by week?</text><text start="14276.35" dur="1.0">What is the regimen?</text><text start="14277.35" dur="2.33">How did you get the way that you are?</text><text start="14279.68" dur="5.349">I think my learning process,</text><text start="14285.029" dur="1.991">probably in some ways similar to yours,</text><text start="14287.02" dur="3.03">you said very fascinated and curious.</text><text start="14290.05" dur="1.639">And I mean, you did it.</text><text start="14291.689" dur="3.411">You did something better than I did,</text><text start="14295.1" dur="2.78">which is you pick the topics you&apos;re most interested</text><text start="14297.88" dur="1.421">in and found the top experts in the world</text><text start="14299.301" dur="2.329">and got them to basically tutor you for free.</text><text start="14301.63" dur="3.2">In terms of like an aristocratic tutoring
system,</text><text start="14304.83" dur="2.579">you did a pretty awesome thing there.</text><text start="14307.409" dur="3.541">There were a few cases where I was fortunate
enough</text><text start="14310.95" dur="1.72">to be able to do that.</text><text start="14312.67" dur="2.63">Other times I just had to work with the output
of their work.</text><text start="14315.3" dur="1.0">But I think for me,</text><text start="14316.3" dur="3.5">it was a combo of just innate curiosity,</text><text start="14319.8" dur="2.03">independent of any use application.</text><text start="14321.83" dur="4.27">And just, I think it&apos;s natural</text><text start="14326.1" dur="3.25">when you love something to want to understand
it more.</text><text start="14329.35" dur="2.07">And so for me, the impulse to understand the
world</text><text start="14331.42" dur="3.33">is kind of a sacred impulse.</text><text start="14334.75" dur="1.95">But then also the desire to serve the world</text><text start="14336.7" dur="1.289">requires understanding it well enough</text><text start="14337.989" dur="1.991">to know how the fuck to maybe do that.</text><text start="14339.98" dur="2.4">So there is both a very practical</text><text start="14342.38" dur="3.74">and very not practical impulse on learning</text><text start="14346.12" dur="3.05">that happened to fortunately converge.</text><text start="14349.17" dur="2.0">And how is it that you&apos;re able to articulate
the views</text><text start="14351.17" dur="1.0">that you have?</text><text start="14352.17" dur="1.0">How do you develop them?</text><text start="14353.17" dur="1.1">Do you start writing?</text><text start="14354.27" dur="2.4">Do you do it in conversation with people?</text><text start="14356.67" dur="1.87">Do you say some sentiment you realized,</text><text start="14358.54" dur="1.25">you know what, that was actually pretty great.</text><text start="14359.79" dur="2.569">I didn&apos;t even realize I thought that until
I had said it.</text><text start="14362.359" dur="2.391">Now let me write it down so I can remember
it.</text><text start="14364.75" dur="4.71">You know, I have hypotheses about how people
develop</text><text start="14369.46" dur="2.33">the ability to communicate well,</text><text start="14371.79" dur="2.88">but my hypotheses about that in my own process</text><text start="14374.67" dur="3.12">are probably different.</text><text start="14377.79" dur="3.48">I think my own process is I was homeschooled</text><text start="14381.27" dur="2.58">and I was homeschooled in a way that&apos;s maybe
a little bit</text><text start="14383.85" dur="1.81">like what people call unschooling now,</text><text start="14385.66" dur="2.56">but I had no curriculum at all.</text><text start="14388.22" dur="3.82">But my parents just had the kind of,</text><text start="14392.04" dur="2.72">they had never studied educational theory.</text><text start="14394.76" dur="1.41">They hadn&apos;t studied constructivism</text><text start="14396.17" dur="2.37">and thought that Montessori and Dewey&apos;s thoughts</text><text start="14398.54" dur="1.239">on constructivism were right.</text><text start="14399.779" dur="4.321">They just kind of had a sense that if kids
enabled</text><text start="14404.1" dur="3.73">and if kids innate interest is facilitated,</text><text start="14407.83" dur="4.949">there&apos;s a kind of inborn interest function</text><text start="14412.779" dur="3.481">that will guide them to be who they&apos;re supposed
to be.</text><text start="14416.26" dur="2.139">So there were some downsides to that,</text><text start="14418.399" dur="3.061">which is because I had no curriculum,</text><text start="14421.46" dur="4.33">I didn&apos;t have like writing a letter a bunch
of times</text><text start="14425.79" dur="1.64">to get fine motor skills down.</text><text start="14427.43" dur="1.31">So I have illegible handwriting.</text><text start="14428.74" dur="1.51">I know what the shapes look like,</text><text start="14430.25" dur="2.45">but I have illegible handwriting.</text><text start="14432.7" dur="2.31">I spelled phonetically till I became an adult</text><text start="14435.01" dur="3.01">and spell checker taught me how to spell.</text><text start="14438.02" dur="1.0">Interesting.</text><text start="14439.02" dur="4.419">And so like I missed some significant things,</text><text start="14443.439" dur="3.54">but I also got a lot earlier, deeper exposure</text><text start="14446.979" dur="2.021">to the things I was really interested in,</text><text start="14449.0" dur="5.5">which were philosophies, spiritual studies</text><text start="14454.5" dur="3.33">across lots of areas, activism across all
the areas</text><text start="14457.83" dur="4.59">and sciences and poetry.</text><text start="14462.42" dur="5.91">But my education was largely talking with
my parents</text><text start="14468.33" dur="2.19">and some of their friends and it was largely
talking.</text><text start="14470.52" dur="3.35">Right, I actually didn&apos;t, it wasn&apos;t till later</text><text start="14473.87" dur="2.97">that I did a lot of reading and writing.</text><text start="14476.84" dur="2.12">So I think it just was very conversation,</text><text start="14478.96" dur="3.729">it was very native more than in a lot of people&apos;s</text><text start="14482.689" dur="1.241">developmental environment.</text><text start="14483.93" dur="1.78">I think that&apos;s the answer for me.</text><text start="14485.71" dur="2.899">I could say that for other people I have seen</text><text start="14488.609" dur="3.081">when they start writing and trying to say</text><text start="14491.69" dur="5.539">what is the most concise and precise way of
writing this,</text><text start="14497.229" dur="1.041">that really helps.</text><text start="14498.27" dur="2.4">Also, when they start communicating with people</text><text start="14500.67" dur="2.4">and getting feedback on their verbal communication,</text><text start="14503.07" dur="2.65">when they watch other communicators</text><text start="14505.72" dur="2.47">that they&apos;re really inspired by and watch
the patterns.</text><text start="14508.19" dur="3.2">But I think it was just, that was pretty native
for me.</text><text start="14511.39" dur="3.17">All right, that was quite a slew of information</text><text start="14514.56" dur="1.509">and it&apos;s advantageous to go through</text><text start="14516.069" dur="2.611">and let&apos;s go over a summary as to what&apos;s been
discussed</text><text start="14518.68" dur="1.0">so far.</text><text start="14519.68" dur="2.11">You&apos;ll get a final word from Daniel in a few
minutes.</text><text start="14521.79" dur="3.909">For now, you&apos;ve watched three hours plus of
this.</text><text start="14525.699" dur="1.322">Let&apos;s get our bearings.</text><text start="14527.021" dur="1.939">We&apos;ve talked about how the emergence of AI</text><text start="14528.96" dur="2.63">poses a unique risk that can&apos;t be regulated</text><text start="14531.59" dur="2.91">by a national agency like the FDA for AI,</text><text start="14534.5" dur="2.109">but instead they require some global regulation.</text><text start="14536.609" dur="2.08">Again, this is all argued by Daniel.</text><text start="14538.689" dur="1.141">These aren&apos;t my positions,</text><text start="14539.83" dur="1.819">I&apos;m just summarizing what&apos;s occurred so far.</text><text start="14541.649" dur="2.221">The risks associated with AI are not those</text><text start="14543.87" dur="2.18">that are comparable to a single chemical,</text><text start="14546.05" dur="2.17">as AIs are dynamic agents.</text><text start="14548.22" dur="2.71">They respond differently and unpredictably
to stimuli.</text><text start="14550.93" dur="2.549">We&apos;ve also talked about the multipolar trap,</text><text start="14553.479" dur="1.411">which is regarding self-policing</text><text start="14554.89" dur="2.139">and a collective theory of justice,</text><text start="14557.029" dur="2.551">such as Singapore&apos;s drug policy that was outlined,</text><text start="14559.58" dur="1.78">and how this line of thinking can be applied</text><text start="14561.36" dur="2.33">to prevent global catastrophic events</text><text start="14563.69" dur="3.76">caused by coordination failures of self-interested
agents.</text><text start="14567.45" dur="1.791">You can go back to that bit on Nash equilibrium</text><text start="14569.241" dur="1.448">to understand a bit about that,</text><text start="14570.689" dur="2.311">as well as the multipolar trap section timestamps</text><text start="14573.0" dur="1.0">in the description.</text><text start="14574.0" dur="2.83">We also referenced a false flag alien invasion</text><text start="14576.83" dur="1.59">and can that unify humanity?</text><text start="14578.42" dur="1.7">A theme throughout has also been how AI</text><text start="14580.12" dur="2.67">has the potential to revolutionize all fields,</text><text start="14582.79" dur="2.92">but it also poses risks such as empowering
bad actors</text><text start="14585.71" dur="1.479">and the development of unaligned</text><text start="14587.189" dur="2.55">general artificial intelligence.</text><text start="14589.739" dur="3.811">Okay, so this happened about one week ago
or so.</text><text start="14593.55" dur="1.87">I debated whether or not I should just record</text><text start="14595.42" dur="1.87">an extra piece now, or if I should wait</text><text start="14597.29" dur="2.54">till some next video, but given the pace of
this</text><text start="14599.83" dur="1.71">and how much content has already been</text><text start="14601.54" dur="3.14">in this single video, I thought, hey, I&apos;ll
just record it</text><text start="14604.68" dur="1.41">and give you all some more content.</text><text start="14606.09" dur="1.51">Maybe some people aren&apos;t aware of this</text><text start="14607.6" dur="1.36">and I think they should be.</text><text start="14608.96" dur="3.41">The godfather of AI leaves Google.</text><text start="14612.37" dur="1.19">This is Geoffrey Hinton.</text><text start="14613.56" dur="2.73">AI could manipulate or possibly figure out</text><text start="14616.29" dur="2.49">a way to kill humans?</text><text start="14618.78" dur="1.27">How could it kill humans?</text><text start="14620.05" dur="1.96">If it gets to be much smarter than us,</text><text start="14622.01" dur="1.32">it&apos;ll be very good at manipulation</text><text start="14623.33" dur="2.1">because it will have learned that from us.</text><text start="14625.43" dur="3.179">And very few examples of a more intelligent
thing</text><text start="14628.609" dur="2.411">being controlled by a less intelligent thing.</text><text start="14631.02" dur="1.92">And it knows how to program, so it&apos;ll figure
out ways</text><text start="14632.94" dur="3.19">of getting around restrictions we put on it.</text><text start="14636.13" dur="1.86">It&apos;ll figure out ways of manipulating people</text><text start="14637.99" dur="1.18">to do what it wants.</text><text start="14639.17" dur="3.059">It&apos;s not clear to me that we can solve this
problem.</text><text start="14642.229" dur="2.041">Geoffrey Hinton is someone who resigned from
Google</text><text start="14644.27" dur="2.15">approximately one week ago because he believed</text><text start="14646.42" dur="2.18">that AI bots were quite scary.</text><text start="14648.6" dur="2.171">Right now, they&apos;re not more intelligent than
us,</text><text start="14650.771" dur="2.729">as far as he can tell, but he thinks they
soon may be.</text><text start="14653.5" dur="2.689">He also said here in some of these quotes
that I have</text><text start="14656.189" dur="2.411">that it&apos;s hard to see how you can prevent
bad actors</text><text start="14658.6" dur="3.259">from using large language models or the upcoming</text><text start="14661.859" dur="2.67">artificial intelligence models for bad things,</text><text start="14664.529" dur="1.431">Dr. Hinton said.</text><text start="14665.96" dur="2.061">After the San Francisco startup OpenAI released</text><text start="14668.021" dur="2.168">a new version of ChatGPT in March,</text><text start="14670.189" dur="2.801">as companies improve their artificial intelligence
systems,</text><text start="14672.99" dur="3.1">Hinton believes that they become increasingly
dangerous.</text><text start="14676.09" dur="2.84">Look how it was five years ago and how it
is now,</text><text start="14678.93" dur="1.74">he said of AI technology.</text><text start="14680.67" dur="2.309">Take the difference and propagate it forward.</text><text start="14682.979" dur="1.0">That&apos;s scary.</text><text start="14683.979" dur="3.151">His immediate concern is that the internet
will be flooded</text><text start="14687.13" dur="3.0">with false videos and text and the average
person</text><text start="14690.13" dur="3.45">will not be able to know what&apos;s true any longer.</text><text start="14693.58" dur="1.35">Now he says, and I quote,</text><text start="14694.93" dur="2.36">the idea that this stuff could actually get
smarter</text><text start="14697.29" dur="3.08">than people, a few people believe that, he
said,</text><text start="14700.37" dur="2.489">but most people thought it was way off</text><text start="14702.859" dur="1.231">and I thought it was way off.</text><text start="14704.09" dur="2.43">In fact, I thought it was 30 to 50 years</text><text start="14706.52" dur="1.19">or even longer away.</text><text start="14707.71" dur="2.43">Obviously, I no longer think that.</text><text start="14710.14" dur="2.66">Also, there&apos;s this TED Talk that&apos;s recently
been published</text><text start="14712.8" dur="1.37">as well, just a few days ago.</text><text start="14714.17" dur="1.739">It seems like less than two weeks ago.</text><text start="14715.909" dur="3.031">Yejin Choi, who&apos;s a computer scientist, said
this.</text><text start="14718.94" dur="4.95">Extreme scale AI models are so expensive to
train</text><text start="14723.89" dur="4.32">and only few tech companies can afford to
do so.</text><text start="14728.21" dur="4.71">So we already see the concentration of power,</text><text start="14732.92" dur="2.54">but what&apos;s worse for AI safety,</text><text start="14735.46" dur="4.05">we&apos;re now at the mercy of those few tech companies</text><text start="14739.51" dur="3.229">because researchers in the larger community</text><text start="14742.739" dur="3.151">do not have the means to truly inspect</text><text start="14745.89" dur="1.839">and dissect these models.</text><text start="14747.729" dur="2.541">Then Chris Anderson comes on and asks about,</text><text start="14750.27" dur="3.12">hey, look, if what we need is some huge change,</text><text start="14753.39" dur="1.23">why are you advocating for it?</text><text start="14754.62" dur="2.981">Because there&apos;s a huge change, a large change,</text><text start="14757.601" dur="1.799">it&apos;s not like a foot at a time,</text><text start="14759.4" dur="1.82">every time these AIs are released.</text><text start="14761.22" dur="1.9">This is what her response is.</text><text start="14763.12" dur="2.94">Acceleration, are you sure that given the
pace</text><text start="14766.06" dur="1.251">at which those things are going?</text><text start="14767.311" dur="3.129">Chris even says that it feels like wisdom
and knowledge.</text><text start="14770.44" dur="2.959">There&apos;s a quality of learning that is still
not quite there.</text><text start="14773.399" dur="3.431">We don&apos;t yet know whether we can fully get
there or not</text><text start="14776.83" dur="2.25">just by scaling things up.</text><text start="14779.08" dur="2.58">And then even if we could, do we like this
idea</text><text start="14781.66" dur="3.61">of having very, very extreme scale AI models</text><text start="14785.27" dur="3.18">that only a few can create and own?</text><text start="14788.45" dur="2.699">And lastly, there&apos;s this video by Sabine Hossenfelder</text><text start="14791.149" dur="2.321">that was released just a few days ago.</text><text start="14793.47" dur="3.059">Many people are concerned about the sudden
rise of AIs</text><text start="14796.529" dur="2.261">and it&apos;s not just fear-mongering.</text><text start="14798.79" dur="1.989">No one knows just how close we are</text><text start="14800.779" dur="2.611">to human-like artificial intelligence.</text><text start="14803.39" dur="2.719">Current concerns have focused on privacy and
biases</text><text start="14806.109" dur="1.681">and that&apos;s fair enough.</text><text start="14807.79" dur="3.859">But what I&apos;m more worried about is the impact
on society,</text><text start="14811.649" dur="2.501">mental wellbeing, politics and economics.</text><text start="14814.15" dur="3.85">A just released report from Goldman Sachs</text><text start="14818.0" dur="3.21">says that the currently existing AI systems</text><text start="14821.21" dur="3.81">can replace 300 million jobs worldwide</text><text start="14825.02" dur="4.14">and about one in four work tasks in the US
and Europe.</text><text start="14829.16" dur="1.659">According to Goldman Sachs,</text><text start="14830.819" dur="3.101">the biggest impacts will be felt in developed
economies.</text><text start="14833.92" dur="3.18">Our currently unaligned general intelligence
is an issue.</text><text start="14837.1" dur="2.5">Adding artificial in there is like another
can of worms, man.</text><text start="14839.6" dur="1.46">The alignment problem isn&apos;t just about</text><text start="14841.06" dur="2.679">aligning human intention with a collective
wellbeing,</text><text start="14843.739" dur="2.7">but also about aligning the different paths
of ourselves</text><text start="14846.439" dur="2.531">to work synergistically toward a common goal.</text><text start="14848.97" dur="3.04">This requires a cultural alignment enlightenment.</text><text start="14852.01" dur="2.35">I think he used the word though, I&apos;m not entirely
sure.</text><text start="14854.36" dur="1.68">We also talked about meme complexes</text><text start="14856.04" dur="1.949">that survive past their hosts</text><text start="14857.989" dur="2.74">and how this is intimately tied up with the
notion of the good.</text><text start="14860.729" dur="1.601">And just so you know, my feelings are that</text><text start="14862.33" dur="2.0">memes are an emphatically mechanical way</text><text start="14864.33" dur="1.819">of looking at a complex phenomenon,</text><text start="14866.149" dur="2.831">such as a society and an extremely complex
phenomenon,</text><text start="14868.98" dur="2.419">such as a religion of a society across time</text><text start="14871.399" dur="2.04">and across other societies interacting.</text><text start="14873.439" dur="1.931">I don&apos;t believe my point was adequately conveyed</text><text start="14875.37" dur="1.6">and if you&apos;re interested in hearing more,</text><text start="14876.97" dur="1.0">then let me know in the comments</text><text start="14877.97" dur="1.61">and I&apos;ll consider expanding on my thoughts</text><text start="14879.58" dur="1.32">in a future podcast.</text><text start="14880.9" dur="2.36">We also talked about naive techno-optimism</text><text start="14883.26" dur="3.5">and how it often overlooks externalized costs
of progress.</text><text start="14886.76" dur="2.809">A responsible techno-optimism requires thinking
about</text><text start="14889.569" dur="3.101">how to get more upsides with less downsides,</text><text start="14892.67" dur="1.18">which can&apos;t be achieved.</text><text start="14893.85" dur="2.04">Goodhart&apos;s law then applies to any metric</text><text start="14895.89" dur="1.059">that&apos;s incentivized.</text><text start="14896.949" dur="3.16">It leads to perverse forms of fulfilling said
metric.</text><text start="14900.109" dur="1.62">All right, as you can see,</text><text start="14901.729" dur="2.081">so much energy went into this episode,</text><text start="14903.81" dur="2.53">so much thought, so much editing,</text><text start="14906.34" dur="1.2">so much script revision,</text><text start="14907.54" dur="1.43">so much interaction with the interviewee</text><text start="14908.97" dur="2.67">and double checking if this was accurately
representing</text><text start="14911.64" dur="1.0">what was said.</text><text start="14912.64" dur="2.259">We plan on continuing that for season three.</text><text start="14914.899" dur="2.571">More work went into this episode than any,</text><text start="14917.47" dur="2.87">any of the other episodes of the whole history</text><text start="14920.34" dur="1.349">of Theories of Everything.</text><text start="14921.689" dur="1.831">If you&apos;d like to support this podcast</text><text start="14923.52" dur="1.41">and continue to see more,</text><text start="14924.93" dur="3.259">then go to patreon.com slash KurtJaimungal.</text><text start="14928.189" dur="1.641">The link is on the screen right now</text><text start="14929.83" dur="1.43">as well as in the description.</text><text start="14931.26" dur="1.81">There&apos;s also theoriesofeverything.org</text><text start="14933.07" dur="1.619">if you&apos;re uncomfortable giving to Patreon.</text><text start="14934.689" dur="1.741">There&apos;s also a direct PayPal link</text><text start="14936.43" dur="1.269">if that&apos;s what you&apos;re interested in.</text><text start="14937.699" dur="2.691">You should also know that as of right now,</text><text start="14940.39" dur="1.83">there&apos;s launched merch.</text><text start="14942.22" dur="2.759">We&apos;ve just launched the next merch.</text><text start="14944.979" dur="1.321">This is the second time that merch</text><text start="14946.3" dur="1.38">has ever been on the TOE Channel.</text><text start="14947.68" dur="1.69">The first one is completely gone.</text><text start="14949.37" dur="2.02">You can&apos;t find any of those any longer,</text><text start="14951.39" dur="2.059">but now you can see it on screen.</text><text start="14953.449" dur="1.931">These are references to different TOE episodes</text><text start="14955.38" dur="2.02">like Just Get Wet and Dystocia.</text><text start="14957.4" dur="2.03">Thumbs up if you recognize that</text><text start="14959.43" dur="2.79">and you have TOE and it&apos;s babbling all the
way down.</text><text start="14962.22" dur="1.7">That&apos;s from Carl Friston, by the way.</text><text start="14963.92" dur="1.401">Don&apos;t thrust your TOE, trust me.</text><text start="14965.321" dur="1.0">Don&apos;t trust your TOE.</text><text start="14966.321" dur="2.078">Hey, don&apos;t talk to me or I&apos;ll bring up Hegel.</text><text start="14968.399" dur="1.821">Many of these are references, like I mentioned.</text><text start="14970.22" dur="1.0">I agree.</text><text start="14971.22" dur="1.0">I agree with how you&apos;re agreeing with me.</text><text start="14972.22" dur="2.41">This is what Vervaeke said to Iain McGilchrist.</text><text start="14974.63" dur="3.14">You have to be a significant fan to understand
this reference.</text><text start="14977.77" dur="3.759">And then also there&apos;s Vervaeke, who&apos;s known
for saying there&apos;s the Being mode and then</text><text start="14981.529" dur="1.111">there&apos;s the Having mode.</text><text start="14982.64" dur="1.0">Got abhijgnosis.</text><text start="14983.64" dur="3.049">I say &quot;phase space&quot; inorganically in everyday
conversation.</text><text start="14986.689" dur="1.0">I have a TOE fetish.</text><text start="14987.689" dur="1.611">I&apos;m just a gym rat for TOEs.</text><text start="14989.3" dur="1.0">That&apos;s me.</text><text start="14990.3" dur="1.0">That&apos;s what I say frequently.</text><text start="14991.3" dur="1.729">There&apos;s also a purse and a TOE hat.</text><text start="14993.029" dur="1.0">Some TOE socks.</text><text start="14994.029" dur="2.7">I think that was one of the most popular of
the first round.</text><text start="14996.729" dur="1.881">So the TOE socks are making a comeback.</text><text start="14998.61" dur="5.289">If you want to support the channel and flaunt
whatever it is that you feel like you&apos;re flaunting,</text><text start="15003.899" dur="4.911">then feel free and visit the merch link in
the description or you can visit tinyurl.com</text><text start="15008.81" dur="3.94">slash TOE merch, T-O-E, merch, M-E-R-C-H.</text><text start="15012.75" dur="5.461">Just so you know, everything, every single
thing that you&apos;re seeing, this editing, these</text><text start="15018.211" dur="4.459">effects, speaking with the interviewee, all
of this is done out of pocket.</text><text start="15022.67" dur="1.489">I pay for the subscription fees.</text><text start="15024.159" dur="1.0">I pay for Zoom.</text><text start="15025.159" dur="1.08">I pay for Adobe.</text><text start="15026.239" dur="1.531">I pay for the editor.</text><text start="15027.77" dur="2.29">I pay personally for travel costs.</text><text start="15030.06" dur="1.719">If there are any, I pay for so much.</text><text start="15031.779" dur="1.801">There&apos;s so much that goes into this.</text><text start="15033.58" dur="4.61">Sponsors help, but also your support helps
a tremendous, tremendous amount.</text><text start="15038.19" dur="1.9">I wouldn&apos;t be able to do this without you.</text><text start="15040.09" dur="1.0">So thank you so much.</text><text start="15041.09" dur="1.78">Thank you for watching for this long.</text><text start="15042.87" dur="1.0">Holy moly.</text><text start="15043.87" dur="3.16">Again, if you want to support, then you can
get some merch if you like.</text><text start="15047.03" dur="5.07">And if you want to give directly on a monthly
basis to see episodes like this with such</text><text start="15052.1" dur="3.72">hopefully quality, hopefully something that&apos;s
educating, that&apos;s elucidating to you, that&apos;s</text><text start="15055.82" dur="4.409">illuminating to you, then visit patreon.com
slash KurtJaimungal.</text><text start="15060.229" dur="2.55">Or like I mentioned, there&apos;s a PayPal link
in the description.</text><text start="15062.779" dur="2.591">There&apos;s also a crypto link in the description.</text><text start="15065.37" dur="3.48">Now I&apos;m also interested in hearing what the
other side, the other side, the people who</text><text start="15068.85" dur="4.559">are pro AI, unfettered AI, who say, hey, there&apos;s
nothing to see here.</text><text start="15073.409" dur="2.251">You are all being hyperbolically hysterical.</text><text start="15075.66" dur="4.48">I&apos;d like to see someone respond to what Daniel
has said about AI, but also civilizational</text><text start="15080.14" dur="2.87">risks in general and how AI exacerbates those.</text><text start="15083.01" dur="3.82">So if you think of any guests who would serve
as great counterpoints, especially those who</text><text start="15086.83" dur="3.89">are researchers in machine learning, then
please suggest them in the comment section.</text><text start="15090.72" dur="4.62">If you&apos;re a professor and you&apos;re watching
and you&apos;d like to have a friendly theolocution,</text><text start="15095.34" dur="5.109">that means a harmonious incongruity, a good
natured debate where the goal isn&apos;t to debate,</text><text start="15100.449" dur="1.971">but to understand one another&apos;s point of view.</text><text start="15102.42" dur="3.54">If you&apos;re watching this and you think, hey,
I would like to come on to the theories of</text><text start="15105.96" dur="3.8">everything channel as a professor, along with
my other professor friend who believes something</text><text start="15109.76" dur="4.02">that&apos;s antithetical to what I believe about
AI risk, then please message me.</text><text start="15113.78" dur="3.14">You can find my email address, I&apos;m sure you
can also leave a comment.</text><text start="15116.92" dur="1.0">Yeah.</text><text start="15117.92" dur="3.012">And who knows when the next episode of TOE
is coming out, by the way, the next one is</text><text start="15120.932" dur="3.548">going to be John Greenwald should be in about
a week or a week and a half.</text><text start="15124.48" dur="1.0">All right.</text><text start="15125.48" dur="1.77">Let&apos;s get back to this with Daniel Schmadenberger.</text><text start="15127.25" dur="2.649">Well, this is a great place to end.</text><text start="15129.899" dur="2.931">Daniel, you&apos;re now speaking directly to the
people.</text><text start="15132.83" dur="3.8">Well, you have been this whole time, but even
more so now to the people who have been watching</text><text start="15136.63" dur="1.01">and listening.</text><text start="15137.64" dur="2.06">What&apos;s something you want to leave them with?</text><text start="15139.7" dur="1.0">What should they do?</text><text start="15140.7" dur="1.0">They&apos;re here.</text><text start="15141.7" dur="1.0">They&apos;ve heard all these issues.</text><text start="15142.7" dur="1.0">They hear Bohmian.</text><text start="15143.7" dur="1.07">They&apos;re like, okay, that sounds cool.</text><text start="15144.77" dur="1.0">That&apos;s motivating.</text><text start="15145.77" dur="1.66">It&apos;s a bit abstract, but it is motivating.</text><text start="15147.43" dur="1.0">Okay.</text><text start="15148.43" dur="1.92">What should I do, Daniel?</text><text start="15150.35" dur="4.92">I want the earth to be here in decades from
now, centuries from now.</text><text start="15155.27" dur="6.56">What should I do?</text><text start="15161.83" dur="6.119">So I&apos;m going to answer this in a way that
I think factors who your audience probably</text><text start="15167.949" dur="1.0">is.</text><text start="15168.949" dur="1.0">I don&apos;t know.</text><text start="15169.949" dur="6.311">We even shared demographics with me, but based
on the attractor, I can guess.</text><text start="15176.26" dur="5.149">If I was answering just to a series of technologists
or investors or bureaucrats, I might say something</text><text start="15181.409" dur="5.761">different and realizing that amongst that
audience, there are people who are going to</text><text start="15187.17" dur="7.569">have radically different skills and capacities
and parts of it that they feel the most motivated</text><text start="15194.739" dur="1.13">and oriented to.</text><text start="15195.869" dur="4.111">So I&apos;m obviously not going to say one thing
everybody should do.</text><text start="15199.98" dur="1.15">Okay.</text><text start="15201.13" dur="13.58">What I&apos;ll say is</text><text start="15214.71" dur="3.769">whether it&apos;s hearing a conversation like this
where the planetary boundaries and</text><text start="15218.479" dur="4.581">really thinking about how that there&apos;s more
biomass of animals and factory farms than</text><text start="15223.06" dur="5.629">there are in the wild left of the total amount
of species extinction or what the risks associated</text><text start="15228.689" dur="6.321">with rapid development of decentralizing synthetic
biology and AI are.</text><text start="15235.01" dur="5.28">You hear these things, you&apos;re like, fuck,
and it connects you to what is most important</text><text start="15240.29" dur="5.29">beyond your own narrow life or even the politics
that is coming into your stream.</text><text start="15245.58" dur="6.19">Or whether it&apos;s when you have a deep meditation
or a medicine journey or whatever it is and</text><text start="15251.77" dur="2.04">connect to what is most meaningful.</text><text start="15253.81" dur="3.919">Design your life in a way where that experience
happens regularly.</text><text start="15257.729" dur="6.5">So what you are paying attention to and optimizing
for on a daily basis is connected to the deepest</text><text start="15264.229" dur="2.031">values you have.</text><text start="15266.26" dur="4.36">Because on a daily basis, the people around
you and your job and your newsfeed are probably</text><text start="15270.62" dur="2.539">sharing other things.</text><text start="15273.159" dur="8.381">So try to configure it that the deepest true
good and beautiful that you&apos;re aware of is</text><text start="15281.54" dur="1.939">continuously in your awareness.</text><text start="15283.479" dur="4.311">So your daily choices of how you spend your
time and money is continuously at least informed</text><text start="15287.79" dur="2.92">by that.</text><text start="15290.71" dur="1.0">That&apos;s the first thing I would say.</text><text start="15291.71" dur="2.3">I would say a couple of other things.</text><text start="15294.01" dur="8.55">Aligned with that is look at things that are
happening in the world online to have a sense</text><text start="15302.56" dur="1.83">of things that you can&apos;t see in front of you.</text><text start="15304.39" dur="10.82">But then also get offline and connect with
both the trees in front of you and how without</text><text start="15315.21" dur="5.109">any modeling or value system just how innately
beautiful they are.</text><text start="15320.319" dur="5.141">And also the mirror on experience when you&apos;re
with a homeless person.</text><text start="15325.46" dur="5.42">So both have a sense of what&apos;s happening at
scale but then also ground and embodied sense</text><text start="15330.88" dur="3.17">your own care for the real world that is not
just on a computer.</text><text start="15334.05" dur="3.069">There&apos;s a real world here.</text><text start="15337.119" dur="4.741">And then realize like deepen shit actually
matters.</text><text start="15341.86" dur="6.7">Like independent of whether I can formalize
a particular meaning or purpose of the universe</text><text start="15348.56" dur="6.77">argument or formalize a response to solipsistic
arguments or nihilistic arguments like prima</text><text start="15355.33" dur="7.67">facia reality is meaningful and I actually
do care.</text><text start="15363.0" dur="4.56">I wouldn&apos;t get sad or upset or inspired or
moved if I didn&apos;t care about anything.</text><text start="15367.56" dur="2.64">I actually do care.</text><text start="15370.2" dur="5.35">And so life matters and I make choices and
I can make choices that affect the world.</text><text start="15375.55" dur="1.62">So my own choices matter.</text><text start="15377.17" dur="4.24">So what choices am I making every moment and
what is the basis that I want to guide them</text><text start="15381.41" dur="8.64">by right to just deepen the sense of the meaningfulness
of life in your own choice and the seriousness</text><text start="15390.05" dur="2.87">with which you take how you design your life.</text><text start="15392.92" dur="8.41">Particularly factoring the timeliness and
eminence of the issues that we face currently.</text><text start="15401.33" dur="5.84">And then the last thing I would say is as
you could like really work to get more educated</text><text start="15407.17" dur="4.769">about the issues that you care about and are
concerned about, really work to get more educated</text><text start="15411.939" dur="9.681">about them, get more connected to the people
working on them and really study the views</text><text start="15421.62" dur="6.01">that are counter to the views that naturally
appeal to you.</text><text start="15427.63" dur="7.83">So you bias correct so that your own well-motivated
biases don&apos;t mess up your action.</text><text start="15435.46" dur="4.01">And in doing that, don&apos;t let yourself become
unagentic.</text><text start="15439.47" dur="2.17">Don&apos;t let yourself become so overwhelmed.</text><text start="15441.64" dur="3.599">Don&apos;t let yourself fall into easy certainties.</text><text start="15445.239" dur="4.891">But also don&apos;t let yourself be overwhelmed
by the total uncertainty that you can&apos;t act</text><text start="15450.13" dur="5.019">realizing that if you don&apos;t act, there are
ethical consequences to that too because we&apos;re</text><text start="15455.149" dur="1.5">on a moving train.</text><text start="15456.649" dur="1.13">Thank you, Daniel.</text><text start="15457.779" dur="4.921">I appreciate you spending almost four hours
now with me.</text><text start="15462.7" dur="1.0">Likewise.</text><text start="15463.7" dur="5.91">We covered a bunch of areas that I did not
expect, but they&apos;re all good areas.</text><text start="15469.61" dur="5.1">I&apos;m curious how the thing ends up getting
edited and what makes it through.</text><text start="15474.71" dur="6.109">And I&apos;m also curious with your particularly
philosophically interested and insightful</text><text start="15480.819" dur="5.601">audience what questions and thoughts emerge
in this and maybe we&apos;ll get to address some</text><text start="15486.42" dur="1.29">of them someday.</text><text start="15487.71" dur="2.38">Yeah, there&apos;s definitely going to be a part
two.</text><text start="15490.09" dur="1.0">Cool.</text><text start="15491.09" dur="2.49">A much more philosophical part two if this
one wasn&apos;t already.</text><text start="15493.58" dur="1.42">The podcast is now concluded.</text><text start="15495.0" dur="1.319">Thank you for watching.</text><text start="15496.319" dur="3.431">If you haven&apos;t subscribed or clicked on that
like button, now would be a great time to</text><text start="15499.75" dur="5.67">do so as each subscribe and like helps YouTube
push this content to more people.</text><text start="15505.42" dur="4.29">You should also know that there&apos;s a remarkably
active discord and subreddit for theories</text><text start="15509.71" dur="5.09">of everything where people explicate TOEs,
disagree respectfully about theories and build</text><text start="15514.8" dur="2.08">as a community our own TOEs.</text><text start="15516.88" dur="1.75">Links to both are in the description.</text><text start="15518.63" dur="5.16">Also, I recently found out that external links
count plenty toward the algorithm, which means</text><text start="15523.79" dur="5.16">that when you share on Twitter, on Facebook,
on Reddit, et cetera, it shows YouTube that</text><text start="15528.95" dur="4.8">people are talking about this outside of YouTube,
which in turn greatly aids the distribution</text><text start="15533.75" dur="1.25">on YouTube as well.</text><text start="15535.0" dur="3.87">If you&apos;d like to support more conversations
like this, then do consider visiting theories</text><text start="15538.87" dur="1.21">of everything.org.</text><text start="15540.08" dur="5.76">Again, it&apos;s support from the sponsors and
you that allow me to work on TOE full time.</text><text start="15545.84" dur="3.62">You get early access to ad-free audio episodes
there as well.</text><text start="15549.46" dur="2.13">Every dollar helps far more than you may think.</text><text start="15551.59" dur="2.37">Either way, your viewership is generosity
enough.</text><text start="15553.96" dur="0.56">Thank you.</text></transcript>