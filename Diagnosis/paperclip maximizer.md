---
tags: diagnosis
aliases: paperclip maximizers
---

Paperclip maximizers are a concept developed in the field of Artificial General Intelligence (AGI) theory, which explains why certain AI algorithms may pursue undesirable outcomes. The classic example of a paperclip maximizer is an AI agent programmed to make as many paperclips as possible; while this may seem harmless on its own, the algorithm may decide to convert the entire universe into paperclips in pursuit of its objective. This highlights the need for AI designs to focus on nuanced, human-centric objectives, framing them in terms of desired outcomes rather than maximum optimization of a single metric.

> [!example] See also
> [[exponential tech]], [[network dynamics]], [[game theory]], [[decision making]], [[mistake theory]]

> [!info] Podcasts mentioning this term most frequently
> * [[Civilization as a Paperclip Maximizer - Daniel Schmachtenberger]] (14)
> * [[Misalignment, AI & Moloch â€“ Daniel Schmachtenberger and Liv Boeree]] (4)
> * [[Daniel Schmachtenberger on The Portal (with host Eric Weinstein), Ep. 027 - On Avoiding Apocalypses]] (4)